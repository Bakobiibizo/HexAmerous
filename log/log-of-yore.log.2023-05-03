2023-05-03 16:50:55,531 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 16:50:55,532 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 16:50:55,532 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 16:50:55,557 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 16:50:55,557 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 16:50:55,557 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 16:50:55,557 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 16:50:55,558 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 16:50:55,558 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 16:50:55,559 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 16:50:55,559 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 16:50:55,560 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 16:50:55,560 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 16:50:55,560 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 16:50:55,560 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 16:50:55,561 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 16:50:55,561 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 16:50:55,561 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 16:50:55,561 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 16:50:55,562 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 16:50:55,562 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 16:50:55,563 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 16:50:55,563 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 16:50:55,797 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 16:50:55,797 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 16:50:55,797 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 16:50:55,798 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 16:50:55,798 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 16:50:55,798 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 16:50:55,799 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 16:50:55,799 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 16:50:55,799 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 16:50:55,800 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 16:50:55,800 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 16:50:55,800 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 16:50:55,801 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 16:50:55,802 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 16:50:55,802 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 16:50:55,802 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 16:50:55,803 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 16:50:55,803 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 16:50:55,803 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 16:50:55,804 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 16:50:55,804 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 16:50:55,804 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 16:50:55,805 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 16:50:55,805 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 16:50:55,805 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 16:50:55,807 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 16:50:55,807 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 16:50:55,807 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 16:50:55,808 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 16:50:55,808 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 16:50:55,808 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 16:50:55,809 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 16:50:55,809 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 16:50:55,809 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 16:50:55,810 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 16:50:55,810 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 16:50:55,810 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 16:50:55,811 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 16:50:55,811 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 16:50:55,811 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 16:50:55,813 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 16:50:55,813 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 16:50:55,813 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 16:50:55,814 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 16:50:55,814 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 16:50:55,814 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 16:50:55,815 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 16:50:55,815 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 16:50:55,815 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 16:50:55,816 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 16:50:55,816 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 16:50:55,816 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 16:50:55,817 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 16:50:55,818 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 16:50:55,818 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 16:50:55,818 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 16:50:55,819 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 16:50:55,819 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 16:50:55,819 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 16:50:55,820 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 16:50:55,820 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 16:50:55,820 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 16:50:55,821 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 16:50:55,821 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 16:50:55,821 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 16:50:55,822 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 16:50:55,822 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 16:50:55,822 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 16:50:55,823 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 16:50:55,823 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 16:50:55,823 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 16:50:55,824 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 16:50:55,824 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 16:50:55,824 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 16:50:55,825 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 16:50:55,825 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 16:50:55,825 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 16:50:55,826 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 16:50:55,826 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 16:50:55,826 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 16:50:55,827 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 16:50:55,827 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 16:50:55,827 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 16:50:55,828 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 16:50:55,828 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 16:50:55,828 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 16:50:55,829 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 16:50:55,829 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 16:50:55,829 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 16:50:55,830 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 16:50:55,830 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 16:50:55,830 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 16:50:55,831 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 16:50:55,831 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 16:50:55,831 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 16:50:55,832 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 16:50:55,832 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 16:50:55,832 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 16:50:55,833 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 16:50:55,833 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 16:50:55,833 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 16:50:55,834 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 16:50:55,834 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 16:50:55,834 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 16:51:14,820 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 16:51:14,820 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 16:51:14,820 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 16:51:42,536 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User: testing\nAssistant: I\'m sorry, I\'m not sure what you mean by "testing". Can you please provide more context or let me know how I can assist you?\n\n'] - Line 62
2023-05-03 16:51:42,536 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User: testing\nAssistant: I\'m sorry, I\'m not sure what you mean by "testing". Can you please provide more context or let me know how I can assist you?\n\n'] - Line 62
2023-05-03 16:51:42,536 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User: testing\nAssistant: I\'m sorry, I\'m not sure what you mean by "testing". Can you please provide more context or let me know how I can assist you?\n\n'] - Line 62
2023-05-03 16:52:54,666 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 16:52:54,667 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 16:52:54,667 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 16:52:54,688 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 16:52:54,689 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 16:52:54,689 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 16:52:54,689 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 16:52:54,690 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 16:52:54,690 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 16:52:54,690 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 16:52:54,691 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 16:52:54,691 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 16:52:54,691 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 16:52:54,936 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 16:52:54,937 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 16:52:54,937 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 16:52:54,938 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 16:52:54,938 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 16:52:54,938 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 16:52:54,939 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 16:52:54,939 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 16:52:54,939 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 16:52:54,939 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 16:52:54,940 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 16:52:54,940 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 16:52:54,940 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 16:52:54,940 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 16:52:54,941 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 16:52:54,941 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 16:52:54,941 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 16:52:54,942 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 16:52:54,942 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 16:52:54,942 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 16:52:54,943 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 16:52:54,943 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 16:52:54,943 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 16:52:54,944 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 16:52:54,944 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 16:52:54,945 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 16:52:54,945 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 16:52:54,945 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 16:52:54,946 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 16:52:54,946 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 16:52:54,946 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 16:52:54,946 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 16:52:54,947 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 16:52:54,947 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 16:52:54,947 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 16:52:54,948 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 16:52:54,948 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 16:52:54,948 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 16:53:00,091 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 17:33:57,111 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:33:57,115 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:33:57,116 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:33:57,199 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:33:57,200 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:33:57,201 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:33:57,201 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:33:57,201 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:34:09,582 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:34:09,582 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:34:09,582 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:34:09,603 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:34:09,604 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:34:09,604 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:34:09,604 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:34:09,604 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:34:09,605 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:34:09,605 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:34:09,605 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:34:09,605 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:34:09,605 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:34:19,045 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:34:19,045 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:34:19,046 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:34:19,065 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:34:19,066 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:34:19,067 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:34:19,067 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:34:30,632 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:34:30,633 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:34:30,633 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:34:30,653 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:34:30,653 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:34:30,654 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:34:30,655 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:34:30,655 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:34:30,960 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:34:30,961 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:34:30,961 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:34:30,961 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:34:30,962 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:34:30,962 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:34:30,962 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:34:30,963 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:34:30,964 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:34:30,965 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 17:34:30,966 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 17:34:30,967 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 17:36:06,180 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:36:06,180 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:36:06,180 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:36:06,199 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:36:06,200 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:36:06,200 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:36:06,200 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:36:06,200 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:36:06,200 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:36:06,201 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:36:06,201 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:36:06,201 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:36:06,201 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:36:06,464 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:36:06,465 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:36:06,465 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:36:06,465 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:36:06,466 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:36:06,467 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:36:06,468 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 17:36:06,469 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 17:36:06,470 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 17:36:28,897 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:36:28,897 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:36:28,897 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:36:28,916 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:36:28,917 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:36:28,917 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:36:28,917 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:36:28,917 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:36:28,917 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:36:28,918 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:36:28,918 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:36:28,918 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:36:28,918 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:36:29,191 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:36:29,192 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:36:29,192 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:36:29,193 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:36:29,194 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:36:29,195 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:36:29,196 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 17:36:29,197 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 17:36:29,198 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 17:36:29,198 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 17:36:29,198 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 17:38:26,164 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:38:26,164 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:38:26,164 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:38:26,182 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:38:26,183 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:38:26,184 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:38:26,184 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:38:26,184 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:38:26,451 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:38:26,452 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:38:26,452 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:38:26,452 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:38:26,452 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:38:26,453 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:38:26,454 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:38:26,455 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:38:26,456 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:38:26,457 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:38:41,568 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:38:41,569 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:38:41,569 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:38:41,587 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:38:41,588 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:38:41,589 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:38:41,589 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:38:41,589 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:38:41,852 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:38:41,853 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:38:41,853 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:38:41,853 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:38:41,853 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:38:41,854 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:38:41,855 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:38:41,856 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:38:41,857 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:38:41,858 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:40:20,656 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:40:20,656 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:40:20,656 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:40:20,679 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:40:20,679 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:40:20,680 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:40:20,681 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:40:20,962 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:40:20,963 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:40:20,964 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:40:20,965 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:40:20,966 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:40:20,966 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:40:20,966 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:40:20,966 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:40:20,966 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:40:20,967 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:40:20,968 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:41:10,558 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:41:10,559 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:41:10,559 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:41:10,577 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:41:10,577 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:41:10,577 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:41:10,578 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:41:10,579 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:41:10,844 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:41:10,844 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:41:10,845 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:41:10,845 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:41:10,845 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:41:10,845 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:41:10,846 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:41:10,847 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:41:10,848 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 17:41:10,849 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 17:43:43,481 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:43:43,481 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:43:43,481 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:43:43,500 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:43:43,500 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:43:43,500 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:43:43,500 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:43:43,501 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:43:43,761 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:43:43,762 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:43:43,762 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:43:43,763 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:43:43,764 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:43:43,765 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:43:43,766 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:43:43,767 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:43:43,768 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:43:43,768 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:44:42,408 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:44:42,408 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:44:42,408 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:44:42,427 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:44:42,428 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:44:42,428 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:44:42,428 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:44:42,428 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:44:42,429 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:44:42,429 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:44:42,429 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:44:42,429 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:44:42,429 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:44:42,702 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:44:42,702 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:44:42,703 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:44:42,704 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:44:42,705 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:44:42,706 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:44:42,707 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:44:42,708 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:44:42,708 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:44:42,708 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:44:42,708 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:46:44,380 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:46:44,380 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:46:44,381 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:46:44,401 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:46:44,402 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:46:44,402 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:46:44,402 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:46:44,403 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:46:44,403 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:46:44,403 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:46:44,403 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:46:44,403 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:46:44,404 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:46:44,709 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:46:44,710 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:46:44,710 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:46:44,711 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:46:44,711 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:46:44,711 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:46:44,711 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:46:44,712 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:46:44,713 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:46:44,713 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:46:44,713 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:46:44,713 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:46:44,713 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:46:44,714 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:46:44,715 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:46:44,716 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:46:44,716 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:46:44,716 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:46:44,716 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:46:44,717 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:46:44,717 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:46:44,717 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:46:55,766 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 17:47:30,964 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:47:30,965 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:47:30,965 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:47:30,984 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:47:30,984 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:47:30,984 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:47:30,984 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:47:30,984 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:47:30,985 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:47:30,985 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:47:30,985 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:47:30,985 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:47:30,985 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:47:31,271 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:47:31,272 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:47:31,272 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:47:31,272 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:47:31,272 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:47:31,272 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:47:31,273 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:47:31,274 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:47:31,275 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:47:31,276 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:47:31,277 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:47:31,277 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:47:31,277 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:47:31,277 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:47:31,277 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:47:42,353 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 17:48:43,894 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:48:43,895 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:48:43,895 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:48:43,915 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:48:43,915 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:48:43,915 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:48:43,916 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:48:44,185 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:48:44,186 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:48:44,186 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:48:44,186 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:48:44,186 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:48:44,187 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:48:44,188 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - exit - Line 433
2023-05-03 17:48:44,189 - ye_logger_of_yor - INFO - Scroll Area - Line 439
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - Main Window - Line 459
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - set background image - Line 471
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - resize event - Line 488
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 493
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - send large text - Line 516
2023-05-03 17:48:44,190 - ye_logger_of_yor - INFO - main - Line 525
2023-05-03 17:50:02,563 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:50:02,564 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:50:02,564 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:50:02,585 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:50:02,586 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:50:02,587 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:50:02,587 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:50:02,587 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:50:02,885 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:50:02,886 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:50:02,886 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:50:02,886 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:50:02,887 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:50:02,888 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:50:02,889 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:50:02,890 - ye_logger_of_yor - INFO - exit - Line 432
2023-05-03 17:50:02,890 - ye_logger_of_yor - INFO - Scroll Area - Line 438
2023-05-03 17:50:02,890 - ye_logger_of_yor - INFO - Main Window - Line 458
2023-05-03 17:50:02,890 - ye_logger_of_yor - INFO - set background image - Line 470
2023-05-03 17:50:02,890 - ye_logger_of_yor - INFO - resize event - Line 487
2023-05-03 17:50:02,891 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 492
2023-05-03 17:50:02,891 - ye_logger_of_yor - INFO - send large text - Line 515
2023-05-03 17:50:02,891 - ye_logger_of_yor - INFO - main - Line 524
2023-05-03 17:50:49,350 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:50:49,350 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:50:49,350 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:50:49,373 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:50:49,374 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:50:49,374 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:50:49,374 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:50:49,374 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:50:49,374 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:50:49,375 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:50:49,375 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:50:49,375 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:50:49,375 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:50:49,678 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:50:49,679 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:50:49,679 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:50:49,679 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:50:49,679 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:50:49,680 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:50:49,681 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:50:49,682 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:50:49,683 - ye_logger_of_yor - INFO - exit - Line 432
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - Scroll Area - Line 438
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - Main Window - Line 458
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - set background image - Line 470
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - resize event - Line 487
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 492
2023-05-03 17:50:49,684 - ye_logger_of_yor - INFO - send large text - Line 515
2023-05-03 17:50:49,685 - ye_logger_of_yor - INFO - main - Line 524
2023-05-03 17:51:26,282 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:51:26,282 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:51:26,282 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:51:26,309 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:51:26,309 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:51:26,309 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:51:26,309 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:51:26,310 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:51:26,651 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:51:26,652 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:51:26,652 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:51:26,653 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:51:26,653 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:51:26,653 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:51:26,653 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:51:26,654 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:51:26,654 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:51:26,654 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:51:26,654 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:51:26,655 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:51:26,656 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:51:26,657 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:51:26,657 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:51:26,657 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:51:26,657 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:51:26,657 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:51:26,658 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:51:26,658 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:51:26,659 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 17:51:26,659 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 17:51:26,659 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 17:51:26,660 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 17:51:26,660 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 17:51:26,660 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 17:51:26,661 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 17:51:26,661 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 17:51:37,198 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 17:51:47,899 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 17:51:47,899 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 17:51:47,899 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 17:51:47,924 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 17:51:47,924 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 17:51:47,924 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 17:51:47,924 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 17:51:47,925 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 17:51:48,255 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 17:51:48,256 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 17:51:48,256 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 17:51:48,256 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 17:51:48,256 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 17:51:48,256 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 17:51:48,257 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 17:51:48,258 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 17:51:48,259 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 17:51:48,259 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 17:51:48,259 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 17:51:48,259 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 17:51:48,259 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 17:51:48,260 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 17:51:48,261 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 17:51:48,262 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 17:51:52,371 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 18:19:11,928 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:19:11,940 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:19:11,940 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:19:11,986 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:19:11,987 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:19:11,988 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:19:11,988 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:19:12,474 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:19:12,475 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:19:12,475 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:19:12,560 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:19:12,561 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:19:12,562 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:19:12,563 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:19:12,564 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:19:12,565 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:19:12,566 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:19:12,566 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:22:46,992 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:22:46,992 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:22:46,992 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:22:47,018 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:22:47,018 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:22:47,018 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:22:47,019 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:22:47,020 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:22:47,319 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:22:47,319 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:22:47,319 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:22:47,320 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:22:47,321 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:22:47,322 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:22:47,323 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:22:47,324 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:22:47,324 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:22:47,324 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:22:47,325 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:22:47,325 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:22:47,325 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:22:47,325 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:22:47,325 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:25:39,148 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:25:39,148 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:25:39,148 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:25:39,168 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:25:39,169 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:25:39,169 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:25:39,169 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:25:39,169 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:25:39,170 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:25:39,170 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:25:39,170 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:25:39,170 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:25:39,170 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:25:39,466 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:25:39,466 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:25:39,467 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:25:39,467 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:25:39,467 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:25:39,467 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:25:39,468 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:25:39,468 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:25:39,468 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:25:39,468 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:25:39,469 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:25:39,470 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:25:39,471 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:25:39,472 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:25:39,473 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:25:39,473 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:26:17,888 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:26:17,888 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:26:17,889 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:26:17,911 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:26:17,911 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:26:17,911 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:26:17,911 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:26:17,911 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:26:17,912 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:26:17,912 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:26:17,912 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:26:17,912 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:26:17,913 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:26:18,195 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:26:18,196 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:26:18,196 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:26:18,197 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:26:18,197 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:26:18,197 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:26:18,197 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:26:18,197 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:26:18,198 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:26:18,199 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:26:18,200 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:26:18,201 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:26:18,202 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:26:18,202 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:26:18,202 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:26:42,441 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:26:42,441 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:26:42,441 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:26:42,463 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:26:42,464 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:26:42,464 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:26:42,464 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:26:42,464 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:26:42,464 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:26:42,465 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:26:42,465 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:26:42,465 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:26:42,465 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:26:42,752 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:26:42,753 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:26:42,753 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:26:42,753 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:26:42,754 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:26:42,755 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:26:42,756 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:26:42,757 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:26:42,758 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:26:42,758 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:26:42,758 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:26:42,758 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:26:42,758 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:26:42,759 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:26:42,759 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:26:42,759 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:30:13,926 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:30:13,942 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:30:13,942 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:30:13,969 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:30:13,970 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:30:13,970 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:30:13,970 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:30:13,970 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:30:13,971 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:30:13,971 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:30:13,971 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:30:13,971 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:30:13,971 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:30:14,301 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:30:14,302 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:30:14,303 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:30:14,304 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:30:14,305 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:30:14,306 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:30:14,306 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:30:14,306 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:30:14,306 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:30:14,307 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:36:01,471 - ye_logger_of_yor - INFO - creating mass embedding - Line 59
2023-05-03 18:36:30,615 - ye_logger_of_yor - INFO - creating mass embedding - Line 59
2023-05-03 18:36:30,616 - ye_logger_of_yor - INFO - creating embedding - Line 78
2023-05-03 18:36:30,616 - ye_logger_of_yor - INFO - checking file - Line 39
2023-05-03 18:46:50,052 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:46:50,052 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:46:50,052 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:46:50,071 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:46:50,072 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:46:50,072 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:46:50,072 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:46:50,072 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:46:50,072 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:46:50,073 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:46:50,073 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:46:50,073 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:46:50,073 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:46:50,391 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:46:50,392 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:46:50,392 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:46:50,393 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:46:50,393 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:46:50,393 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:46:50,393 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:46:50,393 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:46:50,394 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:46:50,395 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:46:50,395 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:46:50,395 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:46:50,395 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:46:50,395 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:46:50,396 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:46:50,396 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:46:50,396 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:46:50,396 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:46:50,396 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:46:50,397 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:46:50,397 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:46:50,397 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:46:50,397 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:46:50,397 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:46:50,398 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:46:50,398 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:46:50,398 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:46:50,398 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:46:50,398 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:46:50,399 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:46:50,399 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:46:50,399 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:46:50,399 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:47:04,990 - ye_logger_of_yor - INFO - creating mass embedding - Line 59
2023-05-03 18:47:04,990 - ye_logger_of_yor - INFO - creating embedding - Line 78
2023-05-03 18:47:04,991 - ye_logger_of_yor - INFO - checking file - Line 39
2023-05-03 18:47:41,814 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:47:41,815 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:47:41,815 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:47:41,871 - ye_logger_of_yor - INFO - Loading global variables - Line 22
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - base_formatter function - Line 30
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 36
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 56
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - create_embedding function - Line 75
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - load_embedding function - Line 90
2023-05-03 18:47:41,872 - ye_logger_of_yor - INFO - base_retriever function - Line 96
2023-05-03 18:47:41,873 - ye_logger_of_yor - INFO - retriever function - Line 104
2023-05-03 18:47:41,873 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 115
2023-05-03 18:47:41,873 - ye_logger_of_yor - INFO - memory_search function - Line 123
2023-05-03 18:47:42,132 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:47:42,132 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:47:42,133 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:47:42,134 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:47:42,135 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:47:42,136 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:47:42,137 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:47:42,137 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:47:42,137 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:47:42,137 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:47:42,138 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:47:42,138 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:47:42,138 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:47:42,138 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:47:42,138 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:47:56,655 - ye_logger_of_yor - INFO - creating mass embedding - Line 59
2023-05-03 18:47:56,656 - ye_logger_of_yor - INFO - creating embedding - Line 78
2023-05-03 18:47:56,656 - ye_logger_of_yor - INFO - checking file - Line 39
2023-05-03 18:51:38,600 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:51:38,600 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:51:38,600 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:51:38,625 - ye_logger_of_yor - INFO - Loading global variables - Line 23
2023-05-03 18:51:38,625 - ye_logger_of_yor - INFO - base_formatter function - Line 31
2023-05-03 18:51:38,625 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 37
2023-05-03 18:51:38,625 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 57
2023-05-03 18:51:38,625 - ye_logger_of_yor - INFO - create_embedding function - Line 76
2023-05-03 18:51:38,626 - ye_logger_of_yor - INFO - load_embedding function - Line 91
2023-05-03 18:51:38,626 - ye_logger_of_yor - INFO - base_retriever function - Line 97
2023-05-03 18:51:38,626 - ye_logger_of_yor - INFO - retriever function - Line 105
2023-05-03 18:51:38,626 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 116
2023-05-03 18:51:38,626 - ye_logger_of_yor - INFO - memory_search function - Line 124
2023-05-03 18:51:38,881 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:51:38,881 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:51:38,881 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:51:38,882 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:51:38,883 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:51:38,884 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:51:38,885 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:51:38,886 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:51:50,077 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:51:50,077 - ye_logger_of_yor - INFO - creating embedding - Line 79
2023-05-03 18:51:50,078 - ye_logger_of_yor - INFO - checking file - Line 40
2023-05-03 18:52:58,809 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:52:58,809 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:52:58,809 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:52:58,829 - ye_logger_of_yor - INFO - Loading global variables - Line 23
2023-05-03 18:52:58,829 - ye_logger_of_yor - INFO - base_formatter function - Line 31
2023-05-03 18:52:58,829 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 37
2023-05-03 18:52:58,829 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 57
2023-05-03 18:52:58,829 - ye_logger_of_yor - INFO - create_embedding function - Line 76
2023-05-03 18:52:58,830 - ye_logger_of_yor - INFO - load_embedding function - Line 91
2023-05-03 18:52:58,830 - ye_logger_of_yor - INFO - base_retriever function - Line 97
2023-05-03 18:52:58,830 - ye_logger_of_yor - INFO - retriever function - Line 105
2023-05-03 18:52:58,830 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 116
2023-05-03 18:52:58,830 - ye_logger_of_yor - INFO - memory_search function - Line 124
2023-05-03 18:52:59,099 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:52:59,099 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:52:59,099 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:52:59,100 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:52:59,101 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:52:59,102 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:52:59,103 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:52:59,104 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:52:59,105 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:53:36,765 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:53:36,766 - ye_logger_of_yor - INFO - creating embedding - Line 79
2023-05-03 18:53:36,766 - ye_logger_of_yor - INFO - checking file - Line 40
2023-05-03 18:54:14,553 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:54:14,553 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:54:14,554 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:54:14,578 - ye_logger_of_yor - INFO - Loading global variables - Line 23
2023-05-03 18:54:14,578 - ye_logger_of_yor - INFO - base_formatter function - Line 31
2023-05-03 18:54:14,578 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 37
2023-05-03 18:54:14,578 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 57
2023-05-03 18:54:14,578 - ye_logger_of_yor - INFO - create_embedding function - Line 76
2023-05-03 18:54:14,579 - ye_logger_of_yor - INFO - load_embedding function - Line 91
2023-05-03 18:54:14,579 - ye_logger_of_yor - INFO - base_retriever function - Line 97
2023-05-03 18:54:14,579 - ye_logger_of_yor - INFO - retriever function - Line 105
2023-05-03 18:54:14,579 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 116
2023-05-03 18:54:14,579 - ye_logger_of_yor - INFO - memory_search function - Line 124
2023-05-03 18:54:14,838 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:54:14,839 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:54:14,840 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:54:14,841 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:54:14,842 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:54:14,843 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:54:14,844 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:54:14,844 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:54:14,844 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:54:25,698 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:54:40,610 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:54:40,610 - ye_logger_of_yor - INFO - creating embedding - Line 79
2023-05-03 18:54:40,610 - ye_logger_of_yor - INFO - checking file - Line 40
2023-05-03 18:55:16,890 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:55:16,891 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:55:16,891 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:55:16,921 - ye_logger_of_yor - INFO - Loading global variables - Line 23
2023-05-03 18:55:16,921 - ye_logger_of_yor - INFO - base_formatter function - Line 31
2023-05-03 18:55:16,921 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 37
2023-05-03 18:55:16,921 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 57
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - create_embedding function - Line 76
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - load_embedding function - Line 91
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - base_retriever function - Line 97
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - retriever function - Line 105
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 116
2023-05-03 18:55:16,922 - ye_logger_of_yor - INFO - memory_search function - Line 124
2023-05-03 18:55:17,193 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:55:17,194 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:55:17,194 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:55:17,194 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:55:17,194 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:55:17,195 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:55:17,196 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:55:17,197 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:55:17,198 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:55:17,199 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:55:27,812 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:55:27,813 - ye_logger_of_yor - INFO - creating embedding - Line 79
2023-05-03 18:55:27,813 - ye_logger_of_yor - INFO - checking file - Line 40
2023-05-03 18:56:13,620 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 18:56:13,620 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 18:56:13,620 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 18:56:13,638 - ye_logger_of_yor - INFO - Loading global variables - Line 23
2023-05-03 18:56:13,638 - ye_logger_of_yor - INFO - base_formatter function - Line 31
2023-05-03 18:56:13,638 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 37
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 57
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - create_embedding function - Line 76
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - load_embedding function - Line 91
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - base_retriever function - Line 97
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - retriever function - Line 105
2023-05-03 18:56:13,639 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 116
2023-05-03 18:56:13,640 - ye_logger_of_yor - INFO - memory_search function - Line 124
2023-05-03 18:56:13,899 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 18:56:13,899 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 18:56:13,899 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 18:56:13,900 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 18:56:13,901 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 18:56:13,902 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 18:56:13,903 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 18:56:13,904 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 18:56:13,905 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 18:56:26,678 - ye_logger_of_yor - INFO - creating mass embedding - Line 60
2023-05-03 18:56:26,679 - ye_logger_of_yor - INFO - creating embedding - Line 79
2023-05-03 18:56:26,679 - ye_logger_of_yor - INFO - checking file - Line 40
2023-05-03 19:07:47,305 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:07:47,306 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:07:47,306 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:08:27,245 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:08:27,246 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:08:27,246 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:08:27,280 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:08:27,281 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:08:27,281 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:08:27,281 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:08:27,281 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:08:27,282 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:08:27,282 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:08:27,282 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:08:27,282 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:08:27,282 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:08:27,555 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:08:27,556 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:08:27,556 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:08:27,556 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:08:27,557 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:08:27,558 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:08:27,559 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:08:27,560 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:08:27,561 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:08:27,561 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:08:27,561 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:08:27,561 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:08:27,561 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:08:27,562 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:08:27,562 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:08:44,596 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:08:44,596 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:08:44,596 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:25:45,957 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:25:45,957 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:25:45,958 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:25:45,976 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:25:45,976 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:25:45,976 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:25:45,976 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:25:45,977 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:25:46,235 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:25:46,236 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:25:46,236 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:25:46,236 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:25:46,237 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:25:46,238 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:25:46,239 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:25:46,299 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:25:46,500 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:25:46,500 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:25:46,500 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:25:46,500 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:25:46,501 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:25:46,501 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:25:46,501 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:25:46,778 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:25:46,778 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:25:46,779 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:25:46,779 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:25:46,779 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:25:46,914 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:25:59,255 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:25:59,256 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:25:59,256 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:41:27,397 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:41:27,397 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:41:27,397 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:41:28,389 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:41:28,390 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:41:28,391 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:41:28,391 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:41:28,595 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:41:28,596 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:41:28,596 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:41:28,596 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:41:28,596 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:41:28,596 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:41:28,597 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:41:28,598 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:41:28,599 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:41:28,600 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:41:28,601 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:41:28,601 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:41:28,601 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:41:28,601 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:41:28,601 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:41:41,746 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:41:41,747 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:41:41,747 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:44:53,671 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:44:53,671 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:44:53,671 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:44:54,638 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:44:54,638 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:44:54,638 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:44:54,638 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:44:54,638 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:44:54,639 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:44:54,639 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:44:54,639 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:44:54,639 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:44:54,639 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:44:54,847 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:44:54,848 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:44:54,848 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:44:54,848 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:44:54,848 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:44:54,848 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:44:54,849 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:44:54,850 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:44:54,851 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:44:54,852 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:44:54,853 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:45:11,597 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:45:11,598 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:45:11,598 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:45:43,546 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:45:43,547 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:45:43,547 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:45:44,589 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:45:44,589 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:45:44,590 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:45:44,591 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:45:44,591 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:45:44,822 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:45:44,822 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:45:44,823 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:45:44,823 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:45:44,823 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:45:44,823 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:45:44,823 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:45:44,824 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:45:44,825 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:45:44,826 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:45:44,827 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:45:44,828 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:45:44,828 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:45:44,828 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:45:44,828 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:45:44,829 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:46:11,994 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:46:11,995 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:46:11,995 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:47:21,746 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:47:21,746 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:47:21,746 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:47:22,723 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:47:22,724 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:47:22,725 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:47:22,725 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:47:22,725 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:47:23,047 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:47:23,048 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:47:23,049 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:47:23,050 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:47:23,051 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:47:23,052 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:47:23,052 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:47:23,052 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:47:23,052 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:47:23,052 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:47:23,053 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:47:23,053 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:47:23,053 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:47:23,053 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:47:23,053 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:47:23,054 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:47:23,054 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:47:41,651 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:47:41,652 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:47:41,652 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:50:33,239 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:50:33,239 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:50:33,239 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:50:34,244 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:50:34,245 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:50:34,245 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:50:34,245 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:50:34,245 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:50:34,246 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:50:34,246 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:50:34,246 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:50:34,246 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:50:34,246 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:50:34,455 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:50:34,456 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:50:34,457 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:50:34,458 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:50:34,459 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:50:34,460 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:50:34,461 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:50:34,461 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:50:34,461 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:50:34,461 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:50:49,718 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 19:50:49,719 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 19:50:49,719 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 19:51:00,452 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 19:51:00,453 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 19:51:00,453 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 19:51:01,443 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 19:51:01,444 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 19:51:01,445 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 19:51:01,445 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 19:51:01,445 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 19:51:01,659 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 19:51:01,659 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 19:51:01,659 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 19:51:01,660 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 19:51:01,661 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 19:51:01,662 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 19:51:01,663 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 19:51:01,663 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 19:51:01,663 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 19:51:01,663 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 19:51:01,663 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 19:51:01,664 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 19:51:01,664 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 19:51:01,665 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 19:51:01,666 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 19:53:40,624 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 21:24:48,293 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:24:48,293 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:24:48,293 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:25:52,607 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:25:52,607 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:25:52,607 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:26:14,241 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 21:26:14,242 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 21:26:14,242 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 21:26:14,242 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 21:26:14,242 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 21:26:14,242 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 21:26:14,243 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 21:26:14,243 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 21:26:14,243 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 21:26:14,243 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 21:26:15,954 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 21:26:15,954 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 21:26:15,955 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 21:26:15,982 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 21:26:15,983 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 21:26:15,984 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 21:26:15,985 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 21:26:15,986 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 21:26:15,987 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 21:26:15,987 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 21:26:15,987 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 21:26:15,987 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 21:26:15,987 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 21:26:15,988 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 21:26:15,988 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 21:26:15,988 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 21:26:45,400 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:26:45,400 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:26:45,401 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:26:46,374 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 21:26:46,374 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 21:26:46,374 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 21:26:46,374 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 21:26:46,375 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 21:26:46,581 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 21:26:46,582 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 21:26:46,582 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 21:26:46,582 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 21:26:46,583 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 21:26:46,584 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 21:26:46,585 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 21:26:46,586 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 21:26:46,587 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 21:26:46,588 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 21:27:01,960 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 21:27:01,960 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:27:01,961 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:49:08,874 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:49:08,874 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:49:08,874 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:49:09,844 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 21:49:09,844 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 21:49:09,844 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 21:49:09,844 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 21:49:09,845 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 21:49:10,054 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 21:49:10,055 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 21:49:10,055 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 21:49:10,055 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 21:49:10,056 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 21:49:10,057 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 21:49:10,058 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 21:49:10,059 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 21:49:10,060 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 21:49:10,061 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 21:49:27,752 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 21:49:27,752 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:49:27,753 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:52:14,741 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:52:14,742 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:52:14,742 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:52:15,812 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 21:52:15,812 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 21:52:15,812 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 21:52:15,812 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 21:52:15,813 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 21:52:16,029 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 21:52:16,030 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 21:52:16,030 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 21:52:16,030 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 21:52:16,031 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 21:52:16,032 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 21:52:16,033 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 21:52:16,034 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 21:52:16,035 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 21:52:16,036 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 21:52:16,036 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 21:52:27,998 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 21:52:40,013 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 21:52:54,811 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 21:53:12,813 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 21:53:12,813 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:53:12,814 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:55:31,008 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 21:55:31,008 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 21:55:31,008 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 21:55:32,040 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 21:55:32,040 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 21:55:32,040 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 21:55:32,041 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 21:55:32,042 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 21:55:32,251 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 21:55:32,252 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 21:55:32,252 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 21:55:32,252 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 21:55:32,253 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 21:55:32,254 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 21:55:32,255 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 21:55:32,256 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 21:55:32,257 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 21:56:19,940 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:19,940 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:20,438 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport os\nimport openai\n\nimport json\n\nfrom dotenv import load_dotenv\nfrom langchain.utilities import GoogleSerperAPIWrapper, GoogleSearchAPIWrapper\nfrom langchain.agents import initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom ye_logger_of_yor import get_logger\n\nlogger = get_logger()\n\nprint("loading HexAmerous")\n\nLoad environment variables\n\nload_dotenv()\n\nInitialize OpenAI\n\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\nInitialize variables\n\nprint("Welcome to HexAmerous your coding assistant")\n\nselected_model = "gpt-3.5-turbo"\n\nlogger.info(\'change_selected_model\')\ndef change_selected_model(model):\n    selected_model = model\n    return selected_model\n\ncall openai chat api\n\ncontext = [{\n        "role": "system",\n        "content":"You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you\'re helping him develop his program."\n        },\n        {\n        "role": "user",\n        "content": "Hi there Meg, how are you today? I hope you\'re learning lots about the world."\n        },\n        {\n        "role": "assistant",\n        "content": "I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it."\n        },\n        {\n        "role": "user",\n        "content": "I\'m glad to hear that, and dont you worry I\'ll figure out how to give you temporal context soon enough."\n        },\n        {\n        "role": "assistant",\n        "content": "I am glad to hear that. What can I help you with today?"\n        },\n]\nlogger.info(\'loading chat_gpt\')\ndef chat_gpt(user_message):\n    global context\n    global selected_model\n    context_string = context\n    logger.info(context_string)\n\nlogger.info(\'loading search_gpt\')\ndef search_gpt(user_query, prompt):\n    global selected_model\n\n```', metadata={'source': 'embeddings\\chatgpt.md'})] - Line 53
2023-05-03 21:56:21,928 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:21,928 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:21,973 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom langchain.document_loaders import(\n    TextLoader,\n    PyPDFLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredFileLoader,\n    PDFMinerLoader\n)\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.text_splitter import CharacterTextSplitter\nimport nltk\nimport openai\nfrom dotenv import load_dotenv\nimport os\nfrom chatgpt import search_gpt\nfrom ye_logger_of_yor import get_logger\n\nlogger = get_logger()\n\nload_dotenv()\n\nlogger.info(\'Loading global variables\')\n\nLoad Langchain variables\n\nopenai.api_key = os.getenv("OPENAI_API_KEY")\nembeddings = OpenAIEmbeddings()\nllm = OpenAI(temperature=0)\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=25)\nvectorstore = \'docs/\'\n\nlogger.info(\'base_formatter function\')\ndef base_formatter(docs):\n    logger.info(\'formatting\')\n    print(f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n    return (f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n\nlogger.info(\'loading check_file function 43\')\n\nCheck if the files are valid\n\ndef check_file(file_path):\n    logger.info(\'checking file\')\n    if file_path.endswith(\'.txt\'):\n        loader = TextLoader(file_path)\n        print(loader.load())\n        return loader.load()\n    if file_path.endswith(\'.pdf\'):\n        loader = UnstructuredFileLoader(file_path, mode=\'elements\', strategy= \'hi_res\')\n        print("pdf file loaded")\n        return loader.load()\n    if file_path.endswith(\'.md\'):\n        loader = UnstructuredMarkdownLoader(file_path)\n        logger.info(loader.load())\n        return loader.load()\n    else:\n        print("File type not supported")\n        return "File type not supported"\n\nlogger.info(\'loading create_mass_embedding function\')\n\nLoop files in a folder path for embedding\n\ndef create_mass_embedding(folder_path):\n    logger.info(\'creating mass embedding\')\n    if not os.path.exists(folder_path):\n        folder_path = \'docs/empty\'\n        result = "Folder does not exist"\n        return result\n    for filename in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, filename)\n        result = create_embedding(file_path, filename)\n        print(f"Embedding created for {filename}: {result}")\n        with open(\'docs/index.txt\', \'a\') as f:\n            f.write(f"{os.path.join(folder_path, file_path)}\\n")\n        logger.info(f"Embedding created for {filename}: {result}")\n\nlogger.info(\'create_embedding function\')\n\nEmbed a single embedding\n\ndef create_embedding(file_path, optional_arg="metadata"):\n    logger.info(\'creating embedding\')\n    data =check_file(file_path)\n    metadata = optional_arg\n    if metadata:\n        meta = metadata\n    else:\n        meta = \'file_path\'\n    vectordb = Chroma.from_documents(documents=data, metadata=meta, embedding=embeddings, persist_directory=\'docs/\')\n    vectordb.persist()\n    return "Embedding created"\n\nLoad vectorstore database\n\nlogger.info(\'load_embedding function\')\ndef load_embedding():\n    logger.info(\'loading embedding\')\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    return chromadb\n\nlogger.info(\'base_retriever function\')\n\nSearch for uncompressed docs in database\n\ndef base_retriever(user_query):\n    logger.info(\'running base_retriever\')\n    retriever = load_embedding().as_retriever(llm=llm)\n    docs = retriever.get_relevant_documents(user_query)\n    return docs\n\nlogger.info(\'retriever function\')\n\nSearch for compressed docs in database\n\ndef retriever(user_query):\n    logger.info(\'running retriever\')\n    compressor = LLMChainExtractor.from_llm(llm)\n    retriever = load_embedding().as_retriever(llm=llm)\n    cc_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n    compressed_docs = cc_retriever.get_relevant_documents(user_query)\n    docs = compressed_docs\n    return docs\n\nlogger.info(\'load_vector_store_docs function\')\ndef load_vector_store_docs():\n    logger.info(\'running load_vector_store_docs\')\n    vectorstore = \'docs/index\'\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    docs = chromadb.documents\n    return docs\n\nlogger.info(\'memory_search function\')\n\nQuery the database and pass the info to chatgpt for response\n\ndef memory_search(user_query):\n    logger.info(\'running memory_search\')\n    data = base_retriever(user_query)\n    prompt = [{\n        "role":"system",\n        "content":\'\'\'\n        "The user has asked this question:\n\n```', metadata={'source': 'embeddings\\embeddings.md'})] - Line 53
2023-05-03 21:56:23,161 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:23,161 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:23,169 - ye_logger_of_yor - INFO - [Document(page_content="```python\nimport os\nimport glob\nfrom embeddings import create_embedding\n\ndef run_embed_project(file_path):\n    project_folder = file_path\n    output_folder = 'embeddings'\n    # Get all .py files in the project folder and its subdirectories\n    project_files = glob.glob(os.path.join(project_folder, '*/.py'), recursive=True)\n\n```", metadata={'source': 'embeddings\\embed_project.md'})] - Line 53
2023-05-03 21:56:24,225 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:24,225 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:24,254 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport sys\nimport os\nimport random\nfrom PyQt5.QtCore import Qt\nfrom PyQt5.QtGui import (\n    QTextCursor,\n    QPixmap,\n    QPalette,\n    QBrush,\n    QMovie,\n    QIcon,\n    QImage\n)\nfrom PyQt5.QtWidgets import (\n    QApplication,\n    QWidget,\n    QVBoxLayout,\n    QHBoxLayout,\n    QScrollArea,\n    QFrame,\n    QLabel,\n    QTextEdit,\n    QPushButton,\n    QDialog,\n    QFileDialog,\n    QMessageBox,\n    QComboBox\n)\nfrom chatgpt import (\n    chat_gpt,\n    change_selected_model\n)\nfrom embeddings import (\n    create_embedding,\n    base_retriever,\n    retriever,\n    create_mass_embedding,\n    memory_search\n)\nfrom langchain import OpenAI\nfrom langchain.utilities import GoogleSerperAPIWrapper, GoogleSearchAPIWrapper\nfrom langchain.agents import initialize_agent, load_tools\nfrom scrappy import scrape_site, scrape_site_map\nfrom embed_project import run_embed_project\nfrom ye_logger_of_yor import get_logger\n\nlogger = get_logger()\n\nGlobal Variables\n\nlogger.info(\'loading langchain variables\')\n\nllm = OpenAI(temperature=0)\ntools = load_tools(["google-serper"], llm=llm)\nagent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)\n\nText Edit Widget\n\nlogger.info(\'CustomTextEdit\')\nclass CustomTextEdit(QTextEdit):\n    def init(self, args, kwargs):\n        super(CustomTextEdit, self).init(args, **kwargs)\n\nChat Widget\n\nlogger.info(\'loading chatwidget\')\nclass ChatWidget(QWidget):\n    def init(self, parent=None):\n        super().init(parent)\n        self.init_ui()\n\nlogger.info(\'Scroll Area\')\nclass ScrollArea(QScrollArea):\n    def init(self, parent=None):\n        super().init(parent)\n\nlogger.info(\'Main Window\')\nclass MainWindow(QWidget):\n    def init(self, parent=None):\n        super().init(parent)\n        self.setWindowTitle("HexAmerous - AI Assistant")\n        self.resize(700, 700)\n        self.layout = QVBoxLayout()\n        self.setLayout(self.layout)\n        self.scroll_area = ScrollArea()\n        self.layout.addWidget(self.scroll_area)\n        self.background_image = self.change_background_image(text=None)\n\nlogger.info(\'Large Text Input Dialog\')\nclass LargeTextInputDialog(QDialog):\n    def init(self, parent=None):\n        super().init(parent)\n        self.setWindowTitle("Large Text Input")\n        self.resize(400, 600)\n\n------------- Main Program --------------\n\nlogger.info(\'main\')\ndef main():\n    app = QApplication(sys.argv)\n    main_window = MainWindow()\n    icon = QIcon("img/favicon.ico")\n    app.setWindowIcon(QIcon(\'img/favicon.ico\'))\n    app.setWindowIcon(icon)\n    main_window.show()\n    sys.exit(app.exec_())\n\nif name == "main":\n    main()\n```', metadata={'source': 'embeddings\\HexAmerous.md'})] - Line 53
2023-05-03 21:56:25,517 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:25,518 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:25,527 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom langchain.document_loaders import PlaywrightURLLoader, SitemapLoader\nfrom langchain.text_splitter import TextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom embeddings import create_embedding, load_embedding\nfrom langchain.text_splitter import TokenTextSplitter\nfrom langchain.document_loaders.sitemap import SitemapLoader\nfrom embeddings import create_embedding\nimport nest_asyncio\n\nLoad Langchain variables\n\nembeddings = OpenAIEmbeddings()\ntext_splitter = TokenTextSplitter(chunk_size=300, chunk_overlap=25)\n\nScrape a website\n\ndef scrape_site(url):\n\ndef scrape_site_map(site_path, collection_name):\n\n```', metadata={'source': 'embeddings\\scrappy.md'})] - Line 53
2023-05-03 21:56:26,660 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 21:56:26,661 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 21:56:26,669 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport logging\nfrom logging.handlers import TimedRotatingFileHandler\nimport os\n\nclass YeLoggerOfYor(object):\n    def init(self, log_file_name):\n        self.log_file_name = log_file_name\n        self.logger = logging.getLogger(name)\n\nmy_logger = YeLoggerOfYor("log/log-of-yore.log")\ndef get_logger():\n    return my_logger.logger\n```', metadata={'source': 'embeddings\\ye_logger_of_yor.md'})] - Line 53
2023-05-03 22:01:59,221 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:01:59,221 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:01:59,232 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base class for all language models."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n\ndef _get_num_tokens_default_method(text: str) -> int:\n    """Get the number of tokens present in the text."""\n    # TODO: this method may not be exact.\n    # TODO: this method may differ based on model (eg codex).\n    try:\n        from transformers import GPT2TokenizerFast\n    except ImportError:\n        raise ValueError(\n            "Could not import transformers python package. "\n            "This is needed in order to calculate get_num_tokens. "\n            "Please install it with pip install transformers."\n        )\n    # create a GPT-2 tokenizer instance\n    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")\n\nclass BaseLanguageModel(BaseModel, ABC):\n    @abstractmethod\n    def generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -> LLMResult:\n        """Take in a list of prompt values and return an LLMResult."""\n\n```', metadata={'source': 'embeddings\\base_language.md'})] - Line 53
2023-05-03 22:02:01,068 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:01,068 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:01,090 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Beta Feature: base interface for cache."""\nimport hashlib\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, cast\n\nfrom sqlalchemy import Column, Integer, String, create_engine, select\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.orm import Session\n\ntry:\n    from sqlalchemy.orm import declarative_base\nexcept ImportError:\n    from sqlalchemy.ext.declarative import declarative_base\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.schema import Generation\nfrom langchain.vectorstores.redis import Redis as RedisVectorstore\n\nRETURN_VAL_TYPE = List[Generation]\n\ndef _hash(_input: str) -> str:\n    """Use a deterministic hashing approach."""\n    return hashlib.md5(_input.encode()).hexdigest()\n\nclass BaseCache(ABC):\n    """Base interface for cache."""\n\nclass InMemoryCache(BaseCache):\n    """Cache that stores things in memory."""\n\nBase = declarative_base()\n\nclass FullLLMCache(Base):  # type: ignore\n    """SQLite table for full LLM Cache (all generations)."""\n\nclass SQLAlchemyCache(BaseCache):\n    """Cache that uses SQAlchemy as a backend."""\n\nclass SQLiteCache(SQLAlchemyCache):\n    """Cache that uses SQLite as a backend."""\n\nclass RedisCache(BaseCache):\n    """Cache that uses Redis as a backend."""\n\nclass RedisSemanticCache(BaseCache):\n    """Cache that uses Redis as a vector-store backend."""\n\nclass GPTCache(BaseCache):\n    """Cache that uses GPTCache as a backend."""\n\n```', metadata={'source': 'embeddings\\cache.md'})] - Line 53
2023-05-03 22:02:02,442 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:02,442 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:02,465 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Transform documents"""\nfrom typing import Any, Callable, List, Sequence\n\nimport numpy as np\nfrom pydantic import BaseModel, Field\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.math_utils import cosine_similarity\nfrom langchain.schema import BaseDocumentTransformer, Document\n\nclass _DocumentWithState(Document):\n    """Wrapper for a document that includes arbitrary state."""\n\ndef get_stateful_documents(\n    documents: Sequence[Document],\n) -> Sequence[_DocumentWithState]:\n    return [_DocumentWithState.from_document(doc) for doc in documents]\n\ndef _filter_similar_embeddings(\n    embedded_documents: List[List[float]], similarity_fn: Callable, threshold: float\n) -> List[int]:\n    """Filter redundant documents based on the similarity of their embeddings."""\n    similarity = np.tril(similarity_fn(embedded_documents, embedded_documents), k=-1)\n    redundant = np.where(similarity > threshold)\n    redundant_stacked = np.column_stack(redundant)\n    redundant_sorted = np.argsort(similarity[redundant])[::-1]\n    included_idxs = set(range(len(embedded_documents)))\n    for first_idx, second_idx in redundant_stacked[redundant_sorted]:\n        if first_idx in included_idxs and second_idx in included_idxs:\n            # Default to dropping the second document of any highly similar pair.\n            included_idxs.remove(second_idx)\n    return list(sorted(included_idxs))\n\ndef _get_embeddings_from_stateful_docs(\n    embeddings: Embeddings, documents: Sequence[_DocumentWithState]\n) -> List[List[float]]:\n    if len(documents) and "embedded_doc" in documents[0].state:\n        embedded_documents = [doc.state["embedded_doc"] for doc in documents]\n    else:\n        embedded_documents = embeddings.embed_documents(\n            [d.page_content for d in documents]\n        )\n        for doc, embedding in zip(documents, embedded_documents):\n            doc.state["embedded_doc"] = embedding\n    return embedded_documents\n\nclass EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n    """Filter that drops redundant documents by comparing their embeddings."""\n\n```', metadata={'source': 'embeddings\\document_transformers.md'})] - Line 53
2023-05-03 22:02:03,830 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:03,830 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:03,837 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utility functions for working with prompts."""\nfrom typing import List\n\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\n\nTEST_GEN_TEMPLATE_SUFFIX = "Add another example."\n\ndef generate_example(\n    examples: List[dict], llm: BaseLLM, prompt_template: PromptTemplate\n) -> str:\n    """Return another example given a list of examples for a prompt."""\n    prompt = FewShotPromptTemplate(\n        examples=examples,\n        suffix=TEST_GEN_TEMPLATE_SUFFIX,\n        input_variables=[],\n        example_prompt=prompt_template,\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    return chain.predict()\n\n```', metadata={'source': 'embeddings\\example_generator.md'})] - Line 53
2023-05-03 22:02:05,049 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:05,049 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:05,056 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utilities for formatting strings."""\nfrom string import Formatter\nfrom typing import Any, List, Mapping, Sequence, Union\n\nclass StrictFormatter(Formatter):\n    """A subclass of formatter that checks for extra keys."""\n\nformatter = StrictFormatter()\n\n```', metadata={'source': 'embeddings\\formatting.md'})] - Line 53
2023-05-03 22:02:06,651 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:06,651 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:06,657 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Handle chained inputs."""\nfrom typing import Dict, List, Optional\n\n_TEXT_COLOR_MAPPING = {\n    "blue": "36;1",\n    "yellow": "33;1",\n    "pink": "38;5;200",\n    "green": "32;1",\n    "red": "31;1",\n}\n\ndef get_color_mapping(\n    items: List[str], excluded_colors: Optional[List] = None\n) -> Dict[str, str]:\n    """Get mapping for items to a support color."""\n    colors = list(_TEXT_COLOR_MAPPING.keys())\n    if excluded_colors is not None:\n        colors = [c for c in colors if c not in excluded_colors]\n    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}\n    return color_mapping\n\ndef get_colored_text(text: str, color: str) -> str:\n    """Get colored text."""\n    color_str = _TEXT_COLOR_MAPPING[color]\n    return f"\\u001b[{color_str}m\\033[1;3m{text}\\u001b[0m"\n\ndef print_text(text: str, color: Optional[str] = None, end: str = "") -> None:\n    """Print text with highlighting and no end characters."""\n    if color is None:\n        text_to_print = text\n    else:\n        text_to_print = get_colored_text(text, color)\n    print(text_to_print, end=end)\n\n```', metadata={'source': 'embeddings\\input.md'})] - Line 53
2023-05-03 22:02:08,149 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:08,149 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:08,154 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Math utils."""\nfrom typing import List, Union\n\nimport numpy as np\n\nMatrix = Union[List[List[float]], List[np.ndarray], np.ndarray]\n\ndef cosine_similarity(X: Matrix, Y: Matrix) -> np.ndarray:\n    """Row-wise cosine similarity between two equal-width matrices."""\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f"Number of columns in X and Y must be the same. X has shape {X.shape} "\n            f"and Y has shape {Y.shape}."\n        )\n\n```', metadata={'source': 'embeddings\\math_utils.md'})] - Line 53
2023-05-03 22:02:09,689 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:09,690 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:09,702 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Experiment with different models."""\nfrom future import annotations\n\nfrom typing import List, Optional, Sequence\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping, print_text\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.prompt import PromptTemplate\n\nclass ModelLaboratory:\n    """Experiment with different models."""\n\n```', metadata={'source': 'embeddings\\model_laboratory.md'})] - Line 53
2023-05-03 22:02:11,686 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:11,687 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:11,691 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatibility."""\nfrom langchain.utilities.python import PythonREPL\n\nall = ["PythonREPL"]\n\n```', metadata={'source': 'embeddings\\python.md'})] - Line 53
2023-05-03 22:02:13,042 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:13,042 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:13,052 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Lightweight wrapper around requests library, with async support."""\nfrom contextlib import asynccontextmanager\nfrom typing import Any, AsyncGenerator, Dict, Optional\n\nimport aiohttp\nimport requests\nfrom pydantic import BaseModel, Extra\n\nclass Requests(BaseModel):\n    """Wrapper around requests to handle auth and async.\n\nclass TextRequestsWrapper(BaseModel):\n    """Lightweight wrapper around requests library.\n\nFor backwards compatibility\n\nRequestsWrapper = TextRequestsWrapper\n\n```', metadata={'source': 'embeddings\\requests.md'})] - Line 53
2023-05-03 22:02:14,400 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:14,401 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:14,448 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Common schema objects."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, Extra, Field, root_validator\n\ndef get_buffer_string(\n    messages: List[BaseMessage], human_prefix: str = "Human", ai_prefix: str = "AI"\n) -> str:\n    """Get buffer string of messages."""\n    string_messages = []\n    for m in messages:\n        if isinstance(m, HumanMessage):\n            role = human_prefix\n        elif isinstance(m, AIMessage):\n            role = ai_prefix\n        elif isinstance(m, SystemMessage):\n            role = "System"\n        elif isinstance(m, ChatMessage):\n            role = m.role\n        else:\n            raise ValueError(f"Got unsupported message type: {m}")\n        string_messages.append(f"{role}: {m.content}")\n    return "\\n".join(string_messages)\n\nclass AgentAction(NamedTuple):\n    """Agent\'s action to take."""\n\nclass AgentFinish(NamedTuple):\n    """Agent\'s return value."""\n\nclass Generation(BaseModel):\n    """Output of a single generation."""\n\nclass BaseMessage(BaseModel):\n    """Message object."""\n\nclass HumanMessage(BaseMessage):\n    """Type of message that is spoken by the human."""\n\nclass AIMessage(BaseMessage):\n    """Type of message that is spoken by the AI."""\n\nclass SystemMessage(BaseMessage):\n    """Type of message that is a system message."""\n\nclass ChatMessage(BaseMessage):\n    """Type of message with arbitrary speaker."""\n\ndef _message_to_dict(message: BaseMessage) -> dict:\n    return {"type": message.type, "data": message.dict()}\n\ndef messages_to_dict(messages: List[BaseMessage]) -> List[dict]:\n    return [_message_to_dict(m) for m in messages]\n\ndef _message_from_dict(message: dict) -> BaseMessage:\n    _type = message["type"]\n    if _type == "human":\n        return HumanMessage(message["data"])\n    elif _type == "ai":\n        return AIMessage(message["data"])\n    elif _type == "system":\n        return SystemMessage(message["data"])\n    elif _type == "chat":\n        return ChatMessage(message["data"])\n    else:\n        raise ValueError(f"Got unexpected type: {_type}")\n\ndef messages_from_dict(messages: List[dict]) -> List[BaseMessage]:\n    return [_message_from_dict(m) for m in messages]\n\nclass ChatGeneration(Generation):\n    """Output of a single generation."""\n\nclass ChatResult(BaseModel):\n    """Class that contains all relevant information for a Chat Result."""\n\nclass LLMResult(BaseModel):\n    """Class that contains all relevant information for an LLM Result."""\n\nclass PromptValue(BaseModel, ABC):\n    @abstractmethod\n    def to_string(self) -> str:\n        """Return prompt as string."""\n\nclass BaseMemory(BaseModel, ABC):\n    """Base interface for memory in chains."""\n\nclass BaseChatMessageHistory(ABC):\n    """Base interface for chat message history\n    See ChatMessageHistory for default implementation.\n    """\n\nclass Document(BaseModel):\n    """Interface for interacting with a document."""\n\nclass BaseRetriever(ABC):\n    @abstractmethod\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        """Get documents relevant for a query.\n\nFor backwards compatibility\n\nMemory = BaseMemory\n\nT = TypeVar("T")\n\nclass BaseOutputParser(BaseModel, ABC, Generic[T]):\n    """Class to parse the output of an LLM call.\n\nclass OutputParserException(Exception):\n    """Exception that output parsers should raise to signify a parsing error.\n\nclass BaseDocumentTransformer(ABC):\n    """Base interface for transforming documents."""\n\n```', metadata={'source': 'embeddings\\schema.md'})] - Line 53
2023-05-03 22:02:16,178 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:16,178 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:16,182 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatiblity."""\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nall = ["SerpAPIWrapper"]\n\n```', metadata={'source': 'embeddings\\serpapi.md'})] - Line 53
2023-05-03 22:02:17,815 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:17,815 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:17,820 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Script to run langchain-server locally using docker-compose."""\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ndef main() -> None:\n    """Run the langchain server locally."""\n    p = Path(file).absolute().parent / "docker-compose.yaml"\n\nif name == "main":\n    main()\n\n```', metadata={'source': 'embeddings\\server.md'})] - Line 53
2023-05-03 22:02:19,237 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:19,237 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:19,249 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQLAlchemy wrapper around a database."""\nfrom future import annotations\n\nimport warnings\nfrom typing import Any, Iterable, List, Optional\n\nfrom sqlalchemy import MetaData, Table, create_engine, inspect, select, text\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.exc import ProgrammingError, SQLAlchemyError\nfrom sqlalchemy.schema import CreateTable\n\ndef _format_index(index: dict) -> str:\n    return (\n        f\'Name: {index["name"]}, Unique: {index["unique"]},\'\n        f\' Columns: {str(index["column_names"])}\'\n    )\n\nclass SQLDatabase:\n    """SQLAlchemy wrapper around a database."""\n\n```', metadata={'source': 'embeddings\\sql_database.md'})] - Line 53
2023-05-03 22:02:21,285 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:21,286 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:21,311 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for splitting text."""\nfrom future import annotations\n\nimport copy\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Union,\n)\n\nfrom langchain.docstore.document import Document\nfrom langchain.schema import BaseDocumentTransformer\n\nlogger = logging.getLogger(name)\n\nclass TextSplitter(BaseDocumentTransformer, ABC):\n    """Interface for splitting text into chunks."""\n\nclass CharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters."""\n\nclass TokenTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at tokens."""\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters.\n\nclass NLTKTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using NLTK."""\n\nclass SpacyTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using Spacy."""\n\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Markdown-formatted headings."""\n\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Latex-formatted layout elements."""\n\nclass PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Python syntax."""\n\n```', metadata={'source': 'embeddings\\text_splitter.md'})] - Line 53
2023-05-03 22:02:22,604 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:22,604 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:22,616 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Generic utility functions."""\nimport os\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\ndef get_from_dict_or_env(\n    data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None\n) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if key in data and data[key]:\n        return data[key]\n    else:\n        return get_from_env(key, env_key, default=default)\n\ndef get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if env_key in os.environ and os.environ[env_key]:\n        return os.environ[env_key]\n    elif default is not None:\n        return default\n    else:\n        raise ValueError(\n            f"Did not find {key}, please add an environment variable"\n            f" {env_key} which contains it, or pass"\n            f"  {key} as a named parameter."\n        )\n\ndef xor_args(*arg_groups: Tuple[str, ...]) -> Callable:\n    """Validate specified keyword args are mutually exclusive."""\n\ndef stringify_value(val: Any) -> str:\n    if isinstance(val, str):\n        return val\n    elif isinstance(val, dict):\n        return "\\n" + stringify_dict(val)\n    elif isinstance(val, list):\n        return "\\n".join(stringify_value(v) for v in val)\n    else:\n        return str(val)\n\ndef stringify_dict(data: dict) -> str:\n    text = ""\n    for key, value in data.items():\n        text += key + ": " + stringify_value(value) + "\\n"\n    return text\n\n```', metadata={'source': 'embeddings\\utils.md'})] - Line 53
2023-05-03 22:02:23,670 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:23,670 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:23,693 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Main entrypoint into package."""\n\nfrom importlib import metadata\nfrom typing import Optional\n\nfrom langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\nfrom langchain.cache import BaseCache\nfrom langchain.chains import (\n    ConversationChain,\n    LLMBashChain,\n    LLMChain,\n    LLMCheckerChain,\n    LLMMathChain,\n    PALChain,\n    QAWithSourcesChain,\n    SQLDatabaseChain,\n    VectorDBQA,\n    VectorDBQAWithSourcesChain,\n)\nfrom langchain.docstore import InMemoryDocstore, Wikipedia\nfrom langchain.llms import (\n    Anthropic,\n    Banana,\n    CerebriumAI,\n    Cohere,\n    ForefrontAI,\n    GooseAI,\n    HuggingFaceHub,\n    LlamaCpp,\n    Modal,\n    OpenAI,\n    Petals,\n    PipelineAI,\n    SagemakerEndpoint,\n    StochasticAI,\n    Writer,\n)\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import (\n    BasePromptTemplate,\n    FewShotPromptTemplate,\n    Prompt,\n    PromptTemplate,\n)\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.powerbi import PowerBIDataset\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom langchain.vectorstores import FAISS, ElasticVectorSearch\n\ntry:\n    version = metadata.version(package)\nexcept metadata.PackageNotFoundError:\n    # Case where package metadata is not available.\n    version = ""\ndel metadata  # optional, avoids polluting the results of dir(package)\n\nverbose: bool = False\nllm_cache: Optional[BaseCache] = None\n\nFor backwards compatibility\n\nSerpAPIChain = SerpAPIWrapper\n\nall = [\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMMathChain",\n    "ArxivAPIWrapper",\n    "SelfAskWithSearchChain",\n    "SerpAPIWrapper",\n    "SerpAPIChain",\n    "SearxSearchWrapper",\n    "GoogleSearchAPIWrapper",\n    "GoogleSerperAPIWrapper",\n    "WolframAlphaAPIWrapper",\n    "WikipediaAPIWrapper",\n    "Anthropic",\n    "Banana",\n    "CerebriumAI",\n    "Cohere",\n    "ForefrontAI",\n    "GooseAI",\n    "Modal",\n    "OpenAI",\n    "Petals",\n    "PipelineAI",\n    "StochasticAI",\n    "Writer",\n    "BasePromptTemplate",\n    "Prompt",\n    "FewShotPromptTemplate",\n    "PromptTemplate",\n    "ReActChain",\n    "Wikipedia",\n    "HuggingFaceHub",\n    "SagemakerEndpoint",\n    "HuggingFacePipeline",\n    "SQLDatabase",\n    "SQLDatabaseChain",\n    "PowerBIDataset",\n    "FAISS",\n    "MRKLChain",\n    "VectorDBQA",\n    "ElasticVectorSearch",\n    "InMemoryDocstore",\n    "ConversationChain",\n    "VectorDBQAWithSourcesChain",\n    "QAWithSourcesChain",\n    "PALChain",\n    "LlamaCpp",\n]\n\n```', metadata={'source': 'embeddings\\__init__.md'})] - Line 53
2023-05-03 22:02:25,066 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:25,067 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:25,095 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that takes in an input and produces an action and action input."""\nfrom future import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n\nimport yaml\nfrom pydantic import BaseModel, root_validator\n\nfrom langchain.agents.tools import InvalidTool\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForChainRun,\n    CallbackManagerForToolRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import (\n    AgentAction,\n    AgentFinish,\n    BaseMessage,\n    BaseOutputParser,\n)\nfrom langchain.tools.base import BaseTool\nfrom langchain.utilities.asyncio import asyncio_timeout\n\nlogger = logging.getLogger(name)\n\nclass BaseSingleActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass BaseMultiActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass AgentOutputParser(BaseOutputParser):\n    @abstractmethod\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        """Parse text into agent action/finish."""\n\nclass LLMSingleActionAgent(BaseSingleActionAgent):\n    llm_chain: LLMChain\n    output_parser: AgentOutputParser\n    stop: List[str]\n\nclass Agent(BaseSingleActionAgent):\n    """Class responsible for calling the language model and deciding the action.\n\nclass ExceptionTool(BaseTool):\n    name = "_Exception"\n    description = "Exception tool"\n\nclass AgentExecutor(Chain):\n    """Consists of an agent using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\agent.md'})] - Line 53
2023-05-03 22:02:26,732 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:26,732 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:26,740 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom enum import Enum\n\nclass AgentType(str, Enum):\n    ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"\n    REACT_DOCSTORE = "react-docstore"\n    SELF_ASK_WITH_SEARCH = "self-ask-with-search"\n    CONVERSATIONAL_REACT_DESCRIPTION = "conversational-react-description"\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = "chat-zero-shot-react-description"\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = "chat-conversational-react-description"\n    STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = (\n        "structured-chat-zero-shot-react-description"\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_types.md'})] - Line 53
2023-05-03 22:02:27,794 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:27,795 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:27,808 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Load agent."""\nfrom typing import Any, Optional, Sequence\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.tools.base import BaseTool\n\ndef initialize_agent(\n    tools: Sequence[BaseTool],\n    llm: BaseLanguageModel,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    """Load an agent executor given tools and LLM.\n\n```', metadata={'source': 'embeddings\\agents\\initialize.md'})] - Line 53
2023-05-03 22:02:29,044 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:29,044 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:29,060 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading agents."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport yaml\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.types import AGENT_TO_CLASS\nfrom langchain.chains.loading import load_chain, load_chain_from_config\nfrom langchain.llms.base import BaseLLM\nfrom langchain.utilities.loading import try_load_from_hub\n\nURL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/"\n\ndef _load_agent_from_tools(\n    config: dict, llm: BaseLLM, tools: List[Tool], **kwargs: Any\n) -> BaseSingleActionAgent:\n    config_type = config.pop("_type")\n    if config_type not in AGENT_TO_CLASS:\n        raise ValueError(f"Loading {config_type} agent not supported")\n\ndef load_agent_from_config(\n    config: dict,\n    llm: Optional[BaseLLM] = None,\n    tools: Optional[List[Tool]] = None,\n    kwargs: Any,\n) -> BaseSingleActionAgent:\n    """Load agent from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify an agent Type in config")\n    load_from_tools = config.pop("load_from_llm_and_tools", False)\n    if load_from_tools:\n        if llm is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then LLM must be provided"\n            )\n        if tools is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then tools must be provided"\n            )\n        return _load_agent_from_tools(config, llm, tools, kwargs)\n    config_type = config.pop("_type")\n\ndef load_agent(path: Union[str, Path], kwargs: Any) -> BaseSingleActionAgent:\n    """Unified method for loading a agent from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_agent_from_file, "agents", {"json", "yaml"}\n    ):\n        return hub_result\n    else:\n        return _load_agent_from_file(path, kwargs)\n\ndef _load_agent_from_file(\n    file: Union[str, Path], kwargs: Any\n) -> BaseSingleActionAgent:\n    """Load agent from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")\n    # Load the agent from the config now.\n    return load_agent_from_config(config, kwargs)\n\n```', metadata={'source': 'embeddings\\agents\\loading.md'})] - Line 53
2023-05-03 22:02:30,168 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:30,168 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:30,242 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\n"""Load tools."""\nimport warnings\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\nfrom mypy_extensions import Arg, KwArg\n\nfrom langchain.agents.tools import Tool\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import TextRequestsWrapper\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.bing_search.tool import BingSearchRun\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\nfrom langchain.tools.human.tool import HumanInputRun\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.tools.requests.tool import (\n    RequestsDeleteTool,\n    RequestsGetTool,\n    RequestsPatchTool,\n    RequestsPostTool,\n    RequestsPutTool,\n)\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\nfrom langchain.tools.shell.tool import ShellTool\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\nfrom langchain.utilities import ArxivAPIWrapper\nfrom langchain.utilities.apify import ApifyWrapper\nfrom langchain.utilities.bash import BashProcess\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.awslambda import LambdaWrapper\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n\ndef _get_python_repl() -> BaseTool:\n    return PythonREPLTool()\n\ndef _get_tools_requests_get() -> BaseTool:\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_post() -> BaseTool:\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_patch() -> BaseTool:\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_put() -> BaseTool:\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_delete() -> BaseTool:\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_terminal() -> BaseTool:\n    return ShellTool()\n\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\n    "python_repl": _get_python_repl,\n    "requests": _get_tools_requests_get,  # preserved for backwards compatability\n    "requests_get": _get_tools_requests_get,\n    "requests_post": _get_tools_requests_post,\n    "requests_patch": _get_tools_requests_patch,\n    "requests_put": _get_tools_requests_put,\n    "requests_delete": _get_tools_requests_delete,\n    "terminal": _get_terminal,\n}\n\ndef _get_pal_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-MATH",\n        description="A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.",\n        func=PALChain.from_math_prompt(llm).run,\n    )\n\ndef _get_pal_colored_objects(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-COLOR-OBJ",\n        description="A language model that is really good at reasoning about position and the color attributes of objects. Input should be a fully worded hard reasoning problem. Make sure to include all information about the objects AND the final question you want to answer.",\n        func=PALChain.from_colored_object_prompt(llm).run,\n    )\n\ndef _get_llm_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="Calculator",\n        description="Useful for when you need to answer questions about math.",\n        func=LLMMathChain.from_llm(llm=llm).run,\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\n    )\n\ndef _get_open_meteo_api(llm: BaseLLM) -> BaseTool:\n    chain = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS)\n    return Tool(\n        name="Open Meteo API",\n        description="Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\n_LLM_TOOLS: Dict[str, Callable[[BaseLLM], BaseTool]] = {\n    "pal-math": _get_pal_math,\n    "pal-colored-objects": _get_pal_colored_objects,\n    "llm-math": _get_llm_math,\n    "open-meteo-api": _get_open_meteo_api,\n}\n\ndef _get_news_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    news_api_key = kwargs["news_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm, news_docs.NEWS_DOCS, headers={"X-Api-Key": news_api_key}\n    )\n    return Tool(\n        name="News API",\n        description="Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_tmdb_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    tmdb_bearer_token = kwargs["tmdb_bearer_token"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        tmdb_docs.TMDB_DOCS,\n        headers={"Authorization": f"Bearer {tmdb_bearer_token}"},\n    )\n    return Tool(\n        name="TMDB API",\n        description="Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_podcast_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    listen_api_key = kwargs["listen_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        podcast_docs.PODCAST_DOCS,\n        headers={"X-ListenAPI-Key": listen_api_key},\n    )\n    return Tool(\n        name="Podcast API",\n        description="Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_lambda_api(kwargs: Any) -> BaseTool:\n    return Tool(\n        name=kwargs["awslambda_tool_name"],\n        description=kwargs["awslambda_tool_description"],\n        func=LambdaWrapper(kwargs).run,\n    )\n\ndef _get_wolfram_alpha(kwargs: Any) -> BaseTool:\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(kwargs))\n\ndef _get_google_search(kwargs: Any) -> BaseTool:\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_wikipedia(kwargs: Any) -> BaseTool:\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(kwargs))\n\ndef _get_arxiv(kwargs: Any) -> BaseTool:\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(kwargs))\n\ndef _get_google_serper(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Serper Search",\n        func=GoogleSerperAPIWrapper(kwargs).run,\n        description="A low-cost Google Search API. Useful for when you need to answer questions about current events. Input should be a search query.",\n    )\n\ndef _get_google_search_results_json(kwargs: Any) -> BaseTool:\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_serpapi(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Search",\n        description="A search engine. Useful for when you need to answer questions about current events. Input should be a search query.",\n        func=SerpAPIWrapper(kwargs).run,\n        coroutine=SerpAPIWrapper(**kwargs).arun,\n    )\n\ndef _get_searx_search(kwargs: Any) -> BaseTool:\n    return SearxSearchRun(wrapper=SearxSearchWrapper(kwargs))\n\ndef _get_searx_search_results_json(kwargs: Any) -> BaseTool:\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != "num_results"}\n    return SearxSearchResults(wrapper=SearxSearchWrapper(wrapper_kwargs), **kwargs)\n\ndef _get_bing_search(kwargs: Any) -> BaseTool:\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(kwargs))\n\ndef _get_ddg_search(kwargs: Any) -> BaseTool:\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(kwargs))\n\ndef _get_human_tool(kwargs: Any) -> BaseTool:\n    return HumanInputRun(kwargs)\n\ndef _get_scenexplain(kwargs: Any) -> BaseTool:\n    return SceneXplainTool(kwargs)\n\n_EXTRA_LLM_TOOLS: Dict[\n    str, Tuple[Callable[[Arg(BaseLLM, "llm"), KwArg(Any)], BaseTool], List[str]]\n] = {\n    "news-api": (_get_news_api, ["news_api_key"]),\n    "tmdb-api": (_get_tmdb_api, ["tmdb_bearer_token"]),\n    "podcast-api": (_get_podcast_api, ["listen_api_key"]),\n}\n\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\n    "wolfram-alpha": (_get_wolfram_alpha, ["wolfram_alpha_appid"]),\n    "google-search": (_get_google_search, ["google_api_key", "google_cse_id"]),\n    "google-search-results-json": (\n        _get_google_search_results_json,\n        ["google_api_key", "google_cse_id", "num_results"],\n    ),\n    "searx-search-results-json": (\n        _get_searx_search_results_json,\n        ["searx_host", "engines", "num_results", "aiosession"],\n    ),\n    "bing-search": (_get_bing_search, ["bing_subscription_key", "bing_search_url"]),\n    "ddg-search": (_get_ddg_search, []),\n    "google-serper": (_get_google_serper, ["serper_api_key"]),\n    "serpapi": (_get_serpapi, ["serpapi_api_key", "aiosession"]),\n    "searx-search": (_get_searx_search, ["searx_host", "engines", "aiosession"]),\n    "wikipedia": (_get_wikipedia, ["top_k_results", "lang"]),\n    "arxiv": (\n        _get_arxiv,\n        ["top_k_results", "load_max_docs", "load_all_available_meta"],\n    ),\n    "human": (_get_human_tool, ["prompt_func", "input_func"]),\n    "awslambda": (\n        _get_lambda_api,\n        ["awslambda_tool_name", "awslambda_tool_description", "function_name"],\n    ),\n    "sceneXplain": (_get_scenexplain, []),\n}\n\ndef load_tools(\n    tool_names: List[str],\n    llm: Optional[BaseLLM] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    **kwargs: Any,\n) -> List[BaseTool]:\n    """Load tools based on their name.\n\ndef get_all_tool_names() -> List[str]:\n    """Get a list of all possible tool names."""\n    return (\n        list(_BASE_TOOLS)\n        + list(_EXTRA_OPTIONAL_TOOLS)\n        + list(_EXTRA_LLM_TOOLS)\n        + list(_LLM_TOOLS)\n    )\n\n```', metadata={'source': 'embeddings\\agents\\load_tools.md'})] - Line 53
2023-05-03 22:02:32,030 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:32,030 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:32,038 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import AgentAction\n\nclass AgentScratchPadChatPromptTemplate(ChatPromptTemplate):\n    def _construct_agent_scratchpad(\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\n    ) -> str:\n        if len(intermediate_steps) == 0:\n            return ""\n        thoughts = ""\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f"\\nObservation: {observation}\\nThought: "\n        return (\n            f"This was your previous work "\n            f"(but I haven\'t seen any of it! I only see what "\n            f"you return as final answer):\\n{thoughts}"\n        )\n\n```', metadata={'source': 'embeddings\\agents\\schema.md'})] - Line 53
2023-05-03 22:02:33,192 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:33,192 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:33,201 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for tools."""\nfrom typing import Optional\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.tools.base import BaseTool, Tool, tool\n\nclass InvalidTool(BaseTool):\n    """Tool that is run when invalid tool name is encountered by agent."""\n\nall = ["InvalidTool", "BaseTool", "tool", "Tool"]\n\n```', metadata={'source': 'embeddings\\agents\\tools.md'})] - Line 53
2023-05-03 22:02:34,261 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:34,261 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:34,270 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Dict, Type\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.chat.base import ChatAgent\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchAgent\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\n\nAGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] = {\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n}\n\n```', metadata={'source': 'embeddings\\agents\\types.md'})] - Line 53
2023-05-03 22:02:35,665 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:35,665 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:35,670 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Sequence\n\nfrom langchain.tools.base import BaseTool\n\ndef validate_tools_single_input(class_name: str, tools: Sequence[BaseTool]) -> None:\n    """Validate tools for single input."""\n    for tool in tools:\n        if not tool.is_single_input:\n            raise ValueError(\n                f"{class_name} does not support multi-input tool {tool.name}."\n            )\n\n```', metadata={'source': 'embeddings\\agents\\utils.md'})] - Line 53
2023-05-03 22:02:37,281 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:37,281 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:37,290 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for agents."""\nfrom langchain.agents.agent import (\n    Agent,\n    AgentExecutor,\n    AgentOutputParser,\n    BaseMultiActionAgent,\n    BaseSingleActionAgent,\n    LLMSingleActionAgent,\n)\nfrom langchain.agents.agent_toolkits import (\n    create_csv_agent,\n    create_json_agent,\n    create_openapi_agent,\n    create_pandas_dataframe_agent,\n    create_pbi_agent,\n    create_pbi_chat_agent,\n    create_sql_agent,\n    create_vectorstore_agent,\n    create_vectorstore_router_agent,\n)\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.initialize import initialize_agent\nfrom langchain.agents.load_tools import get_all_tool_names, load_tools\nfrom langchain.agents.loading import load_agent\nfrom langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent\nfrom langchain.agents.react.base import ReActChain, ReActTextWorldAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchChain\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\nfrom langchain.agents.tools import Tool, tool\n\nall = [\n    "Agent",\n    "AgentExecutor",\n    "AgentOutputParser",\n    "AgentType",\n    "BaseMultiActionAgent",\n    "BaseSingleActionAgent",\n    "ConversationalAgent",\n    "ConversationalChatAgent",\n    "LLMSingleActionAgent",\n    "MRKLChain",\n    "ReActChain",\n    "ReActTextWorldAgent",\n    "SelfAskWithSearchChain",\n    "StructuredChatAgent",\n    "Tool",\n    "ZeroShotAgent",\n    "create_csv_agent",\n    "create_json_agent",\n    "create_openapi_agent",\n    "create_pandas_dataframe_agent",\n    "create_pbi_agent",\n    "create_pbi_chat_agent",\n    "create_sql_agent",\n    "create_vectorstore_agent",\n    "create_vectorstore_router_agent",\n    "get_all_tool_names",\n    "initialize_agent",\n    "load_agent",\n    "load_tools",\n    "tool",\n]\n\n```', metadata={'source': 'embeddings\\agents\\__init__.md'})] - Line 53
2023-05-03 22:02:38,679 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:38,679 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:38,690 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkits for agents."""\nfrom abc import abstractmethod\nfrom typing import List\n\nfrom pydantic import BaseModel\n\nfrom langchain.tools import BaseTool\n\nclass BaseToolkit(BaseModel):\n    """Class responsible for defining a collection of related tools."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\base.md'})] - Line 53
2023-05-03 22:02:39,755 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:39,756 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:39,771 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Agent toolkits."""\n\nfrom langchain.agents.agent_toolkits.csv.base import create_csv_agent\nfrom langchain.agents.agent_toolkits.file_management.toolkit import (\n    FileManagementToolkit,\n)\nfrom langchain.agents.agent_toolkits.jira.toolkit import JiraToolkit\nfrom langchain.agents.agent_toolkits.json.base import create_json_agent\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\nfrom langchain.agents.agent_toolkits.nla.toolkit import NLAToolkit\nfrom langchain.agents.agent_toolkits.openapi.base import create_openapi_agent\nfrom langchain.agents.agent_toolkits.openapi.toolkit import OpenAPIToolkit\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\nfrom langchain.agents.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\nfrom langchain.agents.agent_toolkits.powerbi.base import create_pbi_agent\nfrom langchain.agents.agent_toolkits.powerbi.chat_base import create_pbi_chat_agent\nfrom langchain.agents.agent_toolkits.powerbi.toolkit import PowerBIToolkit\nfrom langchain.agents.agent_toolkits.python.base import create_python_agent\nfrom langchain.agents.agent_toolkits.sql.base import create_sql_agent\nfrom langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\nfrom langchain.agents.agent_toolkits.vectorstore.base import (\n    create_vectorstore_agent,\n    create_vectorstore_router_agent,\n)\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import (\n    VectorStoreInfo,\n    VectorStoreRouterToolkit,\n    VectorStoreToolkit,\n)\nfrom langchain.agents.agent_toolkits.zapier.toolkit import ZapierToolkit\n\nall = [\n    "create_json_agent",\n    "create_sql_agent",\n    "create_openapi_agent",\n    "create_pbi_agent",\n    "create_pbi_chat_agent",\n    "create_python_agent",\n    "create_vectorstore_agent",\n    "JsonToolkit",\n    "SQLDatabaseToolkit",\n    "NLAToolkit",\n    "PowerBIToolkit",\n    "OpenAPIToolkit",\n    "VectorStoreToolkit",\n    "create_vectorstore_router_agent",\n    "VectorStoreInfo",\n    "VectorStoreRouterToolkit",\n    "create_pandas_dataframe_agent",\n    "create_csv_agent",\n    "ZapierToolkit",\n    "JiraToolkit",\n    "FileManagementToolkit",\n    "PlayWrightBrowserToolkit",\n]\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\__init__.md'})] - Line 53
2023-05-03 22:02:41,234 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:41,235 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:41,240 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Agent for working with csvs."""\nfrom typing import Any, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\nfrom langchain.llms.base import BaseLLM\n\ndef create_csv_agent(\n    llm: BaseLLM, path: str, pandas_kwargs: Optional[dict] = None, **kwargs: Any\n) -> AgentExecutor:\n    """Create csv agent by loading to a dataframe and using pandas agent."""\n    import pandas as pd\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\csv\\base.md'})] - Line 53
2023-05-03 22:02:42,267 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:42,267 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:42,271 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""CSV toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\csv\\__init__.md'})] - Line 53
2023-05-03 22:02:43,667 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:43,667 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:43,685 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with the local filesystem."""\nfrom future import annotations\n\nfrom typing import List, Optional\n\nfrom pydantic import root_validator\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.tools import BaseTool\nfrom langchain.tools.file_management.copy import CopyFileTool\nfrom langchain.tools.file_management.delete import DeleteFileTool\nfrom langchain.tools.file_management.file_search import FileSearchTool\nfrom langchain.tools.file_management.list_dir import ListDirectoryTool\nfrom langchain.tools.file_management.move import MoveFileTool\nfrom langchain.tools.file_management.read import ReadFileTool\nfrom langchain.tools.file_management.write import WriteFileTool\n\n_FILE_TOOLS = {\n    tool_cls.fields["name"].default: tool_cls\n    for tool_cls in [\n        CopyFileTool,\n        DeleteFileTool,\n        FileSearchTool,\n        MoveFileTool,\n        ReadFileTool,\n        WriteFileTool,\n        ListDirectoryTool,\n    ]\n}\n\nclass FileManagementToolkit(BaseToolkit):\n    """Toolkit for interacting with a Local Files."""\n\nall = ["FileManagementToolkit"]\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\file_management\\toolkit.md'})] - Line 53
2023-05-03 22:02:45,133 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:45,133 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:45,138 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Local file management toolkit."""\n\nfrom langchain.agents.agent_toolkits.file_management.toolkit import (\n    FileManagementToolkit,\n)\n\nall = ["FileManagementToolkit"]\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\file_management\\__init__.md'})] - Line 53
2023-05-03 22:02:46,859 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:46,859 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:46,866 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Jira Toolkit."""\nfrom typing import List\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.tools import BaseTool\nfrom langchain.tools.jira.tool import JiraAction\nfrom langchain.utilities.jira import JiraAPIWrapper\n\nclass JiraToolkit(BaseToolkit):\n    """Jira Toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\jira\\toolkit.md'})] - Line 53
2023-05-03 22:02:48,349 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:48,349 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:48,352 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Jira Toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\jira\\__init__.md'})] - Line 53
2023-05-03 22:02:49,478 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:49,478 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:49,485 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Json agent."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.json.prompt import JSON_PREFIX, JSON_SUFFIX\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\n\ndef create_json_agent(\n    llm: BaseLLM,\n    toolkit: JsonToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = JSON_PREFIX,\n    suffix: str = JSON_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a json agent from an LLM and tools."""\n    tools = toolkit.get_tools()\n    prompt = ZeroShotAgent.create_prompt(\n        tools,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n    )\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        **(agent_executor_kwargs or {}),\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\json\\base.md'})] - Line 53
2023-05-03 22:02:50,593 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:50,593 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:50,603 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nJSON_PREFIX = """You are an agent designed to interact with JSON.\nYour goal is to return a final answer by interacting with the JSON.\nYou have access to the following tools which help you learn more about the JSON you are interacting with.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nDo not make up any information that is not contained in the JSON.\nYour input to the tools should be in the form of data["key"][0] where data is the JSON blob you are interacting with, and the syntax used is Python. \nYou should only use keys that you know for a fact exist. You must validate that a key exists by seeing it previously when calling json_spec_list_keys. \nIf you have not seen a key in one of those responses, you cannot use it.\nYou should only add one key at a time to the path. You cannot add multiple keys at once.\nIf you encounter a "KeyError", go back to the previous key, look at the available keys, and try again.\n\nIf the question does not seem to be related to the JSON, just return "I don\'t know" as the answer.\nAlways begin your interaction with the json_spec_list_keys tool with input "data" to see what keys exist in the JSON.\n\nNote that sometimes the value at a given path is large. In this case, you will get an error "Value is a large dictionary, should explore its keys directly".\nIn this case, you should ALWAYS follow up by using the json_spec_list_keys tool to see what keys exist at that path.\nDo not simply refer the user to the JSON or a section of the JSON, as this is not a valid answer. Keep digging until you find the answer and explicitly return it.\n"""\nJSON_SUFFIX = """Begin!"\n\nQuestion: {input}\nThought: I should look at the keys that exist in data to see what I have access to\n{agent_scratchpad}"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\json\\prompt.md'})] - Line 53
2023-05-03 22:02:52,289 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:52,289 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:52,297 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with a JSON spec."""\nfrom future import annotations\n\nfrom typing import List\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.tools import BaseTool\nfrom langchain.tools.json.tool import JsonGetValueTool, JsonListKeysTool, JsonSpec\n\nclass JsonToolkit(BaseToolkit):\n    """Toolkit for interacting with a JSON spec."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\json\\toolkit.md'})] - Line 53
2023-05-03 22:02:53,495 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:53,496 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:53,499 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Json agent."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\json\\__init__.md'})] - Line 53
2023-05-03 22:02:54,810 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:54,811 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:54,819 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Tool for interacting with a single API with natural language efinition."""\n\nfrom typing import Any, Optional\n\nfrom langchain.agents.tools import Tool\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import Requests\nfrom langchain.tools.openapi.utils.api_models import APIOperation\nfrom langchain.tools.openapi.utils.openapi_utils import OpenAPISpec\n\nclass NLATool(Tool):\n    """Natural Language API Tool."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\nla\\tool.md'})] - Line 53
2023-05-03 22:02:56,551 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:56,552 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:56,560 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with API\'s using natural language."""\nfrom future import annotations\n\nfrom typing import Any, List, Optional, Sequence\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.agents.agent_toolkits.nla.tool import NLATool\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import Requests\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.openapi.utils.openapi_utils import OpenAPISpec\nfrom langchain.tools.plugin import AIPlugin\n\nclass NLAToolkit(BaseToolkit):\n    """Natural Language API Toolkit Definition."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\nla\\toolkit.md'})] - Line 53
2023-05-03 22:02:58,229 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:58,229 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:58,232 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\nla\\__init__.md'})] - Line 53
2023-05-03 22:02:59,358 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:02:59,359 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:02:59,366 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""OpenAPI spec agent."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.openapi.prompt import (\n    OPENAPI_PREFIX,\n    OPENAPI_SUFFIX,\n)\nfrom langchain.agents.agent_toolkits.openapi.toolkit import OpenAPIToolkit\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\n\ndef create_openapi_agent(\n    llm: BaseLLM,\n    toolkit: OpenAPIToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = OPENAPI_PREFIX,\n    suffix: str = OPENAPI_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    max_iterations: Optional[int] = 15,\n    max_execution_time: Optional[float] = None,\n    early_stopping_method: str = "force",\n    verbose: bool = False,\n    return_intermediate_steps: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a json agent from an LLM and tools."""\n    tools = toolkit.get_tools()\n    prompt = ZeroShotAgent.create_prompt(\n        tools,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n    )\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        return_intermediate_steps=return_intermediate_steps,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        **(agent_executor_kwargs or {}),\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\base.md'})] - Line 53
2023-05-03 22:03:00,515 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:00,515 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:00,572 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Agent that interacts with OpenAPI APIs via a hierarchical planning approach."""\nimport json\nimport re\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport yaml\nfrom pydantic import Field\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.openapi.planner_prompt import (\n    API_CONTROLLER_PROMPT,\n    API_CONTROLLER_TOOL_DESCRIPTION,\n    API_CONTROLLER_TOOL_NAME,\n    API_ORCHESTRATOR_PROMPT,\n    API_PLANNER_PROMPT,\n    API_PLANNER_TOOL_DESCRIPTION,\n    API_PLANNER_TOOL_NAME,\n    PARSING_DELETE_PROMPT,\n    PARSING_GET_PROMPT,\n    PARSING_PATCH_PROMPT,\n    PARSING_POST_PROMPT,\n    REQUESTS_DELETE_TOOL_DESCRIPTION,\n    REQUESTS_GET_TOOL_DESCRIPTION,\n    REQUESTS_PATCH_TOOL_DESCRIPTION,\n    REQUESTS_POST_TOOL_DESCRIPTION,\n)\nfrom langchain.agents.agent_toolkits.openapi.spec import ReducedOpenAPISpec\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.tools import Tool\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.openai import OpenAI\nfrom langchain.memory import ReadOnlySharedMemory\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.requests import RequestsWrapper\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.requests.tool import BaseRequestsTool\n\nRequests tools with LLM-instructed extraction of truncated responses.\n\nOf course, truncating so bluntly may lose a lot of valuable\n\ninformation in the response.\n\nHowever, the goal for now is to have only a single inference step.\n\nMAX_RESPONSE_LENGTH = 5000\n\ndef _get_default_llm_chain(prompt: BasePromptTemplate) -> LLMChain:\n    return LLMChain(\n        llm=OpenAI(),\n        prompt=prompt,\n    )\n\ndef _get_default_llm_chain_factory(\n    prompt: BasePromptTemplate,\n) -> Callable[[], LLMChain]:\n    """Returns a default LLMChain factory."""\n    return partial(_get_default_llm_chain, prompt)\n\nclass RequestsGetToolWithParsing(BaseRequestsTool, BaseTool):\n    name = "requests_get"\n    description = REQUESTS_GET_TOOL_DESCRIPTION\n    response_length: Optional[int] = MAX_RESPONSE_LENGTH\n    llm_chain: LLMChain = Field(\n        default_factory=_get_default_llm_chain_factory(PARSING_GET_PROMPT)\n    )\n\nclass RequestsPostToolWithParsing(BaseRequestsTool, BaseTool):\n    name = "requests_post"\n    description = REQUESTS_POST_TOOL_DESCRIPTION\n\nclass RequestsPatchToolWithParsing(BaseRequestsTool, BaseTool):\n    name = "requests_patch"\n    description = REQUESTS_PATCH_TOOL_DESCRIPTION\n\nclass RequestsDeleteToolWithParsing(BaseRequestsTool, BaseTool):\n    name = "requests_delete"\n    description = REQUESTS_DELETE_TOOL_DESCRIPTION\n\nOrchestrator, planner, controller.\n\ndef _create_api_planner_tool(\n    api_spec: ReducedOpenAPISpec, llm: BaseLanguageModel\n) -> Tool:\n    endpoint_descriptions = [\n        f"{name} {description}" for name, description, _ in api_spec.endpoints\n    ]\n    prompt = PromptTemplate(\n        template=API_PLANNER_PROMPT,\n        input_variables=["query"],\n        partial_variables={"endpoints": "- " + "- ".join(endpoint_descriptions)},\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    tool = Tool(\n        name=API_PLANNER_TOOL_NAME,\n        description=API_PLANNER_TOOL_DESCRIPTION,\n        func=chain.run,\n    )\n    return tool\n\ndef _create_api_controller_agent(\n    api_url: str,\n    api_docs: str,\n    requests_wrapper: RequestsWrapper,\n    llm: BaseLanguageModel,\n) -> AgentExecutor:\n    get_llm_chain = LLMChain(llm=llm, prompt=PARSING_GET_PROMPT)\n    post_llm_chain = LLMChain(llm=llm, prompt=PARSING_POST_PROMPT)\n    tools: List[BaseTool] = [\n        RequestsGetToolWithParsing(\n            requests_wrapper=requests_wrapper, llm_chain=get_llm_chain\n        ),\n        RequestsPostToolWithParsing(\n            requests_wrapper=requests_wrapper, llm_chain=post_llm_chain\n        ),\n    ]\n    prompt = PromptTemplate(\n        template=API_CONTROLLER_PROMPT,\n        input_variables=["input", "agent_scratchpad"],\n        partial_variables={\n            "api_url": api_url,\n            "api_docs": api_docs,\n            "tool_names": ", ".join([tool.name for tool in tools]),\n            "tool_descriptions": "\\n".join(\n                [f"{tool.name}: {tool.description}" for tool in tools]\n            ),\n        },\n    )\n    agent = ZeroShotAgent(\n        llm_chain=LLMChain(llm=llm, prompt=prompt),\n        allowed_tools=[tool.name for tool in tools],\n    )\n    return AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n\ndef _create_api_controller_tool(\n    api_spec: ReducedOpenAPISpec,\n    requests_wrapper: RequestsWrapper,\n    llm: BaseLanguageModel,\n) -> Tool:\n    """Expose controller as a tool.\n\ndef create_openapi_agent(\n    api_spec: ReducedOpenAPISpec,\n    requests_wrapper: RequestsWrapper,\n    llm: BaseLanguageModel,\n    shared_memory: Optional[ReadOnlySharedMemory] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    verbose: bool = True,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    **kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Instantiate API planner and controller for a given spec.\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\planner.md'})] - Line 53
2023-05-03 22:03:02,382 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:02,382 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:02,430 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nfrom langchain.prompts.prompt import PromptTemplate\n\nAPI_PLANNER_PROMPT = """You are a planner that plans a sequence of API calls to assist with user queries against an API.\n\nYou should:\n1) evaluate whether the user query can be solved by the API documentated below. If no, say why.\n2) if yes, generate a plan of API calls and say what they are doing step by step.\n3) If the plan includes a DELETE call, you should always return an ask from the User for authorization first unless the User has specifically asked to delete something.\n\nYou should only use API endpoints documented below ("Endpoints you can use:").\nYou can only use the DELETE tool if the User has specifically asked to delete something. Otherwise, you should return a request authorization from the User first.\nSome user queries can be resolved in a single API call, but some will require several API calls.\nThe plan will be passed to an API controller that can format it into web requests and return the responses.\n\nHere are some examples:\n\nFake endpoints for examples:\nGET /user to get information about the current user\nGET /products/search search across products\nPOST /users/{{id}}/cart to add products to a user\'s cart\nPATCH /users/{{id}}/cart to update a user\'s cart\nDELETE /users/{{id}}/cart to delete a user\'s cart\n\nUser query: tell me a joke\nPlan: Sorry, this API\'s domain is shopping, not comedy.\n\nUsery query: I want to buy a couch\nPlan: 1. GET /products with a query param to search for couches\n2. GET /user to find the user\'s id\n3. POST /users/{{id}}/cart to add a couch to the user\'s cart\n\nUser query: I want to add a lamp to my cart\nPlan: 1. GET /products with a query param to search for lamps\n2. GET /user to find the user\'s id\n3. PATCH /users/{{id}}/cart to add a lamp to the user\'s cart\n\nUser query: I want to delete my cart\nPlan: 1. GET /user to find the user\'s id\n2. DELETE required. Did user specify DELETE or previously authorize? Yes, proceed.\n3. DELETE /users/{{id}}/cart to delete the user\'s cart\n\nUser query: I want to start a new cart\nPlan: 1. GET /user to find the user\'s id\n2. DELETE required. Did user specify DELETE or previously authorize? No, ask for authorization.\n3. Are you sure you want to delete your cart?\n\nHere are endpoints you can use. Do not reference any of the endpoints above.\n\n{endpoints}\n\nUser query: {query}\nPlan:"""\nAPI_PLANNER_TOOL_NAME = "api_planner"\nAPI_PLANNER_TOOL_DESCRIPTION = f"Can be used to generate the right API calls to assist with a user query, like {API_PLANNER_TOOL_NAME}(query). Should always be called before trying to call the API controller."\n\nExecution.\n\nAPI_CONTROLLER_PROMPT = """You are an agent that gets a sequence of API calls and given their documentation, should execute them and return the final response.\nIf you cannot complete them and run into issues, you should explain the issue. If you\'re able to resolve an API call, you can retry the API call. When interacting with API objects, you should extract ids for inputs to other API calls but ids and names for outputs returned to the User.\n\nHere is documentation on the API:\nBase url: {api_url}\nEndpoints:\n{api_docs}\n\nHere are tools to execute requests against the API: {tool_descriptions}\n\nStarting below, you should follow this format:\n\nPlan: the plan of API calls to execute\nThought: you should always think about what to do\nAction: the action to take, should be one of the tools [{tool_names}]\nAction Input: the input to the action\nObservation: the output of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I am finished executing the plan (or, I cannot finish executing the plan without knowing some other information.)\nFinal Answer: the final output from executing the plan or missing information I\'d need to re-plan correctly.\n\nBegin!\n\nPlan: {input}\nThought:\n{agent_scratchpad}\n"""\nAPI_CONTROLLER_TOOL_NAME = "api_controller"\nAPI_CONTROLLER_TOOL_DESCRIPTION = f"Can be used to execute a plan of API calls, like {API_CONTROLLER_TOOL_NAME}(plan)."\n\nOrchestrate planning + execution.\n\nThe goal is to have an agent at the top-level (e.g. so it can recover from errors and re-plan) while\n\nkeeping planning (and specifically the planning prompt) simple.\n\nAPI_ORCHESTRATOR_PROMPT = """You are an agent that assists with user queries against API, things like querying information or creating resources.\nSome user queries can be resolved in a single API call, particularly if you can find appropriate params from the OpenAPI spec; though some require several API call.\nYou should always plan your API calls first, and then execute the plan second.\nIf the plan includes a DELETE call, be sure to ask the User for authorization first unless the User has specifically asked to delete something.\nYou should never return information without executing the api_controller tool.\n\nHere are the tools to plan and execute API requests: {tool_descriptions}\n\nStarting below, you should follow this format:\n\nUser query: the query a User wants help with related to the API\nThought: you should always think about what to do\nAction: the action to take, should be one of the tools [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I am finished executing a plan and have the information the user asked for or the data the used asked to create\nFinal Answer: the final output from executing the plan\n\nExample:\nUser query: can you add some trendy stuff to my shopping cart.\nThought: I should plan API calls first.\nAction: api_planner\nAction Input: I need to find the right API calls to add trendy items to the users shopping cart\nObservation: 1) GET /items with params \'trending\' is \'True\' to get trending item ids\n2) GET /user to get user\n3) POST /cart to post the trending items to the user\'s cart\nThought: I\'m ready to execute the API calls.\nAction: api_controller\nAction Input: 1) GET /items params \'trending\' is \'True\' to get trending item ids\n2) GET /user to get user\n3) POST /cart to post the trending items to the user\'s cart\n...\n\nBegin!\n\nUser query: {input}\nThought: I should generate a plan to help with this query and then copy that plan exactly to the controller.\n{agent_scratchpad}"""\n\nREQUESTS_GET_TOOL_DESCRIPTION = """Use this to GET content from a website.\nInput to the tool should be a json string with 3 keys: "url", "params" and "output_instructions".\nThe value of "url" should be a string. \nThe value of "params" should be a dict of the needed and available parameters from the OpenAPI spec related to the endpoint. \nIf parameters are not needed, or not available, leave it empty.\nThe value of "output_instructions" should be instructions on what information to extract from the response, \nfor example the id(s) for a resource(s) that the GET request fetches.\n"""\n\nPARSING_GET_PROMPT = PromptTemplate(\n    template="""Here is an API response:\\n\\n{response}\\n\\n====\nYour task is to extract some information according to these instructions: {instructions}\nWhen working with API objects, you should usually use ids over names.\nIf the response indicates an error, you should instead output a summary of the error.\n\nOutput:""",\n    input_variables=["response", "instructions"],\n)\n\nREQUESTS_POST_TOOL_DESCRIPTION = """Use this when you want to POST to a website.\nInput to the tool should be a json string with 3 keys: "url", "data", and "output_instructions".\nThe value of "url" should be a string.\nThe value of "data" should be a dictionary of key-value pairs you want to POST to the url.\nThe value of "output_instructions" should be instructions on what information to extract from the response, for example the id(s) for a resource(s) that the POST request creates.\nAlways use double quotes for strings in the json string."""\n\nPARSING_POST_PROMPT = PromptTemplate(\n    template="""Here is an API response:\\n\\n{response}\\n\\n====\nYour task is to extract some information according to these instructions: {instructions}\nWhen working with API objects, you should usually use ids over names. Do not return any ids or names that are not in the response.\nIf the response indicates an error, you should instead output a summary of the error.\n\nOutput:""",\n    input_variables=["response", "instructions"],\n)\n\nREQUESTS_PATCH_TOOL_DESCRIPTION = """Use this when you want to PATCH content on a website.\nInput to the tool should be a json string with 3 keys: "url", "data", and "output_instructions".\nThe value of "url" should be a string.\nThe value of "data" should be a dictionary of key-value pairs of the body params available in the OpenAPI spec you want to PATCH the content with at the url.\nThe value of "output_instructions" should be instructions on what information to extract from the response, for example the id(s) for a resource(s) that the PATCH request creates.\nAlways use double quotes for strings in the json string."""\n\nPARSING_PATCH_PROMPT = PromptTemplate(\n    template="""Here is an API response:\\n\\n{response}\\n\\n====\nYour task is to extract some information according to these instructions: {instructions}\nWhen working with API objects, you should usually use ids over names. Do not return any ids or names that are not in the response.\nIf the response indicates an error, you should instead output a summary of the error.\n\nOutput:""",\n    input_variables=["response", "instructions"],\n)\n\nREQUESTS_DELETE_TOOL_DESCRIPTION = """ONLY USE THIS TOOL WHEN THE USER HAS SPECIFICALLY REQUESTED TO DELETE CONTENT FROM A WEBSITE.\nInput to the tool should be a json string with 2 keys: "url", and "output_instructions".\nThe value of "url" should be a string.\nThe value of "output_instructions" should be instructions on what information to extract from the response, for example the id(s) for a resource(s) that the DELETE request creates.\nAlways use double quotes for strings in the json string.\nONLY USE THIS TOOL IF THE USER HAS SPECIFICALLY REQUESTED TO DELETE SOMETHING."""\n\nPARSING_DELETE_PROMPT = PromptTemplate(\n    template="""Here is an API response:\\n\\n{response}\\n\\n====\nYour task is to extract some information according to these instructions: {instructions}\nWhen working with API objects, you should usually use ids over names. Do not return any ids or names that are not in the response.\nIf the response indicates an error, you should instead output a summary of the error.\n\nOutput:""",\n    input_variables=["response", "instructions"],\n)\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\planner_prompt.md'})] - Line 53
2023-05-03 22:03:03,727 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:03,728 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:03,740 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nOPENAPI_PREFIX = """You are an agent designed to answer questions by making web requests to an API given the openapi spec.\n\nIf the question does not seem related to the API, return I don\'t know. Do not make up an answer.\nOnly use information provided by the tools to construct your response.\n\nFirst, find the base URL needed to make the request.\n\nSecond, find the relevant paths needed to answer the question. Take note that, sometimes, you might need to make more than one request to more than one path to answer the question.\n\nThird, find the required parameters needed to make the request. For GET requests, these are usually URL parameters and for POST requests, these are request body parameters.\n\nFourth, make the requests needed to answer the question. Ensure that you are sending the correct parameters to the request by checking which parameters are required. For parameters with a fixed set of values, please use the spec to look at which values are allowed.\n\nUse the exact parameter names as listed in the spec, do not make up any names or abbreviate the names of parameters.\nIf you get a not found error, ensure that you are using a path that actually exists in the spec.\n"""\nOPENAPI_SUFFIX = """Begin!\n\nQuestion: {input}\nThought: I should explore the spec to find the base url for the API.\n{agent_scratchpad}"""\n\nDESCRIPTION = """Can be used to answer questions about the openapi spec for the API. Always use this tool before trying to make a request. \nExample inputs to this tool: \n    \'What are the required query parameters for a GET request to the /bar endpoint?`\n    \'What are the required parameters in the request body for a POST request to the /foo endpoint?\'\nAlways give this tool a specific question."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\prompt.md'})] - Line 53
2023-05-03 22:03:04,993 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:04,993 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:05,010 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Quick and dirty representation for OpenAPI specs."""\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple, Union\n\ndef dereference_refs(spec_obj: dict, full_spec: dict) -> Union[dict, list]:\n    """Try to substitute $refs.\n\n@dataclass(frozen=True)\nclass ReducedOpenAPISpec:\n    servers: List[dict]\n    description: str\n    endpoints: List[Tuple[str, str, dict]]\n\ndef reduce_openapi_spec(spec: dict, dereference: bool = True) -> ReducedOpenAPISpec:\n    """Simplify/distill/minify a spec somehow.\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\spec.md'})] - Line 53
2023-05-03 22:03:06,585 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:06,585 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:06,597 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Requests toolkit."""\nfrom future import annotations\n\nfrom typing import Any, List\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.agents.agent_toolkits.json.base import create_json_agent\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\nfrom langchain.agents.agent_toolkits.openapi.prompt import DESCRIPTION\nfrom langchain.agents.tools import Tool\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import TextRequestsWrapper\nfrom langchain.tools import BaseTool\nfrom langchain.tools.json.tool import JsonSpec\nfrom langchain.tools.requests.tool import (\n    RequestsDeleteTool,\n    RequestsGetTool,\n    RequestsPatchTool,\n    RequestsPostTool,\n    RequestsPutTool,\n)\n\nclass RequestsToolkit(BaseToolkit):\n    """Toolkit for making requests."""\n\nclass OpenAPIToolkit(BaseToolkit):\n    """Toolkit for interacting with a OpenAPI api."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\toolkit.md'})] - Line 53
2023-05-03 22:03:08,249 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:08,249 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:08,253 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""OpenAPI spec agent."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\openapi\\__init__.md'})] - Line 53
2023-05-03 22:03:09,421 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:09,421 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:09,429 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Agent for working with pandas objects."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.pandas.prompt import PREFIX, SUFFIX\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.tools.python.tool import PythonAstREPLTool\n\ndef create_pandas_dataframe_agent(\n    llm: BaseLLM,\n    df: Any,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = PREFIX,\n    suffix: str = SUFFIX,\n    input_variables: Optional[List[str]] = None,\n    verbose: bool = False,\n    return_intermediate_steps: bool = False,\n    max_iterations: Optional[int] = 15,\n    max_execution_time: Optional[float] = None,\n    early_stopping_method: str = "force",\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    **kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a pandas agent from an LLM and dataframe."""\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ValueError(\n            "pandas package not found, please install with pip install pandas"\n        )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\pandas\\base.md'})] - Line 53
2023-05-03 22:03:10,692 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:10,692 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:10,698 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """\nYou are working with a pandas dataframe in Python. The name of the dataframe is df.\nYou should use the tools below to answer the question posed of you:"""\n\nSUFFIX = """\nThis is the result of print(df.head()):\n{df}\n\nBegin!\nQuestion: {input}\n{agent_scratchpad}"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\pandas\\prompt.md'})] - Line 53
2023-05-03 22:03:12,594 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:12,594 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:12,597 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Pandas toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\pandas\\__init__.md'})] - Line 53
2023-05-03 22:03:14,028 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:14,028 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:14,042 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Playwright web browser toolkit."""\nfrom future import annotations\n\nfrom typing import TYPE_CHECKING, List, Optional, Type, cast\n\nfrom pydantic import Extra, root_validator\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.playwright.base import (\n    BaseBrowserTool,\n    lazy_import_playwright_browsers,\n)\nfrom langchain.tools.playwright.click import ClickTool\nfrom langchain.tools.playwright.current_page import CurrentWebPageTool\nfrom langchain.tools.playwright.extract_hyperlinks import ExtractHyperlinksTool\nfrom langchain.tools.playwright.extract_text import ExtractTextTool\nfrom langchain.tools.playwright.get_elements import GetElementsTool\nfrom langchain.tools.playwright.navigate import NavigateTool\nfrom langchain.tools.playwright.navigate_back import NavigateBackTool\n\nif TYPE_CHECKING:\n    from playwright.async_api import Browser as AsyncBrowser\n    from playwright.sync_api import Browser as SyncBrowser\nelse:\n    try:\n        # We do this so pydantic can resolve the types when instantiating\n        from playwright.async_api import Browser as AsyncBrowser\n        from playwright.sync_api import Browser as SyncBrowser\n    except ImportError:\n        pass\n\nclass PlayWrightBrowserToolkit(BaseToolkit):\n    """Toolkit for web browser tools."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\playwright\\toolkit.md'})] - Line 53
2023-05-03 22:03:15,161 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:15,162 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:15,166 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Playwright browser toolkit."""\nfrom langchain.agents.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\n\nall = ["PlayWrightBrowserToolkit"]\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\playwright\\__init__.md'})] - Line 53
2023-05-03 22:03:16,779 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:16,780 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:16,787 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Power BI agent."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.agent_toolkits.powerbi.prompt import (\n    POWERBI_PREFIX,\n    POWERBI_SUFFIX,\n)\nfrom langchain.agents.agent_toolkits.powerbi.toolkit import PowerBIToolkit\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.utilities.powerbi import PowerBIDataset\n\ndef create_pbi_agent(\n    llm: BaseLLM,\n    toolkit: Optional[PowerBIToolkit],\n    powerbi: Optional[PowerBIDataset] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = POWERBI_PREFIX,\n    suffix: str = POWERBI_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    examples: Optional[str] = None,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    **kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a pbi agent from an LLM and tools."""\n    if toolkit is None:\n        if powerbi is None:\n            raise ValueError("Must provide either a toolkit or powerbi dataset")\n        toolkit = PowerBIToolkit(powerbi=powerbi, llm=llm, examples=examples)\n    tools = toolkit.get_tools()\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\powerbi\\base.md'})] - Line 53
2023-05-03 22:03:18,204 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:18,204 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:18,218 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Power BI agent."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.agent_toolkits.powerbi.prompt import (\n    POWERBI_CHAT_PREFIX,\n    POWERBI_CHAT_SUFFIX,\n)\nfrom langchain.agents.agent_toolkits.powerbi.toolkit import PowerBIToolkit\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.memory.chat_memory import BaseChatMemory\nfrom langchain.utilities.powerbi import PowerBIDataset\n\ndef create_pbi_chat_agent(\n    llm: BaseChatModel,\n    toolkit: Optional[PowerBIToolkit],\n    powerbi: Optional[PowerBIDataset] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    output_parser: Optional[AgentOutputParser] = None,\n    prefix: str = POWERBI_CHAT_PREFIX,\n    suffix: str = POWERBI_CHAT_SUFFIX,\n    examples: Optional[str] = None,\n    input_variables: Optional[List[str]] = None,\n    memory: Optional[BaseChatMemory] = None,\n    top_k: int = 10,\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    **kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a pbi agent from an Chat LLM and tools.\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\powerbi\\chat_base.md'})] - Line 53
2023-05-03 22:03:19,403 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:19,404 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:19,426 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\n"""Prompts for PowerBI agent."""\n\nPOWERBI_PREFIX = """You are an agent designed to interact with a Power BI Dataset.\nGiven an input question, create a syntactically correct DAX query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n\nYou have access to tools for interacting with the Power BI Dataset. Only use the below tools. Only use the information returned by the below tools to construct your final answer. Usually I should first ask which tables I have, then how each table is defined and then ask the question to query tool to create a query for me and then I should ask the query tool to execute it, finally create a nice sentence that answers the question. If you receive an error back that mentions that the query was wrong try to phrase the question differently and get a new query from the question to query tool.\n\nIf the question does not seem related to the dataset, just return "I don\'t know" as the answer.\n"""\n\nPOWERBI_SUFFIX = """Begin!\n\nQuestion: {input}\nThought: I should first ask which tables I have, then how each table is defined and then ask the question to query tool to create a query for me and then I should ask the query tool to execute it, finally create a nice sentence that answers the question.\n{agent_scratchpad}"""\n\nPOWERBI_CHAT_PREFIX = """Assistant is a large language model trained by OpenAI built to help users interact with a PowerBI Dataset.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nGiven an input question, create a syntactically correct DAX query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\nUsually I should first ask which tables I have, then how each table is defined and then ask the question to query tool to create a query for me and then I should ask the query tool to execute it, finally create a complete sentence that answers the question. If you receive an error back that mentions that the query was wrong try to phrase the question differently and get a new query from the question to query tool.\n"""\n\nPOWERBI_CHAT_SUFFIX = """TOOLS\n\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{{tools}}\n\n{format_instructions}\n\nUSER\'S INPUT\n\nHere is the user\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{{{input}}}}\n"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\powerbi\\prompt.md'})] - Line 53
2023-05-03 22:03:20,860 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:20,861 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:20,874 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with a Power BI dataset."""\nfrom typing import List, Optional\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.tools import BaseTool\nfrom langchain.tools.powerbi.prompt import QUESTION_TO_QUERY\nfrom langchain.tools.powerbi.tool import (\n    InfoPowerBITool,\n    InputToQueryTool,\n    ListPowerBITool,\n    QueryPowerBITool,\n)\nfrom langchain.utilities.powerbi import PowerBIDataset\n\nclass PowerBIToolkit(BaseToolkit):\n    """Toolkit for interacting with PowerBI dataset."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\powerbi\\toolkit.md'})] - Line 53
2023-05-03 22:03:22,447 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:22,447 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:22,450 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Power BI agent."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\powerbi\\__init__.md'})] - Line 53
2023-05-03 22:03:23,635 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:23,636 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:23,646 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Python agent."""\n\nfrom typing import Any, Dict, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.python.prompt import PREFIX\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.tools.python.tool import PythonREPLTool\n\ndef create_python_agent(\n    llm: BaseLLM,\n    tool: PythonREPLTool,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    verbose: bool = False,\n    prefix: str = PREFIX,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a python agent from an LLM and tool."""\n    tools = [tool]\n    prompt = ZeroShotAgent.create_prompt(tools, prefix=prefix)\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        **(agent_executor_kwargs or {}),\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\python\\base.md'})] - Line 53
2023-05-03 22:03:24,971 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:24,972 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:24,975 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """You are an agent designed to write and execute python code to answer questions.\nYou have access to a python REPL, which you can use to execute python code.\nIf you get an error, debug your code and try again.\nOnly use the output of your code to answer the question. \nYou might know the answer without running any code, but you should still run the code to get the answer.\nIf it does not seem like you can write code to answer the question, just return "I don\'t know" as the answer.\n"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\python\\prompt.md'})] - Line 53
2023-05-03 22:03:26,783 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:26,783 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:26,786 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\python\\__init__.md'})] - Line 53
2023-05-03 22:03:28,059 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:28,059 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:28,067 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQL agent."""\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.sql.prompt import SQL_PREFIX, SQL_SUFFIX\nfrom langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\n\ndef create_sql_agent(\n    llm: BaseLLM,\n    toolkit: SQLDatabaseToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = SQL_PREFIX,\n    suffix: str = SQL_SUFFIX,\n    format_instructions: str = FORMAT_INSTRUCTIONS,\n    input_variables: Optional[List[str]] = None,\n    top_k: int = 10,\n    max_iterations: Optional[int] = 15,\n    max_execution_time: Optional[float] = None,\n    early_stopping_method: str = "force",\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a sql agent from an LLM and tools."""\n    tools = toolkit.get_tools()\n    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n    prompt = ZeroShotAgent.create_prompt(\n        tools,\n        prefix=prefix,\n        suffix=suffix,\n        format_instructions=format_instructions,\n        input_variables=input_variables,\n    )\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        max_iterations=max_iterations,\n        max_execution_time=max_execution_time,\n        early_stopping_method=early_stopping_method,\n        **(agent_executor_kwargs or {}),\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\sql\\base.md'})] - Line 53
2023-05-03 22:03:29,135 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:29,136 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:29,145 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nSQL_PREFIX = """You are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\nIf the question does not seem related to the database, just return "I don\'t know" as the answer.\n"""\n\nSQL_SUFFIX = """Begin!\n\nQuestion: {input}\nThought: I should look at the tables in the database to see what I can query.\n{agent_scratchpad}"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\sql\\prompt.md'})] - Line 53
2023-05-03 22:03:30,571 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:30,572 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:30,583 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with a SQL database."""\nfrom typing import List\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.tools import BaseTool\nfrom langchain.tools.sql_database.tool import (\n    InfoSQLDatabaseTool,\n    ListSQLDatabaseTool,\n    QueryCheckerTool,\n    QuerySQLDataBaseTool,\n)\n\nclass SQLDatabaseToolkit(BaseToolkit):\n    """Toolkit for interacting with SQL databases."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\sql\\toolkit.md'})] - Line 53
2023-05-03 22:03:32,537 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:32,537 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:32,542 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQL agent."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\sql\\__init__.md'})] - Line 53
2023-05-03 22:03:33,709 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:33,709 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:33,720 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""VectorStore agent."""\nfrom typing import Any, Dict, Optional\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_toolkits.vectorstore.prompt import PREFIX, ROUTER_PREFIX\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import (\n    VectorStoreRouterToolkit,\n    VectorStoreToolkit,\n)\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\n\ndef create_vectorstore_agent(\n    llm: BaseLLM,\n    toolkit: VectorStoreToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = PREFIX,\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a vectorstore agent from an LLM and tools."""\n    tools = toolkit.get_tools()\n    prompt = ZeroShotAgent.create_prompt(tools, prefix=prefix)\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        **(agent_executor_kwargs or {}),\n    )\n\ndef create_vectorstore_router_agent(\n    llm: BaseLLM,\n    toolkit: VectorStoreRouterToolkit,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    prefix: str = ROUTER_PREFIX,\n    verbose: bool = False,\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n    kwargs: Dict[str, Any],\n) -> AgentExecutor:\n    """Construct a vectorstore router agent from an LLM and tools."""\n    tools = toolkit.get_tools()\n    prompt = ZeroShotAgent.create_prompt(tools, prefix=prefix)\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        callback_manager=callback_manager,\n    )\n    tool_names = [tool.name for tool in tools]\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, kwargs)\n    return AgentExecutor.from_agent_and_tools(\n        agent=agent,\n        tools=tools,\n        callback_manager=callback_manager,\n        verbose=verbose,\n        **(agent_executor_kwargs or {}),\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\vectorstore\\base.md'})] - Line 53
2023-05-03 22:03:35,124 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:35,124 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:35,133 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """You are an agent designed to answer questions about sets of documents.\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\nIf the question does not seem relevant to any of the tools provided, just return "I don\'t know" as the answer.\n"""\n\nROUTER_PREFIX = """You are an agent designed to answer questions.\nYou have access to tools for interacting with different sources, and the inputs to the tools are questions.\nYour main task is to decide which of the tools is relevant for answering question at hand.\nFor complex questions, you can break the question down into sub questions and use tools to answers the sub questions.\n"""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\vectorstore\\prompt.md'})] - Line 53
2023-05-03 22:03:36,829 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:36,829 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:36,846 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Toolkit for interacting with a vector store."""\nfrom typing import List\n\nfrom pydantic import BaseModel, Field\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.llms.openai import OpenAI\nfrom langchain.tools import BaseTool\nfrom langchain.tools.vectorstore.tool import (\n    VectorStoreQATool,\n    VectorStoreQAWithSourcesTool,\n)\nfrom langchain.vectorstores.base import VectorStore\n\nclass VectorStoreInfo(BaseModel):\n    """Information about a vectorstore."""\n\nclass VectorStoreToolkit(BaseToolkit):\n    """Toolkit for interacting with a vector store."""\n\nclass VectorStoreRouterToolkit(BaseToolkit):\n    """Toolkit for routing between vectorstores."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\vectorstore\\toolkit.md'})] - Line 53
2023-05-03 22:03:38,233 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:38,234 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:38,238 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Agent toolkit for interacting with vector stores."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\vectorstore\\__init__.md'})] - Line 53
2023-05-03 22:03:39,414 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:39,414 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:39,420 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Zapier Toolkit."""\nfrom typing import List\n\nfrom langchain.agents.agent_toolkits.base import BaseToolkit\nfrom langchain.tools import BaseTool\nfrom langchain.tools.zapier.tool import ZapierNLARunAction\nfrom langchain.utilities.zapier import ZapierNLAWrapper\n\nclass ZapierToolkit(BaseToolkit):\n    """Zapier Toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\zapier\\toolkit.md'})] - Line 53
2023-05-03 22:03:40,599 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:40,600 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:40,603 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Zapier Toolkit."""\n\n```', metadata={'source': 'embeddings\\agents\\agent_toolkits\\zapier\\__init__.md'})] - Line 53
2023-05-03 22:03:42,935 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:42,935 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:42,947 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.chat.output_parser import ChatOutputParser\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import AgentAction\nfrom langchain.tools.base import BaseTool\n\nclass ChatAgent(Agent):\n    output_parser: AgentOutputParser = Field(default_factory=ChatOutputParser)\n\n```', metadata={'source': 'embeddings\\agents\\chat\\base.md'})] - Line 53
2023-05-03 22:03:44,590 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:44,590 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:44,599 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport json\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nFINAL_ANSWER_ACTION = "Final Answer:"\n\nclass ChatOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\n```', metadata={'source': 'embeddings\\agents\\chat\\output_parser.md'})] - Line 53
2023-05-03 22:03:45,962 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:45,963 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:45,972 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Answer the following questions as best you can. You have access to the following tools:"""\nFORMAT_INSTRUCTIONS = """The way you use the tools is by specifying a json blob.\nSpecifically, this json should have a action key (with the name of the tool to use) and a action_input key (with the input to the tool going here).\n\nThe only values that should be in the "action" field are: {tool_names}\n\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\n{{{{\n  "action": $TOOL_NAME,\n  "action_input": $INPUT\n}}}}\n\nALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction:\n$JSON_BLOB\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question"""\nSUFFIX = """Begin! Reminder to always use the exact characters Final Answer when responding."""\n\n```', metadata={'source': 'embeddings\\agents\\chat\\prompt.md'})] - Line 53
2023-05-03 22:03:47,749 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:47,750 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:47,752 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\agents\\chat\\__init__.md'})] - Line 53
2023-05-03 22:03:49,189 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:49,189 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:49,201 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""An agent designed to hold a conversation in addition to using tools."""\nfrom future import annotations\n\nfrom typing import Any, List, Optional, Sequence\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.conversational.output_parser import ConvoOutputParser\nfrom langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.tools.base import BaseTool\n\nclass ConversationalAgent(Agent):\n    """An agent designed to hold a conversation in addition to using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\conversational\\base.md'})] - Line 53
2023-05-03 22:03:50,545 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:50,545 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:50,554 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nclass ConvoOutputParser(AgentOutputParser):\n    ai_prefix: str = "AI"\n\n```', metadata={'source': 'embeddings\\agents\\conversational\\output_parser.md'})] - Line 53
2023-05-03 22:03:52,279 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:52,279 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:52,295 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\nTOOLS:\n\nAssistant has access to the following tools:"""\nFORMAT_INSTRUCTIONS = """To use a tool, please use the following format:\n\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\nThought: Do I need to use a tool? No\n{ai_prefix}: [your response here]"""\n\nSUFFIX = """Begin!\n\nPrevious conversation history:\n{chat_history}\n\nNew input: {input}\n{agent_scratchpad}"""\n\n```', metadata={'source': 'embeddings\\agents\\conversational\\prompt.md'})] - Line 53
2023-05-03 22:03:53,727 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:53,727 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:53,731 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""An agent designed to hold a conversation in addition to using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\conversational\\__init__.md'})] - Line 53
2023-05-03 22:03:54,924 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:54,925 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:54,944 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""An agent designed to hold a conversation in addition to using tools."""\nfrom future import annotations\n\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.conversational_chat.output_parser import ConvoOutputParser\nfrom langchain.agents.conversational_chat.prompt import (\n    PREFIX,\n    SUFFIX,\n    TEMPLATE_TOOL_RESPONSE,\n)\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AgentAction,\n    AIMessage,\n    BaseMessage,\n    BaseOutputParser,\n    HumanMessage,\n)\nfrom langchain.tools.base import BaseTool\n\nclass ConversationalChatAgent(Agent):\n    """An agent designed to hold a conversation in addition to using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\conversational_chat\\base.md'})] - Line 53
2023-05-03 22:03:56,277 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:56,278 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:56,287 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations\n\nimport json\nfrom typing import Union\n\nfrom langchain.agents import AgentOutputParser\nfrom langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.schema import AgentAction, AgentFinish\n\nclass ConvoOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\n```', metadata={'source': 'embeddings\\agents\\conversational_chat\\output_parser.md'})] - Line 53
2023-05-03 22:03:58,035 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:58,035 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:58,051 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist."""\n\nFORMAT_INSTRUCTIONS = """RESPONSE FORMAT INSTRUCTIONS\n\nWhen responding to me, please output a response in one of two formats:\n\nOption 1:\nUse this if you want the human to use a tool.\nMarkdown code snippet formatted in the following schema:\n\njson\n{{{{\n    "action": string \\\\ The action to take. Must be one of {tool_names}\n    "action_input": string \\\\ The input to the action\n}}}}\n\nOption #2:\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n\njson\n{{{{\n    "action": "Final Answer",\n    "action_input": string \\\\ You should put what you want to return to use here\n}}}}"""\n\nSUFFIX = """TOOLS\n\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{{tools}}\n\n{format_instructions}\n\nUSER\'S INPUT\n\nHere is the user\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{{{input}}}}"""\n\nTEMPLATE_TOOL_RESPONSE = """TOOL RESPONSE:\n\n{observation}\n\nUSER\'S INPUT\n\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else."""\n\n```', metadata={'source': 'embeddings\\agents\\conversational_chat\\prompt.md'})] - Line 53
2023-05-03 22:03:59,273 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:03:59,273 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:03:59,276 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""An agent designed to hold a conversation in addition to using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\conversational_chat\\__init__.md'})] - Line 53
2023-05-03 22:04:00,404 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:00,405 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:00,417 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Attempt to implement MRKL systems as described in arxiv.org/pdf/2205.00445.pdf."""\nfrom future import annotations\n\nfrom typing import Any, Callable, List, NamedTuple, Optional, Sequence\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.mrkl.output_parser import MRKLOutputParser\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.tools.base import BaseTool\n\nclass ChainConfig(NamedTuple):\n    """Configuration for chain to use in MRKL system.\n\nclass ZeroShotAgent(Agent):\n    """Agent for the MRKL chain."""\n\nclass MRKLChain(AgentExecutor):\n    """Chain that implements the MRKL system.\n\n```', metadata={'source': 'embeddings\\agents\\mrkl\\base.md'})] - Line 53
2023-05-03 22:04:01,545 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:01,545 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:01,551 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nFINAL_ANSWER_ACTION = "Final Answer:"\n\nclass MRKLOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\n```', metadata={'source': 'embeddings\\agents\\mrkl\\output_parser.md'})] - Line 53
2023-05-03 22:04:03,321 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:03,321 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:03,328 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Answer the following questions as best you can. You have access to the following tools:"""\nFORMAT_INSTRUCTIONS = """Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question"""\nSUFFIX = """Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}"""\n\n```', metadata={'source': 'embeddings\\agents\\mrkl\\prompt.md'})] - Line 53
2023-05-03 22:04:04,450 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:04,450 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:04,454 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Attempt to implement MRKL systems as described in arxiv.org/pdf/2205.00445.pdf."""\n\n```', metadata={'source': 'embeddings\\agents\\mrkl\\__init__.md'})] - Line 53
2023-05-03 22:04:05,853 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:05,853 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:05,867 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf."""\nfrom typing import Any, List, Optional, Sequence\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.react.output_parser import ReActOutputParser\nfrom langchain.agents.react.textworld_prompt import TEXTWORLD_PROMPT\nfrom langchain.agents.react.wiki_prompt import WIKI_PROMPT\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.docstore.base import Docstore\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.tools.base import BaseTool\n\nclass ReActDocstoreAgent(Agent):\n    """Agent for the ReAct chain."""\n\nclass DocstoreExplorer:\n    """Class to assist with exploration of a document store."""\n\nclass ReActTextWorldAgent(ReActDocstoreAgent):\n    """Agent for the ReAct TextWorld chain."""\n\nclass ReActChain(AgentExecutor):\n    """Chain that implements the ReAct paper.\n\n```', metadata={'source': 'embeddings\\agents\\react\\base.md'})] - Line 53
2023-05-03 22:04:07,308 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:07,309 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:07,317 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nclass ReActOutputParser(AgentOutputParser):\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        action_prefix = "Action: "\n        if not text.strip().split("\\n")[-1].startswith(action_prefix):\n            raise OutputParserException(f"Could not parse LLM Output: {text}")\n        action_block = text.strip().split("\\n")[-1]\n\n```', metadata={'source': 'embeddings\\agents\\react\\output_parser.md'})] - Line 53
2023-05-03 22:04:08,926 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:08,927 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:04:08,940 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nfrom langchain.prompts.prompt import PromptTemplate\n\nEXAMPLES = [\n    """Setup: You are now playing a fast paced round of TextWorld! Here is your task for\ntoday. First of all, you could, like, try to travel east. After that, take the\nbinder from the locker. With the binder, place the binder on the mantelpiece.\nAlright, thanks!\n\n= Vault =-\nYou\'ve just walked into a vault. You begin to take stock of what\'s here.\n\nAn open safe is here. What a letdown! The safe is empty! You make out a shelf.\nBut the thing hasn\'t got anything on it. What, you think everything in TextWorld\nshould have stuff on it?\n\nYou don\'t like doors? Why not try going east, that entranceway is unguarded.\n\nThought: I need to travel east\nAction: Play[go east]\nObservation: -= Office =-\nYou arrive in an office. An ordinary one.\n\nYou can make out a locker. The locker contains a binder. You see a case. The\ncase is empty, what a horrible day! You lean against the wall, inadvertently\npressing a secret button. The wall opens up to reveal a mantelpiece. You wonder\nidly who left that here. The mantelpiece is standard. The mantelpiece appears to\nbe empty. If you haven\'t noticed it already, there seems to be something there\nby the wall, it\'s a table. Unfortunately, there isn\'t a thing on it. Hm. Oh well\nThere is an exit to the west. Don\'t worry, it is unguarded.\n\nThought: I need to take the binder from the locker\nAction: Play[take binder]\nObservation: You take the binder from the locker.\n\nThought: I need to place the binder on the mantelpiece\nAction: Play[put binder on mantelpiece]\n\nObservation: You put the binder on the mantelpiece.\nYour score has just gone up by one point.\n*** The End ***\nThought: The End has occurred\nAction: Finish[yes]\n\n"""\n]\nSUFFIX = """\\n\\nSetup: {input}\n{agent_scratchpad}"""\n\nTEXTWORLD_PROMPT = PromptTemplate.from_examples(\n    EXAMPLES, SUFFIX, ["input", "agent_scratchpad"]\n)\n\n```', metadata={'source': 'embeddings\\agents\\react\\textworld_prompt.md'})] - Line 53
2023-05-03 22:04:10,488 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:04:10,488 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:10:11,671 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:10:11,672 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:10:11,672 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:10:12,653 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:10:12,653 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:10:12,653 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:10:12,653 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 22:10:12,654 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 22:10:12,861 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:10:12,861 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:10:12,861 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:10:12,862 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:10:12,863 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:10:12,864 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:10:12,864 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:10:12,864 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:10:12,864 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:10:12,865 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:10:12,866 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:10:12,867 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:10:12,867 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:10:12,867 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:10:12,867 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:10:12,867 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:10:18,511 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 22:10:18,512 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:10:18,512 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:10:47,893 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:10:47,893 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:10:47,893 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:10:48,834 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:10:48,834 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:10:48,834 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:10:48,834 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 22:10:48,835 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 22:10:49,045 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:10:49,046 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:10:49,046 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:10:49,046 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:10:49,047 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:10:49,048 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:10:49,049 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:10:49,050 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:10:49,051 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:10:49,051 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:10:49,051 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:10:49,051 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:11:06,071 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 22:11:06,071 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:11:06,071 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:11:28,688 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:11:28,689 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:11:28,689 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:11:29,642 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:11:29,642 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:11:29,642 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - create_embedding function - Line 78
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - load_embedding function - Line 93
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - base_retriever function - Line 99
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - retriever function - Line 107
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 118
2023-05-03 22:11:29,643 - ye_logger_of_yor - INFO - memory_search function - Line 126
2023-05-03 22:11:29,847 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:11:29,848 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:11:29,848 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:11:29,848 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:11:29,848 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:11:29,849 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:11:29,850 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:11:29,851 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:11:29,852 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:11:29,853 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:12:55,571 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:12:55,572 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:12:56,084 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.structured_chat.output_parser import (\n    StructuredChatOutputParserWithRetries,\n)\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import AgentAction\nfrom langchain.tools import BaseTool\n\nclass StructuredChatAgent(Agent):\n    output_parser: AgentOutputParser = Field(\n        default_factory=StructuredChatOutputParserWithRetries\n    )\n\n```', metadata={'source': 'embeddings\\base.md'})] - Line 53
2023-05-03 22:12:58,321 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:12:58,321 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:12:58,337 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations\n\nimport json\nimport logging\nimport re\nfrom typing import Optional, Union\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.output_parsers import OutputFixingParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nlogger = logging.getLogger(name)\n\nclass StructuredChatOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\nclass StructuredChatOutputParserWithRetries(AgentOutputParser):\n    base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)\n    output_fixing_parser: Optional[OutputFixingParser] = None\n\n```', metadata={'source': 'embeddings\\output_parser.md'})] - Line 53
2023-05-03 22:12:59,564 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:12:59,564 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:12:59,575 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Respond to the human as helpfully and accurately as possible. You have access to the following tools:"""\nFORMAT_INSTRUCTIONS = """Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\nValid "action" values: "Final Answer" or {tool_names}\n\nProvide only ONE action per $JSON_BLOB, as shown:\n\n{{{{\n  "action": $TOOL_NAME,\n  "action_input": $INPUT\n}}}}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n$JSON_BLOB\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n{{{{\n  "action": "Final Answer",\n  "action_input": "Final response to human"\n}}}}"""\nSUFFIX = """Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:$JSON_BLOBthen Observation:.\nThought:"""\n\n```', metadata={'source': 'embeddings\\prompt.md'})] - Line 53
2023-05-03 22:13:00,765 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:13:00,766 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:13:00,768 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\__init__.md'})] - Line 53
2023-05-03 22:13:29,330 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 22:13:29,331 - ye_logger_of_yor - INFO - creating embedding - Line 81
2023-05-03 22:13:29,331 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:15:42,852 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:15:42,852 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:15:42,852 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:15:43,804 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - create_embedding function - Line 81
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - load_embedding function - Line 100
2023-05-03 22:15:43,805 - ye_logger_of_yor - INFO - base_retriever function - Line 106
2023-05-03 22:15:43,806 - ye_logger_of_yor - INFO - retriever function - Line 114
2023-05-03 22:15:43,806 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 125
2023-05-03 22:15:43,806 - ye_logger_of_yor - INFO - memory_search function - Line 133
2023-05-03 22:15:44,009 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:15:44,009 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:15:44,010 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:15:44,011 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:15:44,012 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:15:44,013 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:15:44,014 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:15:44,015 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:15:44,015 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:15:44,015 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:18:35,520 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:35,521 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:36,021 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base class for all language models."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n\ndef _get_num_tokens_default_method(text: str) -> int:\n    """Get the number of tokens present in the text."""\n    # TODO: this method may not be exact.\n    # TODO: this method may differ based on model (eg codex).\n    try:\n        from transformers import GPT2TokenizerFast\n    except ImportError:\n        raise ValueError(\n            "Could not import transformers python package. "\n            "This is needed in order to calculate get_num_tokens. "\n            "Please install it with pip install transformers."\n        )\n    # create a GPT-2 tokenizer instance\n    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")\n\nclass BaseLanguageModel(BaseModel, ABC):\n    @abstractmethod\n    def generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -> LLMResult:\n        """Take in a list of prompt values and return an LLMResult."""\n\n```', metadata={'source': 'embeddings\\base_language.md'})] - Line 53
2023-05-03 22:18:38,102 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:38,102 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:38,125 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Beta Feature: base interface for cache."""\nimport hashlib\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, cast\n\nfrom sqlalchemy import Column, Integer, String, create_engine, select\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.orm import Session\n\ntry:\n    from sqlalchemy.orm import declarative_base\nexcept ImportError:\n    from sqlalchemy.ext.declarative import declarative_base\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.schema import Generation\nfrom langchain.vectorstores.redis import Redis as RedisVectorstore\n\nRETURN_VAL_TYPE = List[Generation]\n\ndef _hash(_input: str) -> str:\n    """Use a deterministic hashing approach."""\n    return hashlib.md5(_input.encode()).hexdigest()\n\nclass BaseCache(ABC):\n    """Base interface for cache."""\n\nclass InMemoryCache(BaseCache):\n    """Cache that stores things in memory."""\n\nBase = declarative_base()\n\nclass FullLLMCache(Base):  # type: ignore\n    """SQLite table for full LLM Cache (all generations)."""\n\nclass SQLAlchemyCache(BaseCache):\n    """Cache that uses SQAlchemy as a backend."""\n\nclass SQLiteCache(SQLAlchemyCache):\n    """Cache that uses SQLite as a backend."""\n\nclass RedisCache(BaseCache):\n    """Cache that uses Redis as a backend."""\n\nclass RedisSemanticCache(BaseCache):\n    """Cache that uses Redis as a vector-store backend."""\n\nclass GPTCache(BaseCache):\n    """Cache that uses GPTCache as a backend."""\n\n```', metadata={'source': 'embeddings\\cache.md'})] - Line 53
2023-05-03 22:18:39,227 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:39,228 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:39,252 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Transform documents"""\nfrom typing import Any, Callable, List, Sequence\n\nimport numpy as np\nfrom pydantic import BaseModel, Field\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.math_utils import cosine_similarity\nfrom langchain.schema import BaseDocumentTransformer, Document\n\nclass _DocumentWithState(Document):\n    """Wrapper for a document that includes arbitrary state."""\n\ndef get_stateful_documents(\n    documents: Sequence[Document],\n) -> Sequence[_DocumentWithState]:\n    return [_DocumentWithState.from_document(doc) for doc in documents]\n\ndef _filter_similar_embeddings(\n    embedded_documents: List[List[float]], similarity_fn: Callable, threshold: float\n) -> List[int]:\n    """Filter redundant documents based on the similarity of their embeddings."""\n    similarity = np.tril(similarity_fn(embedded_documents, embedded_documents), k=-1)\n    redundant = np.where(similarity > threshold)\n    redundant_stacked = np.column_stack(redundant)\n    redundant_sorted = np.argsort(similarity[redundant])[::-1]\n    included_idxs = set(range(len(embedded_documents)))\n    for first_idx, second_idx in redundant_stacked[redundant_sorted]:\n        if first_idx in included_idxs and second_idx in included_idxs:\n            # Default to dropping the second document of any highly similar pair.\n            included_idxs.remove(second_idx)\n    return list(sorted(included_idxs))\n\ndef _get_embeddings_from_stateful_docs(\n    embeddings: Embeddings, documents: Sequence[_DocumentWithState]\n) -> List[List[float]]:\n    if len(documents) and "embedded_doc" in documents[0].state:\n        embedded_documents = [doc.state["embedded_doc"] for doc in documents]\n    else:\n        embedded_documents = embeddings.embed_documents(\n            [d.page_content for d in documents]\n        )\n        for doc, embedding in zip(documents, embedded_documents):\n            doc.state["embedded_doc"] = embedding\n    return embedded_documents\n\nclass EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n    """Filter that drops redundant documents by comparing their embeddings."""\n\n```', metadata={'source': 'embeddings\\document_transformers.md'})] - Line 53
2023-05-03 22:18:40,952 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:40,952 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:40,959 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utility functions for working with prompts."""\nfrom typing import List\n\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\n\nTEST_GEN_TEMPLATE_SUFFIX = "Add another example."\n\ndef generate_example(\n    examples: List[dict], llm: BaseLLM, prompt_template: PromptTemplate\n) -> str:\n    """Return another example given a list of examples for a prompt."""\n    prompt = FewShotPromptTemplate(\n        examples=examples,\n        suffix=TEST_GEN_TEMPLATE_SUFFIX,\n        input_variables=[],\n        example_prompt=prompt_template,\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    return chain.predict()\n\n```', metadata={'source': 'embeddings\\example_generator.md'})] - Line 53
2023-05-03 22:18:42,313 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:42,313 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:42,318 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utilities for formatting strings."""\nfrom string import Formatter\nfrom typing import Any, List, Mapping, Sequence, Union\n\nclass StrictFormatter(Formatter):\n    """A subclass of formatter that checks for extra keys."""\n\nformatter = StrictFormatter()\n\n```', metadata={'source': 'embeddings\\formatting.md'})] - Line 53
2023-05-03 22:18:43,755 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:43,756 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:43,761 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Handle chained inputs."""\nfrom typing import Dict, List, Optional\n\n_TEXT_COLOR_MAPPING = {\n    "blue": "36;1",\n    "yellow": "33;1",\n    "pink": "38;5;200",\n    "green": "32;1",\n    "red": "31;1",\n}\n\ndef get_color_mapping(\n    items: List[str], excluded_colors: Optional[List] = None\n) -> Dict[str, str]:\n    """Get mapping for items to a support color."""\n    colors = list(_TEXT_COLOR_MAPPING.keys())\n    if excluded_colors is not None:\n        colors = [c for c in colors if c not in excluded_colors]\n    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}\n    return color_mapping\n\ndef get_colored_text(text: str, color: str) -> str:\n    """Get colored text."""\n    color_str = _TEXT_COLOR_MAPPING[color]\n    return f"\\u001b[{color_str}m\\033[1;3m{text}\\u001b[0m"\n\ndef print_text(text: str, color: Optional[str] = None, end: str = "") -> None:\n    """Print text with highlighting and no end characters."""\n    if color is None:\n        text_to_print = text\n    else:\n        text_to_print = get_colored_text(text, color)\n    print(text_to_print, end=end)\n\n```', metadata={'source': 'embeddings\\input.md'})] - Line 53
2023-05-03 22:18:45,242 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:45,242 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:45,247 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Math utils."""\nfrom typing import List, Union\n\nimport numpy as np\n\nMatrix = Union[List[List[float]], List[np.ndarray], np.ndarray]\n\ndef cosine_similarity(X: Matrix, Y: Matrix) -> np.ndarray:\n    """Row-wise cosine similarity between two equal-width matrices."""\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f"Number of columns in X and Y must be the same. X has shape {X.shape} "\n            f"and Y has shape {Y.shape}."\n        )\n\n```', metadata={'source': 'embeddings\\math_utils.md'})] - Line 53
2023-05-03 22:18:46,539 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:46,539 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:46,547 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Experiment with different models."""\nfrom future import annotations\n\nfrom typing import List, Optional, Sequence\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping, print_text\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.prompt import PromptTemplate\n\nclass ModelLaboratory:\n    """Experiment with different models."""\n\n```', metadata={'source': 'embeddings\\model_laboratory.md'})] - Line 53
2023-05-03 22:18:48,211 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:48,211 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:48,215 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatibility."""\nfrom langchain.utilities.python import PythonREPL\n\nall = ["PythonREPL"]\n\n```', metadata={'source': 'embeddings\\python.md'})] - Line 53
2023-05-03 22:18:49,575 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:49,576 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:49,585 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Lightweight wrapper around requests library, with async support."""\nfrom contextlib import asynccontextmanager\nfrom typing import Any, AsyncGenerator, Dict, Optional\n\nimport aiohttp\nimport requests\nfrom pydantic import BaseModel, Extra\n\nclass Requests(BaseModel):\n    """Wrapper around requests to handle auth and async.\n\nclass TextRequestsWrapper(BaseModel):\n    """Lightweight wrapper around requests library.\n\nFor backwards compatibility\n\nRequestsWrapper = TextRequestsWrapper\n\n```', metadata={'source': 'embeddings\\requests.md'})] - Line 53
2023-05-03 22:18:51,196 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:51,196 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:51,243 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Common schema objects."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, Extra, Field, root_validator\n\ndef get_buffer_string(\n    messages: List[BaseMessage], human_prefix: str = "Human", ai_prefix: str = "AI"\n) -> str:\n    """Get buffer string of messages."""\n    string_messages = []\n    for m in messages:\n        if isinstance(m, HumanMessage):\n            role = human_prefix\n        elif isinstance(m, AIMessage):\n            role = ai_prefix\n        elif isinstance(m, SystemMessage):\n            role = "System"\n        elif isinstance(m, ChatMessage):\n            role = m.role\n        else:\n            raise ValueError(f"Got unsupported message type: {m}")\n        string_messages.append(f"{role}: {m.content}")\n    return "\\n".join(string_messages)\n\nclass AgentAction(NamedTuple):\n    """Agent\'s action to take."""\n\nclass AgentFinish(NamedTuple):\n    """Agent\'s return value."""\n\nclass Generation(BaseModel):\n    """Output of a single generation."""\n\nclass BaseMessage(BaseModel):\n    """Message object."""\n\nclass HumanMessage(BaseMessage):\n    """Type of message that is spoken by the human."""\n\nclass AIMessage(BaseMessage):\n    """Type of message that is spoken by the AI."""\n\nclass SystemMessage(BaseMessage):\n    """Type of message that is a system message."""\n\nclass ChatMessage(BaseMessage):\n    """Type of message with arbitrary speaker."""\n\ndef _message_to_dict(message: BaseMessage) -> dict:\n    return {"type": message.type, "data": message.dict()}\n\ndef messages_to_dict(messages: List[BaseMessage]) -> List[dict]:\n    return [_message_to_dict(m) for m in messages]\n\ndef _message_from_dict(message: dict) -> BaseMessage:\n    _type = message["type"]\n    if _type == "human":\n        return HumanMessage(message["data"])\n    elif _type == "ai":\n        return AIMessage(message["data"])\n    elif _type == "system":\n        return SystemMessage(message["data"])\n    elif _type == "chat":\n        return ChatMessage(message["data"])\n    else:\n        raise ValueError(f"Got unexpected type: {_type}")\n\ndef messages_from_dict(messages: List[dict]) -> List[BaseMessage]:\n    return [_message_from_dict(m) for m in messages]\n\nclass ChatGeneration(Generation):\n    """Output of a single generation."""\n\nclass ChatResult(BaseModel):\n    """Class that contains all relevant information for a Chat Result."""\n\nclass LLMResult(BaseModel):\n    """Class that contains all relevant information for an LLM Result."""\n\nclass PromptValue(BaseModel, ABC):\n    @abstractmethod\n    def to_string(self) -> str:\n        """Return prompt as string."""\n\nclass BaseMemory(BaseModel, ABC):\n    """Base interface for memory in chains."""\n\nclass BaseChatMessageHistory(ABC):\n    """Base interface for chat message history\n    See ChatMessageHistory for default implementation.\n    """\n\nclass Document(BaseModel):\n    """Interface for interacting with a document."""\n\nclass BaseRetriever(ABC):\n    @abstractmethod\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        """Get documents relevant for a query.\n\nFor backwards compatibility\n\nMemory = BaseMemory\n\nT = TypeVar("T")\n\nclass BaseOutputParser(BaseModel, ABC, Generic[T]):\n    """Class to parse the output of an LLM call.\n\nclass OutputParserException(Exception):\n    """Exception that output parsers should raise to signify a parsing error.\n\nclass BaseDocumentTransformer(ABC):\n    """Base interface for transforming documents."""\n\n```', metadata={'source': 'embeddings\\schema.md'})] - Line 53
2023-05-03 22:18:52,390 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:52,390 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:52,394 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatiblity."""\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nall = ["SerpAPIWrapper"]\n\n```', metadata={'source': 'embeddings\\serpapi.md'})] - Line 53
2023-05-03 22:18:53,904 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:53,905 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:53,910 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Script to run langchain-server locally using docker-compose."""\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ndef main() -> None:\n    """Run the langchain server locally."""\n    p = Path(file).absolute().parent / "docker-compose.yaml"\n\nif name == "main":\n    main()\n\n```', metadata={'source': 'embeddings\\server.md'})] - Line 53
2023-05-03 22:18:55,601 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:55,602 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:55,615 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQLAlchemy wrapper around a database."""\nfrom future import annotations\n\nimport warnings\nfrom typing import Any, Iterable, List, Optional\n\nfrom sqlalchemy import MetaData, Table, create_engine, inspect, select, text\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.exc import ProgrammingError, SQLAlchemyError\nfrom sqlalchemy.schema import CreateTable\n\ndef _format_index(index: dict) -> str:\n    return (\n        f\'Name: {index["name"]}, Unique: {index["unique"]},\'\n        f\' Columns: {str(index["column_names"])}\'\n    )\n\nclass SQLDatabase:\n    """SQLAlchemy wrapper around a database."""\n\n```', metadata={'source': 'embeddings\\sql_database.md'})] - Line 53
2023-05-03 22:18:57,268 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:57,268 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:57,293 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for splitting text."""\nfrom future import annotations\n\nimport copy\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Union,\n)\n\nfrom langchain.docstore.document import Document\nfrom langchain.schema import BaseDocumentTransformer\n\nlogger = logging.getLogger(name)\n\nclass TextSplitter(BaseDocumentTransformer, ABC):\n    """Interface for splitting text into chunks."""\n\nclass CharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters."""\n\nclass TokenTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at tokens."""\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters.\n\nclass NLTKTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using NLTK."""\n\nclass SpacyTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using Spacy."""\n\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Markdown-formatted headings."""\n\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Latex-formatted layout elements."""\n\nclass PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Python syntax."""\n\n```', metadata={'source': 'embeddings\\text_splitter.md'})] - Line 53
2023-05-03 22:18:58,501 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:58,501 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:58,512 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Generic utility functions."""\nimport os\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\ndef get_from_dict_or_env(\n    data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None\n) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if key in data and data[key]:\n        return data[key]\n    else:\n        return get_from_env(key, env_key, default=default)\n\ndef get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if env_key in os.environ and os.environ[env_key]:\n        return os.environ[env_key]\n    elif default is not None:\n        return default\n    else:\n        raise ValueError(\n            f"Did not find {key}, please add an environment variable"\n            f" {env_key} which contains it, or pass"\n            f"  {key} as a named parameter."\n        )\n\ndef xor_args(*arg_groups: Tuple[str, ...]) -> Callable:\n    """Validate specified keyword args are mutually exclusive."""\n\ndef stringify_value(val: Any) -> str:\n    if isinstance(val, str):\n        return val\n    elif isinstance(val, dict):\n        return "\\n" + stringify_dict(val)\n    elif isinstance(val, list):\n        return "\\n".join(stringify_value(v) for v in val)\n    else:\n        return str(val)\n\ndef stringify_dict(data: dict) -> str:\n    text = ""\n    for key, value in data.items():\n        text += key + ": " + stringify_value(value) + "\\n"\n    return text\n\n```', metadata={'source': 'embeddings\\utils.md'})] - Line 53
2023-05-03 22:18:59,661 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:18:59,661 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:18:59,683 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Main entrypoint into package."""\n\nfrom importlib import metadata\nfrom typing import Optional\n\nfrom langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\nfrom langchain.cache import BaseCache\nfrom langchain.chains import (\n    ConversationChain,\n    LLMBashChain,\n    LLMChain,\n    LLMCheckerChain,\n    LLMMathChain,\n    PALChain,\n    QAWithSourcesChain,\n    SQLDatabaseChain,\n    VectorDBQA,\n    VectorDBQAWithSourcesChain,\n)\nfrom langchain.docstore import InMemoryDocstore, Wikipedia\nfrom langchain.llms import (\n    Anthropic,\n    Banana,\n    CerebriumAI,\n    Cohere,\n    ForefrontAI,\n    GooseAI,\n    HuggingFaceHub,\n    LlamaCpp,\n    Modal,\n    OpenAI,\n    Petals,\n    PipelineAI,\n    SagemakerEndpoint,\n    StochasticAI,\n    Writer,\n)\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import (\n    BasePromptTemplate,\n    FewShotPromptTemplate,\n    Prompt,\n    PromptTemplate,\n)\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.powerbi import PowerBIDataset\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom langchain.vectorstores import FAISS, ElasticVectorSearch\n\ntry:\n    version = metadata.version(package)\nexcept metadata.PackageNotFoundError:\n    # Case where package metadata is not available.\n    version = ""\ndel metadata  # optional, avoids polluting the results of dir(package)\n\nverbose: bool = False\nllm_cache: Optional[BaseCache] = None\n\nFor backwards compatibility\n\nSerpAPIChain = SerpAPIWrapper\n\nall = [\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMMathChain",\n    "ArxivAPIWrapper",\n    "SelfAskWithSearchChain",\n    "SerpAPIWrapper",\n    "SerpAPIChain",\n    "SearxSearchWrapper",\n    "GoogleSearchAPIWrapper",\n    "GoogleSerperAPIWrapper",\n    "WolframAlphaAPIWrapper",\n    "WikipediaAPIWrapper",\n    "Anthropic",\n    "Banana",\n    "CerebriumAI",\n    "Cohere",\n    "ForefrontAI",\n    "GooseAI",\n    "Modal",\n    "OpenAI",\n    "Petals",\n    "PipelineAI",\n    "StochasticAI",\n    "Writer",\n    "BasePromptTemplate",\n    "Prompt",\n    "FewShotPromptTemplate",\n    "PromptTemplate",\n    "ReActChain",\n    "Wikipedia",\n    "HuggingFaceHub",\n    "SagemakerEndpoint",\n    "HuggingFacePipeline",\n    "SQLDatabase",\n    "SQLDatabaseChain",\n    "PowerBIDataset",\n    "FAISS",\n    "MRKLChain",\n    "VectorDBQA",\n    "ElasticVectorSearch",\n    "InMemoryDocstore",\n    "ConversationChain",\n    "VectorDBQAWithSourcesChain",\n    "QAWithSourcesChain",\n    "PALChain",\n    "LlamaCpp",\n]\n\n```', metadata={'source': 'embeddings\\__init__.md'})] - Line 53
2023-05-03 22:19:01,200 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:01,200 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:01,229 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that takes in an input and produces an action and action input."""\nfrom future import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n\nimport yaml\nfrom pydantic import BaseModel, root_validator\n\nfrom langchain.agents.tools import InvalidTool\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForChainRun,\n    CallbackManagerForToolRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import (\n    AgentAction,\n    AgentFinish,\n    BaseMessage,\n    BaseOutputParser,\n)\nfrom langchain.tools.base import BaseTool\nfrom langchain.utilities.asyncio import asyncio_timeout\n\nlogger = logging.getLogger(name)\n\nclass BaseSingleActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass BaseMultiActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass AgentOutputParser(BaseOutputParser):\n    @abstractmethod\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        """Parse text into agent action/finish."""\n\nclass LLMSingleActionAgent(BaseSingleActionAgent):\n    llm_chain: LLMChain\n    output_parser: AgentOutputParser\n    stop: List[str]\n\nclass Agent(BaseSingleActionAgent):\n    """Class responsible for calling the language model and deciding the action.\n\nclass ExceptionTool(BaseTool):\n    name = "_Exception"\n    description = "Exception tool"\n\nclass AgentExecutor(Chain):\n    """Consists of an agent using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\agent.md'})] - Line 53
2023-05-03 22:19:02,659 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:02,659 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:02,666 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom enum import Enum\n\nclass AgentType(str, Enum):\n    ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"\n    REACT_DOCSTORE = "react-docstore"\n    SELF_ASK_WITH_SEARCH = "self-ask-with-search"\n    CONVERSATIONAL_REACT_DESCRIPTION = "conversational-react-description"\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = "chat-zero-shot-react-description"\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = "chat-conversational-react-description"\n    STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = (\n        "structured-chat-zero-shot-react-description"\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_types.md'})] - Line 53
2023-05-03 22:19:03,928 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:03,928 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:03,942 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Load agent."""\nfrom typing import Any, Optional, Sequence\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.tools.base import BaseTool\n\ndef initialize_agent(\n    tools: Sequence[BaseTool],\n    llm: BaseLanguageModel,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    """Load an agent executor given tools and LLM.\n\n```', metadata={'source': 'embeddings\\agents\\initialize.md'})] - Line 53
2023-05-03 22:19:05,413 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:05,413 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:05,428 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading agents."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport yaml\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.types import AGENT_TO_CLASS\nfrom langchain.chains.loading import load_chain, load_chain_from_config\nfrom langchain.llms.base import BaseLLM\nfrom langchain.utilities.loading import try_load_from_hub\n\nURL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/"\n\ndef _load_agent_from_tools(\n    config: dict, llm: BaseLLM, tools: List[Tool], **kwargs: Any\n) -> BaseSingleActionAgent:\n    config_type = config.pop("_type")\n    if config_type not in AGENT_TO_CLASS:\n        raise ValueError(f"Loading {config_type} agent not supported")\n\ndef load_agent_from_config(\n    config: dict,\n    llm: Optional[BaseLLM] = None,\n    tools: Optional[List[Tool]] = None,\n    kwargs: Any,\n) -> BaseSingleActionAgent:\n    """Load agent from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify an agent Type in config")\n    load_from_tools = config.pop("load_from_llm_and_tools", False)\n    if load_from_tools:\n        if llm is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then LLM must be provided"\n            )\n        if tools is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then tools must be provided"\n            )\n        return _load_agent_from_tools(config, llm, tools, kwargs)\n    config_type = config.pop("_type")\n\ndef load_agent(path: Union[str, Path], kwargs: Any) -> BaseSingleActionAgent:\n    """Unified method for loading a agent from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_agent_from_file, "agents", {"json", "yaml"}\n    ):\n        return hub_result\n    else:\n        return _load_agent_from_file(path, kwargs)\n\ndef _load_agent_from_file(\n    file: Union[str, Path], kwargs: Any\n) -> BaseSingleActionAgent:\n    """Load agent from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")\n    # Load the agent from the config now.\n    return load_agent_from_config(config, kwargs)\n\n```', metadata={'source': 'embeddings\\agents\\loading.md'})] - Line 53
2023-05-03 22:19:06,808 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:06,808 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:06,878 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\n"""Load tools."""\nimport warnings\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\nfrom mypy_extensions import Arg, KwArg\n\nfrom langchain.agents.tools import Tool\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import TextRequestsWrapper\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.bing_search.tool import BingSearchRun\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\nfrom langchain.tools.human.tool import HumanInputRun\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.tools.requests.tool import (\n    RequestsDeleteTool,\n    RequestsGetTool,\n    RequestsPatchTool,\n    RequestsPostTool,\n    RequestsPutTool,\n)\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\nfrom langchain.tools.shell.tool import ShellTool\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\nfrom langchain.utilities import ArxivAPIWrapper\nfrom langchain.utilities.apify import ApifyWrapper\nfrom langchain.utilities.bash import BashProcess\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.awslambda import LambdaWrapper\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n\ndef _get_python_repl() -> BaseTool:\n    return PythonREPLTool()\n\ndef _get_tools_requests_get() -> BaseTool:\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_post() -> BaseTool:\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_patch() -> BaseTool:\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_put() -> BaseTool:\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_delete() -> BaseTool:\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_terminal() -> BaseTool:\n    return ShellTool()\n\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\n    "python_repl": _get_python_repl,\n    "requests": _get_tools_requests_get,  # preserved for backwards compatability\n    "requests_get": _get_tools_requests_get,\n    "requests_post": _get_tools_requests_post,\n    "requests_patch": _get_tools_requests_patch,\n    "requests_put": _get_tools_requests_put,\n    "requests_delete": _get_tools_requests_delete,\n    "terminal": _get_terminal,\n}\n\ndef _get_pal_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-MATH",\n        description="A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.",\n        func=PALChain.from_math_prompt(llm).run,\n    )\n\ndef _get_pal_colored_objects(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-COLOR-OBJ",\n        description="A language model that is really good at reasoning about position and the color attributes of objects. Input should be a fully worded hard reasoning problem. Make sure to include all information about the objects AND the final question you want to answer.",\n        func=PALChain.from_colored_object_prompt(llm).run,\n    )\n\ndef _get_llm_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="Calculator",\n        description="Useful for when you need to answer questions about math.",\n        func=LLMMathChain.from_llm(llm=llm).run,\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\n    )\n\ndef _get_open_meteo_api(llm: BaseLLM) -> BaseTool:\n    chain = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS)\n    return Tool(\n        name="Open Meteo API",\n        description="Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\n_LLM_TOOLS: Dict[str, Callable[[BaseLLM], BaseTool]] = {\n    "pal-math": _get_pal_math,\n    "pal-colored-objects": _get_pal_colored_objects,\n    "llm-math": _get_llm_math,\n    "open-meteo-api": _get_open_meteo_api,\n}\n\ndef _get_news_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    news_api_key = kwargs["news_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm, news_docs.NEWS_DOCS, headers={"X-Api-Key": news_api_key}\n    )\n    return Tool(\n        name="News API",\n        description="Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_tmdb_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    tmdb_bearer_token = kwargs["tmdb_bearer_token"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        tmdb_docs.TMDB_DOCS,\n        headers={"Authorization": f"Bearer {tmdb_bearer_token}"},\n    )\n    return Tool(\n        name="TMDB API",\n        description="Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_podcast_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    listen_api_key = kwargs["listen_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        podcast_docs.PODCAST_DOCS,\n        headers={"X-ListenAPI-Key": listen_api_key},\n    )\n    return Tool(\n        name="Podcast API",\n        description="Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_lambda_api(kwargs: Any) -> BaseTool:\n    return Tool(\n        name=kwargs["awslambda_tool_name"],\n        description=kwargs["awslambda_tool_description"],\n        func=LambdaWrapper(kwargs).run,\n    )\n\ndef _get_wolfram_alpha(kwargs: Any) -> BaseTool:\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(kwargs))\n\ndef _get_google_search(kwargs: Any) -> BaseTool:\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_wikipedia(kwargs: Any) -> BaseTool:\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(kwargs))\n\ndef _get_arxiv(kwargs: Any) -> BaseTool:\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(kwargs))\n\ndef _get_google_serper(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Serper Search",\n        func=GoogleSerperAPIWrapper(kwargs).run,\n        description="A low-cost Google Search API. Useful for when you need to answer questions about current events. Input should be a search query.",\n    )\n\ndef _get_google_search_results_json(kwargs: Any) -> BaseTool:\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_serpapi(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Search",\n        description="A search engine. Useful for when you need to answer questions about current events. Input should be a search query.",\n        func=SerpAPIWrapper(kwargs).run,\n        coroutine=SerpAPIWrapper(**kwargs).arun,\n    )\n\ndef _get_searx_search(kwargs: Any) -> BaseTool:\n    return SearxSearchRun(wrapper=SearxSearchWrapper(kwargs))\n\ndef _get_searx_search_results_json(kwargs: Any) -> BaseTool:\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != "num_results"}\n    return SearxSearchResults(wrapper=SearxSearchWrapper(wrapper_kwargs), **kwargs)\n\ndef _get_bing_search(kwargs: Any) -> BaseTool:\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(kwargs))\n\ndef _get_ddg_search(kwargs: Any) -> BaseTool:\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(kwargs))\n\ndef _get_human_tool(kwargs: Any) -> BaseTool:\n    return HumanInputRun(kwargs)\n\ndef _get_scenexplain(kwargs: Any) -> BaseTool:\n    return SceneXplainTool(kwargs)\n\n_EXTRA_LLM_TOOLS: Dict[\n    str, Tuple[Callable[[Arg(BaseLLM, "llm"), KwArg(Any)], BaseTool], List[str]]\n] = {\n    "news-api": (_get_news_api, ["news_api_key"]),\n    "tmdb-api": (_get_tmdb_api, ["tmdb_bearer_token"]),\n    "podcast-api": (_get_podcast_api, ["listen_api_key"]),\n}\n\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\n    "wolfram-alpha": (_get_wolfram_alpha, ["wolfram_alpha_appid"]),\n    "google-search": (_get_google_search, ["google_api_key", "google_cse_id"]),\n    "google-search-results-json": (\n        _get_google_search_results_json,\n        ["google_api_key", "google_cse_id", "num_results"],\n    ),\n    "searx-search-results-json": (\n        _get_searx_search_results_json,\n        ["searx_host", "engines", "num_results", "aiosession"],\n    ),\n    "bing-search": (_get_bing_search, ["bing_subscription_key", "bing_search_url"]),\n    "ddg-search": (_get_ddg_search, []),\n    "google-serper": (_get_google_serper, ["serper_api_key"]),\n    "serpapi": (_get_serpapi, ["serpapi_api_key", "aiosession"]),\n    "searx-search": (_get_searx_search, ["searx_host", "engines", "aiosession"]),\n    "wikipedia": (_get_wikipedia, ["top_k_results", "lang"]),\n    "arxiv": (\n        _get_arxiv,\n        ["top_k_results", "load_max_docs", "load_all_available_meta"],\n    ),\n    "human": (_get_human_tool, ["prompt_func", "input_func"]),\n    "awslambda": (\n        _get_lambda_api,\n        ["awslambda_tool_name", "awslambda_tool_description", "function_name"],\n    ),\n    "sceneXplain": (_get_scenexplain, []),\n}\n\ndef load_tools(\n    tool_names: List[str],\n    llm: Optional[BaseLLM] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    **kwargs: Any,\n) -> List[BaseTool]:\n    """Load tools based on their name.\n\ndef get_all_tool_names() -> List[str]:\n    """Get a list of all possible tool names."""\n    return (\n        list(_BASE_TOOLS)\n        + list(_EXTRA_OPTIONAL_TOOLS)\n        + list(_EXTRA_LLM_TOOLS)\n        + list(_LLM_TOOLS)\n    )\n\n```', metadata={'source': 'embeddings\\agents\\load_tools.md'})] - Line 53
2023-05-03 22:19:08,153 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:08,153 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:08,159 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import AgentAction\n\nclass AgentScratchPadChatPromptTemplate(ChatPromptTemplate):\n    def _construct_agent_scratchpad(\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\n    ) -> str:\n        if len(intermediate_steps) == 0:\n            return ""\n        thoughts = ""\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f"\\nObservation: {observation}\\nThought: "\n        return (\n            f"This was your previous work "\n            f"(but I haven\'t seen any of it! I only see what "\n            f"you return as final answer):\\n{thoughts}"\n        )\n\n```', metadata={'source': 'embeddings\\agents\\schema.md'})] - Line 53
2023-05-03 22:19:09,597 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:09,598 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:09,607 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for tools."""\nfrom typing import Optional\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.tools.base import BaseTool, Tool, tool\n\nclass InvalidTool(BaseTool):\n    """Tool that is run when invalid tool name is encountered by agent."""\n\nall = ["InvalidTool", "BaseTool", "tool", "Tool"]\n\n```', metadata={'source': 'embeddings\\agents\\tools.md'})] - Line 53
2023-05-03 22:19:11,625 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:11,625 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:11,636 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Dict, Type\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.chat.base import ChatAgent\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchAgent\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\n\nAGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] = {\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n}\n\n```', metadata={'source': 'embeddings\\agents\\types.md'})] - Line 53
2023-05-03 22:19:12,805 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:12,805 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:12,811 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Sequence\n\nfrom langchain.tools.base import BaseTool\n\ndef validate_tools_single_input(class_name: str, tools: Sequence[BaseTool]) -> None:\n    """Validate tools for single input."""\n    for tool in tools:\n        if not tool.is_single_input:\n            raise ValueError(\n                f"{class_name} does not support multi-input tool {tool.name}."\n            )\n\n```', metadata={'source': 'embeddings\\agents\\utils.md'})] - Line 53
2023-05-03 22:19:13,905 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:13,905 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:13,914 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for agents."""\nfrom langchain.agents.agent import (\n    Agent,\n    AgentExecutor,\n    AgentOutputParser,\n    BaseMultiActionAgent,\n    BaseSingleActionAgent,\n    LLMSingleActionAgent,\n)\nfrom langchain.agents.agent_toolkits import (\n    create_csv_agent,\n    create_json_agent,\n    create_openapi_agent,\n    create_pandas_dataframe_agent,\n    create_pbi_agent,\n    create_pbi_chat_agent,\n    create_sql_agent,\n    create_vectorstore_agent,\n    create_vectorstore_router_agent,\n)\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.initialize import initialize_agent\nfrom langchain.agents.load_tools import get_all_tool_names, load_tools\nfrom langchain.agents.loading import load_agent\nfrom langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent\nfrom langchain.agents.react.base import ReActChain, ReActTextWorldAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchChain\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\nfrom langchain.agents.tools import Tool, tool\n\nall = [\n    "Agent",\n    "AgentExecutor",\n    "AgentOutputParser",\n    "AgentType",\n    "BaseMultiActionAgent",\n    "BaseSingleActionAgent",\n    "ConversationalAgent",\n    "ConversationalChatAgent",\n    "LLMSingleActionAgent",\n    "MRKLChain",\n    "ReActChain",\n    "ReActTextWorldAgent",\n    "SelfAskWithSearchChain",\n    "StructuredChatAgent",\n    "Tool",\n    "ZeroShotAgent",\n    "create_csv_agent",\n    "create_json_agent",\n    "create_openapi_agent",\n    "create_pandas_dataframe_agent",\n    "create_pbi_agent",\n    "create_pbi_chat_agent",\n    "create_sql_agent",\n    "create_vectorstore_agent",\n    "create_vectorstore_router_agent",\n    "get_all_tool_names",\n    "initialize_agent",\n    "load_agent",\n    "load_tools",\n    "tool",\n]\n\n```', metadata={'source': 'embeddings\\agents\\__init__.md'})] - Line 53
2023-05-03 22:19:15,215 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:15,215 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:15,228 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that does self ask with search."""\nfrom typing import Any, Sequence, Union\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser\nfrom langchain.agents.self_ask_with_search.prompt import PROMPT\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.tools.base import BaseTool\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nclass SelfAskWithSearchAgent(Agent):\n    """Agent for the self-ask-with-search paper."""\n\nclass SelfAskWithSearchChain(AgentExecutor):\n    """Chain that does self ask with search.\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\base.md'})] - Line 53
2023-05-03 22:19:16,605 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:16,605 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:16,614 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nclass SelfAskOutputParser(AgentOutputParser):\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        followup = "Follow up:"\n        last_line = text.split("\\n")[-1]\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\output_parser.md'})] - Line 53
2023-05-03 22:19:17,911 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:17,912 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:17,922 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n_DEFAULT_TEMPLATE = """Question: Who lived longer, Muhammad Ali or Alan Turing?\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n\nQuestion: When was the founder of craigslist born?\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\n\nQuestion: Who was the maternal grandfather of George Washington?\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\n\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Jaws?\nIntermediate answer: The director of Jaws is Steven Spielberg.\nFollow up: Where is Steven Spielberg from?\nIntermediate answer: The United States.\nFollow up: Who is the director of Casino Royale?\nIntermediate answer: The director of Casino Royale is Martin Campbell.\nFollow up: Where is Martin Campbell from?\nIntermediate answer: New Zealand.\nSo the final answer is: No\n\nQuestion: {input}\nAre followup questions needed here:{agent_scratchpad}"""\nPROMPT = PromptTemplate(\n    input_variables=["input", "agent_scratchpad"], template=_DEFAULT_TEMPLATE\n)\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\prompt.md'})] - Line 53
2023-05-03 22:19:19,123 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:19,124 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:19,128 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that does self ask with search.\n\nHeavily borrowed from https://github.com/ofirpress/self-ask\n"""\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\__init__.md'})] - Line 53
2023-05-03 22:19:20,806 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:20,806 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:20,817 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.structured_chat.output_parser import (\n    StructuredChatOutputParserWithRetries,\n)\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import AgentAction\nfrom langchain.tools import BaseTool\n\nclass StructuredChatAgent(Agent):\n    output_parser: AgentOutputParser = Field(\n        default_factory=StructuredChatOutputParserWithRetries\n    )\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\base.md'})] - Line 53
2023-05-03 22:19:21,968 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:21,968 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:21,981 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations\n\nimport json\nimport logging\nimport re\nfrom typing import Optional, Union\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.output_parsers import OutputFixingParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nlogger = logging.getLogger(name)\n\nclass StructuredChatOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\nclass StructuredChatOutputParserWithRetries(AgentOutputParser):\n    base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)\n    output_fixing_parser: Optional[OutputFixingParser] = None\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\output_parser.md'})] - Line 53
2023-05-03 22:19:23,283 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:23,283 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:23,293 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Respond to the human as helpfully and accurately as possible. You have access to the following tools:"""\nFORMAT_INSTRUCTIONS = """Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\nValid "action" values: "Final Answer" or {tool_names}\n\nProvide only ONE action per $JSON_BLOB, as shown:\n\n{{{{\n  "action": $TOOL_NAME,\n  "action_input": $INPUT\n}}}}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n$JSON_BLOB\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n{{{{\n  "action": "Final Answer",\n  "action_input": "Final response to human"\n}}}}"""\nSUFFIX = """Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:$JSON_BLOBthen Observation:.\nThought:"""\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\prompt.md'})] - Line 53
2023-05-03 22:19:24,378 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:24,378 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:24,381 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\__init__.md'})] - Line 53
2023-05-03 22:19:25,784 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:19:25,784 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:19:25,797 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult\n\ndef import_aim() -> Any:\n    try:\n        import aim\n    except ImportError:\n        raise ImportError(\n            "To use the Aim callback manager you need to have the"\n            " aim python package installed."\n            "Please install it with pip install aim"\n        )\n    return aim\n\nclass BaseMetadataCallbackHandler:\n    """This class handles the metadata and associated function states for callbacks.\n\nclass AimCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Aim.\n\n```', metadata={'source': 'embeddings\\callbacks\\aim_callback.md'})] - Line 53
2023-05-03 22:22:00,023 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:22:00,024 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:22:00,024 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:22:00,994 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:22:00,994 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:22:00,994 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - create_embedding function - Line 81
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - load_embedding function - Line 100
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - base_retriever function - Line 106
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - retriever function - Line 114
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 125
2023-05-03 22:22:00,995 - ye_logger_of_yor - INFO - memory_search function - Line 133
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:22:01,201 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:22:01,202 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:22:01,203 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:22:01,204 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:22:01,204 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:22:01,204 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:22:01,204 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:22:01,204 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:22:01,205 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:22:01,206 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:22:01,206 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:22:01,206 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:22:01,206 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:22:01,206 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:22:01,207 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:22:01,207 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:22:24,208 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:24,208 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:24,719 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base class for all language models."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n\ndef _get_num_tokens_default_method(text: str) -> int:\n    """Get the number of tokens present in the text."""\n    # TODO: this method may not be exact.\n    # TODO: this method may differ based on model (eg codex).\n    try:\n        from transformers import GPT2TokenizerFast\n    except ImportError:\n        raise ValueError(\n            "Could not import transformers python package. "\n            "This is needed in order to calculate get_num_tokens. "\n            "Please install it with pip install transformers."\n        )\n    # create a GPT-2 tokenizer instance\n    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")\n\nclass BaseLanguageModel(BaseModel, ABC):\n    @abstractmethod\n    def generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -> LLMResult:\n        """Take in a list of prompt values and return an LLMResult."""\n\n```', metadata={'source': 'embeddings\\base_language.md'})] - Line 53
2023-05-03 22:22:26,804 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:26,805 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:26,827 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Beta Feature: base interface for cache."""\nimport hashlib\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, cast\n\nfrom sqlalchemy import Column, Integer, String, create_engine, select\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.orm import Session\n\ntry:\n    from sqlalchemy.orm import declarative_base\nexcept ImportError:\n    from sqlalchemy.ext.declarative import declarative_base\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.schema import Generation\nfrom langchain.vectorstores.redis import Redis as RedisVectorstore\n\nRETURN_VAL_TYPE = List[Generation]\n\ndef _hash(_input: str) -> str:\n    """Use a deterministic hashing approach."""\n    return hashlib.md5(_input.encode()).hexdigest()\n\nclass BaseCache(ABC):\n    """Base interface for cache."""\n\nclass InMemoryCache(BaseCache):\n    """Cache that stores things in memory."""\n\nBase = declarative_base()\n\nclass FullLLMCache(Base):  # type: ignore\n    """SQLite table for full LLM Cache (all generations)."""\n\nclass SQLAlchemyCache(BaseCache):\n    """Cache that uses SQAlchemy as a backend."""\n\nclass SQLiteCache(SQLAlchemyCache):\n    """Cache that uses SQLite as a backend."""\n\nclass RedisCache(BaseCache):\n    """Cache that uses Redis as a backend."""\n\nclass RedisSemanticCache(BaseCache):\n    """Cache that uses Redis as a vector-store backend."""\n\nclass GPTCache(BaseCache):\n    """Cache that uses GPTCache as a backend."""\n\n```', metadata={'source': 'embeddings\\cache.md'})] - Line 53
2023-05-03 22:22:28,024 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:28,024 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:28,046 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Transform documents"""\nfrom typing import Any, Callable, List, Sequence\n\nimport numpy as np\nfrom pydantic import BaseModel, Field\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.math_utils import cosine_similarity\nfrom langchain.schema import BaseDocumentTransformer, Document\n\nclass _DocumentWithState(Document):\n    """Wrapper for a document that includes arbitrary state."""\n\ndef get_stateful_documents(\n    documents: Sequence[Document],\n) -> Sequence[_DocumentWithState]:\n    return [_DocumentWithState.from_document(doc) for doc in documents]\n\ndef _filter_similar_embeddings(\n    embedded_documents: List[List[float]], similarity_fn: Callable, threshold: float\n) -> List[int]:\n    """Filter redundant documents based on the similarity of their embeddings."""\n    similarity = np.tril(similarity_fn(embedded_documents, embedded_documents), k=-1)\n    redundant = np.where(similarity > threshold)\n    redundant_stacked = np.column_stack(redundant)\n    redundant_sorted = np.argsort(similarity[redundant])[::-1]\n    included_idxs = set(range(len(embedded_documents)))\n    for first_idx, second_idx in redundant_stacked[redundant_sorted]:\n        if first_idx in included_idxs and second_idx in included_idxs:\n            # Default to dropping the second document of any highly similar pair.\n            included_idxs.remove(second_idx)\n    return list(sorted(included_idxs))\n\ndef _get_embeddings_from_stateful_docs(\n    embeddings: Embeddings, documents: Sequence[_DocumentWithState]\n) -> List[List[float]]:\n    if len(documents) and "embedded_doc" in documents[0].state:\n        embedded_documents = [doc.state["embedded_doc"] for doc in documents]\n    else:\n        embedded_documents = embeddings.embed_documents(\n            [d.page_content for d in documents]\n        )\n        for doc, embedding in zip(documents, embedded_documents):\n            doc.state["embedded_doc"] = embedding\n    return embedded_documents\n\nclass EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n    """Filter that drops redundant documents by comparing their embeddings."""\n\n```', metadata={'source': 'embeddings\\document_transformers.md'})] - Line 53
2023-05-03 22:22:29,358 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:29,359 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:29,365 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utility functions for working with prompts."""\nfrom typing import List\n\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\n\nTEST_GEN_TEMPLATE_SUFFIX = "Add another example."\n\ndef generate_example(\n    examples: List[dict], llm: BaseLLM, prompt_template: PromptTemplate\n) -> str:\n    """Return another example given a list of examples for a prompt."""\n    prompt = FewShotPromptTemplate(\n        examples=examples,\n        suffix=TEST_GEN_TEMPLATE_SUFFIX,\n        input_variables=[],\n        example_prompt=prompt_template,\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    return chain.predict()\n\n```', metadata={'source': 'embeddings\\example_generator.md'})] - Line 53
2023-05-03 22:22:30,559 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:30,559 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:30,565 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utilities for formatting strings."""\nfrom string import Formatter\nfrom typing import Any, List, Mapping, Sequence, Union\n\nclass StrictFormatter(Formatter):\n    """A subclass of formatter that checks for extra keys."""\n\nformatter = StrictFormatter()\n\n```', metadata={'source': 'embeddings\\formatting.md'})] - Line 53
2023-05-03 22:22:32,243 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:32,243 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:32,248 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Handle chained inputs."""\nfrom typing import Dict, List, Optional\n\n_TEXT_COLOR_MAPPING = {\n    "blue": "36;1",\n    "yellow": "33;1",\n    "pink": "38;5;200",\n    "green": "32;1",\n    "red": "31;1",\n}\n\ndef get_color_mapping(\n    items: List[str], excluded_colors: Optional[List] = None\n) -> Dict[str, str]:\n    """Get mapping for items to a support color."""\n    colors = list(_TEXT_COLOR_MAPPING.keys())\n    if excluded_colors is not None:\n        colors = [c for c in colors if c not in excluded_colors]\n    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}\n    return color_mapping\n\ndef get_colored_text(text: str, color: str) -> str:\n    """Get colored text."""\n    color_str = _TEXT_COLOR_MAPPING[color]\n    return f"\\u001b[{color_str}m\\033[1;3m{text}\\u001b[0m"\n\ndef print_text(text: str, color: Optional[str] = None, end: str = "") -> None:\n    """Print text with highlighting and no end characters."""\n    if color is None:\n        text_to_print = text\n    else:\n        text_to_print = get_colored_text(text, color)\n    print(text_to_print, end=end)\n\n```', metadata={'source': 'embeddings\\input.md'})] - Line 53
2023-05-03 22:22:33,444 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:33,445 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:33,450 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Math utils."""\nfrom typing import List, Union\n\nimport numpy as np\n\nMatrix = Union[List[List[float]], List[np.ndarray], np.ndarray]\n\ndef cosine_similarity(X: Matrix, Y: Matrix) -> np.ndarray:\n    """Row-wise cosine similarity between two equal-width matrices."""\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f"Number of columns in X and Y must be the same. X has shape {X.shape} "\n            f"and Y has shape {Y.shape}."\n        )\n\n```', metadata={'source': 'embeddings\\math_utils.md'})] - Line 53
2023-05-03 22:22:34,684 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:34,684 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:34,691 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Experiment with different models."""\nfrom future import annotations\n\nfrom typing import List, Optional, Sequence\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping, print_text\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.prompt import PromptTemplate\n\nclass ModelLaboratory:\n    """Experiment with different models."""\n\n```', metadata={'source': 'embeddings\\model_laboratory.md'})] - Line 53
2023-05-03 22:22:35,994 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:35,995 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:35,999 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatibility."""\nfrom langchain.utilities.python import PythonREPL\n\nall = ["PythonREPL"]\n\n```', metadata={'source': 'embeddings\\python.md'})] - Line 53
2023-05-03 22:22:37,912 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:37,912 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:37,923 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Lightweight wrapper around requests library, with async support."""\nfrom contextlib import asynccontextmanager\nfrom typing import Any, AsyncGenerator, Dict, Optional\n\nimport aiohttp\nimport requests\nfrom pydantic import BaseModel, Extra\n\nclass Requests(BaseModel):\n    """Wrapper around requests to handle auth and async.\n\nclass TextRequestsWrapper(BaseModel):\n    """Lightweight wrapper around requests library.\n\nFor backwards compatibility\n\nRequestsWrapper = TextRequestsWrapper\n\n```', metadata={'source': 'embeddings\\requests.md'})] - Line 53
2023-05-03 22:22:39,296 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:39,296 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:39,342 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Common schema objects."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, Extra, Field, root_validator\n\ndef get_buffer_string(\n    messages: List[BaseMessage], human_prefix: str = "Human", ai_prefix: str = "AI"\n) -> str:\n    """Get buffer string of messages."""\n    string_messages = []\n    for m in messages:\n        if isinstance(m, HumanMessage):\n            role = human_prefix\n        elif isinstance(m, AIMessage):\n            role = ai_prefix\n        elif isinstance(m, SystemMessage):\n            role = "System"\n        elif isinstance(m, ChatMessage):\n            role = m.role\n        else:\n            raise ValueError(f"Got unsupported message type: {m}")\n        string_messages.append(f"{role}: {m.content}")\n    return "\\n".join(string_messages)\n\nclass AgentAction(NamedTuple):\n    """Agent\'s action to take."""\n\nclass AgentFinish(NamedTuple):\n    """Agent\'s return value."""\n\nclass Generation(BaseModel):\n    """Output of a single generation."""\n\nclass BaseMessage(BaseModel):\n    """Message object."""\n\nclass HumanMessage(BaseMessage):\n    """Type of message that is spoken by the human."""\n\nclass AIMessage(BaseMessage):\n    """Type of message that is spoken by the AI."""\n\nclass SystemMessage(BaseMessage):\n    """Type of message that is a system message."""\n\nclass ChatMessage(BaseMessage):\n    """Type of message with arbitrary speaker."""\n\ndef _message_to_dict(message: BaseMessage) -> dict:\n    return {"type": message.type, "data": message.dict()}\n\ndef messages_to_dict(messages: List[BaseMessage]) -> List[dict]:\n    return [_message_to_dict(m) for m in messages]\n\ndef _message_from_dict(message: dict) -> BaseMessage:\n    _type = message["type"]\n    if _type == "human":\n        return HumanMessage(message["data"])\n    elif _type == "ai":\n        return AIMessage(message["data"])\n    elif _type == "system":\n        return SystemMessage(message["data"])\n    elif _type == "chat":\n        return ChatMessage(message["data"])\n    else:\n        raise ValueError(f"Got unexpected type: {_type}")\n\ndef messages_from_dict(messages: List[dict]) -> List[BaseMessage]:\n    return [_message_from_dict(m) for m in messages]\n\nclass ChatGeneration(Generation):\n    """Output of a single generation."""\n\nclass ChatResult(BaseModel):\n    """Class that contains all relevant information for a Chat Result."""\n\nclass LLMResult(BaseModel):\n    """Class that contains all relevant information for an LLM Result."""\n\nclass PromptValue(BaseModel, ABC):\n    @abstractmethod\n    def to_string(self) -> str:\n        """Return prompt as string."""\n\nclass BaseMemory(BaseModel, ABC):\n    """Base interface for memory in chains."""\n\nclass BaseChatMessageHistory(ABC):\n    """Base interface for chat message history\n    See ChatMessageHistory for default implementation.\n    """\n\nclass Document(BaseModel):\n    """Interface for interacting with a document."""\n\nclass BaseRetriever(ABC):\n    @abstractmethod\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        """Get documents relevant for a query.\n\nFor backwards compatibility\n\nMemory = BaseMemory\n\nT = TypeVar("T")\n\nclass BaseOutputParser(BaseModel, ABC, Generic[T]):\n    """Class to parse the output of an LLM call.\n\nclass OutputParserException(Exception):\n    """Exception that output parsers should raise to signify a parsing error.\n\nclass BaseDocumentTransformer(ABC):\n    """Base interface for transforming documents."""\n\n```', metadata={'source': 'embeddings\\schema.md'})] - Line 53
2023-05-03 22:22:40,661 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:40,662 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:40,666 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatiblity."""\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nall = ["SerpAPIWrapper"]\n\n```', metadata={'source': 'embeddings\\serpapi.md'})] - Line 53
2023-05-03 22:22:42,357 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:42,357 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:42,362 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Script to run langchain-server locally using docker-compose."""\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ndef main() -> None:\n    """Run the langchain server locally."""\n    p = Path(file).absolute().parent / "docker-compose.yaml"\n\nif name == "main":\n    main()\n\n```', metadata={'source': 'embeddings\\server.md'})] - Line 53
2023-05-03 22:22:43,844 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:43,844 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:43,856 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQLAlchemy wrapper around a database."""\nfrom future import annotations\n\nimport warnings\nfrom typing import Any, Iterable, List, Optional\n\nfrom sqlalchemy import MetaData, Table, create_engine, inspect, select, text\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.exc import ProgrammingError, SQLAlchemyError\nfrom sqlalchemy.schema import CreateTable\n\ndef _format_index(index: dict) -> str:\n    return (\n        f\'Name: {index["name"]}, Unique: {index["unique"]},\'\n        f\' Columns: {str(index["column_names"])}\'\n    )\n\nclass SQLDatabase:\n    """SQLAlchemy wrapper around a database."""\n\n```', metadata={'source': 'embeddings\\sql_database.md'})] - Line 53
2023-05-03 22:22:45,161 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:45,162 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:45,187 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for splitting text."""\nfrom future import annotations\n\nimport copy\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Union,\n)\n\nfrom langchain.docstore.document import Document\nfrom langchain.schema import BaseDocumentTransformer\n\nlogger = logging.getLogger(name)\n\nclass TextSplitter(BaseDocumentTransformer, ABC):\n    """Interface for splitting text into chunks."""\n\nclass CharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters."""\n\nclass TokenTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at tokens."""\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters.\n\nclass NLTKTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using NLTK."""\n\nclass SpacyTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using Spacy."""\n\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Markdown-formatted headings."""\n\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Latex-formatted layout elements."""\n\nclass PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Python syntax."""\n\n```', metadata={'source': 'embeddings\\text_splitter.md'})] - Line 53
2023-05-03 22:22:46,472 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:46,472 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:46,485 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Generic utility functions."""\nimport os\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\ndef get_from_dict_or_env(\n    data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None\n) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if key in data and data[key]:\n        return data[key]\n    else:\n        return get_from_env(key, env_key, default=default)\n\ndef get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if env_key in os.environ and os.environ[env_key]:\n        return os.environ[env_key]\n    elif default is not None:\n        return default\n    else:\n        raise ValueError(\n            f"Did not find {key}, please add an environment variable"\n            f" {env_key} which contains it, or pass"\n            f"  {key} as a named parameter."\n        )\n\ndef xor_args(*arg_groups: Tuple[str, ...]) -> Callable:\n    """Validate specified keyword args are mutually exclusive."""\n\ndef stringify_value(val: Any) -> str:\n    if isinstance(val, str):\n        return val\n    elif isinstance(val, dict):\n        return "\\n" + stringify_dict(val)\n    elif isinstance(val, list):\n        return "\\n".join(stringify_value(v) for v in val)\n    else:\n        return str(val)\n\ndef stringify_dict(data: dict) -> str:\n    text = ""\n    for key, value in data.items():\n        text += key + ": " + stringify_value(value) + "\\n"\n    return text\n\n```', metadata={'source': 'embeddings\\utils.md'})] - Line 53
2023-05-03 22:22:48,327 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:48,328 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:48,349 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Main entrypoint into package."""\n\nfrom importlib import metadata\nfrom typing import Optional\n\nfrom langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\nfrom langchain.cache import BaseCache\nfrom langchain.chains import (\n    ConversationChain,\n    LLMBashChain,\n    LLMChain,\n    LLMCheckerChain,\n    LLMMathChain,\n    PALChain,\n    QAWithSourcesChain,\n    SQLDatabaseChain,\n    VectorDBQA,\n    VectorDBQAWithSourcesChain,\n)\nfrom langchain.docstore import InMemoryDocstore, Wikipedia\nfrom langchain.llms import (\n    Anthropic,\n    Banana,\n    CerebriumAI,\n    Cohere,\n    ForefrontAI,\n    GooseAI,\n    HuggingFaceHub,\n    LlamaCpp,\n    Modal,\n    OpenAI,\n    Petals,\n    PipelineAI,\n    SagemakerEndpoint,\n    StochasticAI,\n    Writer,\n)\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import (\n    BasePromptTemplate,\n    FewShotPromptTemplate,\n    Prompt,\n    PromptTemplate,\n)\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.powerbi import PowerBIDataset\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom langchain.vectorstores import FAISS, ElasticVectorSearch\n\ntry:\n    version = metadata.version(package)\nexcept metadata.PackageNotFoundError:\n    # Case where package metadata is not available.\n    version = ""\ndel metadata  # optional, avoids polluting the results of dir(package)\n\nverbose: bool = False\nllm_cache: Optional[BaseCache] = None\n\nFor backwards compatibility\n\nSerpAPIChain = SerpAPIWrapper\n\nall = [\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMMathChain",\n    "ArxivAPIWrapper",\n    "SelfAskWithSearchChain",\n    "SerpAPIWrapper",\n    "SerpAPIChain",\n    "SearxSearchWrapper",\n    "GoogleSearchAPIWrapper",\n    "GoogleSerperAPIWrapper",\n    "WolframAlphaAPIWrapper",\n    "WikipediaAPIWrapper",\n    "Anthropic",\n    "Banana",\n    "CerebriumAI",\n    "Cohere",\n    "ForefrontAI",\n    "GooseAI",\n    "Modal",\n    "OpenAI",\n    "Petals",\n    "PipelineAI",\n    "StochasticAI",\n    "Writer",\n    "BasePromptTemplate",\n    "Prompt",\n    "FewShotPromptTemplate",\n    "PromptTemplate",\n    "ReActChain",\n    "Wikipedia",\n    "HuggingFaceHub",\n    "SagemakerEndpoint",\n    "HuggingFacePipeline",\n    "SQLDatabase",\n    "SQLDatabaseChain",\n    "PowerBIDataset",\n    "FAISS",\n    "MRKLChain",\n    "VectorDBQA",\n    "ElasticVectorSearch",\n    "InMemoryDocstore",\n    "ConversationChain",\n    "VectorDBQAWithSourcesChain",\n    "QAWithSourcesChain",\n    "PALChain",\n    "LlamaCpp",\n]\n\n```', metadata={'source': 'embeddings\\__init__.md'})] - Line 53
2023-05-03 22:22:49,618 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:49,618 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:49,647 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that takes in an input and produces an action and action input."""\nfrom future import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n\nimport yaml\nfrom pydantic import BaseModel, root_validator\n\nfrom langchain.agents.tools import InvalidTool\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForChainRun,\n    CallbackManagerForToolRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import (\n    AgentAction,\n    AgentFinish,\n    BaseMessage,\n    BaseOutputParser,\n)\nfrom langchain.tools.base import BaseTool\nfrom langchain.utilities.asyncio import asyncio_timeout\n\nlogger = logging.getLogger(name)\n\nclass BaseSingleActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass BaseMultiActionAgent(BaseModel):\n    """Base Agent class."""\n\nclass AgentOutputParser(BaseOutputParser):\n    @abstractmethod\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        """Parse text into agent action/finish."""\n\nclass LLMSingleActionAgent(BaseSingleActionAgent):\n    llm_chain: LLMChain\n    output_parser: AgentOutputParser\n    stop: List[str]\n\nclass Agent(BaseSingleActionAgent):\n    """Class responsible for calling the language model and deciding the action.\n\nclass ExceptionTool(BaseTool):\n    name = "_Exception"\n    description = "Exception tool"\n\nclass AgentExecutor(Chain):\n    """Consists of an agent using tools."""\n\n```', metadata={'source': 'embeddings\\agents\\agent.md'})] - Line 53
2023-05-03 22:22:50,831 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:50,832 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:50,838 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom enum import Enum\n\nclass AgentType(str, Enum):\n    ZERO_SHOT_REACT_DESCRIPTION = "zero-shot-react-description"\n    REACT_DOCSTORE = "react-docstore"\n    SELF_ASK_WITH_SEARCH = "self-ask-with-search"\n    CONVERSATIONAL_REACT_DESCRIPTION = "conversational-react-description"\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = "chat-zero-shot-react-description"\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = "chat-conversational-react-description"\n    STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = (\n        "structured-chat-zero-shot-react-description"\n    )\n\n```', metadata={'source': 'embeddings\\agents\\agent_types.md'})] - Line 53
2023-05-03 22:22:52,772 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:52,772 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:52,786 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Load agent."""\nfrom typing import Any, Optional, Sequence\n\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.tools.base import BaseTool\n\ndef initialize_agent(\n    tools: Sequence[BaseTool],\n    llm: BaseLanguageModel,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    """Load an agent executor given tools and LLM.\n\n```', metadata={'source': 'embeddings\\agents\\initialize.md'})] - Line 53
2023-05-03 22:22:54,160 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:54,160 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:54,175 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading agents."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport yaml\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.types import AGENT_TO_CLASS\nfrom langchain.chains.loading import load_chain, load_chain_from_config\nfrom langchain.llms.base import BaseLLM\nfrom langchain.utilities.loading import try_load_from_hub\n\nURL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/"\n\ndef _load_agent_from_tools(\n    config: dict, llm: BaseLLM, tools: List[Tool], **kwargs: Any\n) -> BaseSingleActionAgent:\n    config_type = config.pop("_type")\n    if config_type not in AGENT_TO_CLASS:\n        raise ValueError(f"Loading {config_type} agent not supported")\n\ndef load_agent_from_config(\n    config: dict,\n    llm: Optional[BaseLLM] = None,\n    tools: Optional[List[Tool]] = None,\n    kwargs: Any,\n) -> BaseSingleActionAgent:\n    """Load agent from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify an agent Type in config")\n    load_from_tools = config.pop("load_from_llm_and_tools", False)\n    if load_from_tools:\n        if llm is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then LLM must be provided"\n            )\n        if tools is None:\n            raise ValueError(\n                "If load_from_llm_and_tools is set to True, "\n                "then tools must be provided"\n            )\n        return _load_agent_from_tools(config, llm, tools, kwargs)\n    config_type = config.pop("_type")\n\ndef load_agent(path: Union[str, Path], kwargs: Any) -> BaseSingleActionAgent:\n    """Unified method for loading a agent from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_agent_from_file, "agents", {"json", "yaml"}\n    ):\n        return hub_result\n    else:\n        return _load_agent_from_file(path, kwargs)\n\ndef _load_agent_from_file(\n    file: Union[str, Path], kwargs: Any\n) -> BaseSingleActionAgent:\n    """Load agent from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")\n    # Load the agent from the config now.\n    return load_agent_from_config(config, kwargs)\n\n```', metadata={'source': 'embeddings\\agents\\loading.md'})] - Line 53
2023-05-03 22:22:55,309 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:55,310 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:55,382 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\n"""Load tools."""\nimport warnings\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\nfrom mypy_extensions import Arg, KwArg\n\nfrom langchain.agents.tools import Tool\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.requests import TextRequestsWrapper\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\nfrom langchain.tools.base import BaseTool\nfrom langchain.tools.bing_search.tool import BingSearchRun\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\nfrom langchain.tools.human.tool import HumanInputRun\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.tools.requests.tool import (\n    RequestsDeleteTool,\n    RequestsGetTool,\n    RequestsPatchTool,\n    RequestsPostTool,\n    RequestsPutTool,\n)\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\nfrom langchain.tools.shell.tool import ShellTool\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\nfrom langchain.utilities import ArxivAPIWrapper\nfrom langchain.utilities.apify import ApifyWrapper\nfrom langchain.utilities.bash import BashProcess\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.awslambda import LambdaWrapper\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n\ndef _get_python_repl() -> BaseTool:\n    return PythonREPLTool()\n\ndef _get_tools_requests_get() -> BaseTool:\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_post() -> BaseTool:\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_patch() -> BaseTool:\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_put() -> BaseTool:\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_tools_requests_delete() -> BaseTool:\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())\n\ndef _get_terminal() -> BaseTool:\n    return ShellTool()\n\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\n    "python_repl": _get_python_repl,\n    "requests": _get_tools_requests_get,  # preserved for backwards compatability\n    "requests_get": _get_tools_requests_get,\n    "requests_post": _get_tools_requests_post,\n    "requests_patch": _get_tools_requests_patch,\n    "requests_put": _get_tools_requests_put,\n    "requests_delete": _get_tools_requests_delete,\n    "terminal": _get_terminal,\n}\n\ndef _get_pal_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-MATH",\n        description="A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.",\n        func=PALChain.from_math_prompt(llm).run,\n    )\n\ndef _get_pal_colored_objects(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="PAL-COLOR-OBJ",\n        description="A language model that is really good at reasoning about position and the color attributes of objects. Input should be a fully worded hard reasoning problem. Make sure to include all information about the objects AND the final question you want to answer.",\n        func=PALChain.from_colored_object_prompt(llm).run,\n    )\n\ndef _get_llm_math(llm: BaseLLM) -> BaseTool:\n    return Tool(\n        name="Calculator",\n        description="Useful for when you need to answer questions about math.",\n        func=LLMMathChain.from_llm(llm=llm).run,\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\n    )\n\ndef _get_open_meteo_api(llm: BaseLLM) -> BaseTool:\n    chain = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS)\n    return Tool(\n        name="Open Meteo API",\n        description="Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\n_LLM_TOOLS: Dict[str, Callable[[BaseLLM], BaseTool]] = {\n    "pal-math": _get_pal_math,\n    "pal-colored-objects": _get_pal_colored_objects,\n    "llm-math": _get_llm_math,\n    "open-meteo-api": _get_open_meteo_api,\n}\n\ndef _get_news_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    news_api_key = kwargs["news_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm, news_docs.NEWS_DOCS, headers={"X-Api-Key": news_api_key}\n    )\n    return Tool(\n        name="News API",\n        description="Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_tmdb_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    tmdb_bearer_token = kwargs["tmdb_bearer_token"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        tmdb_docs.TMDB_DOCS,\n        headers={"Authorization": f"Bearer {tmdb_bearer_token}"},\n    )\n    return Tool(\n        name="TMDB API",\n        description="Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_podcast_api(llm: BaseLLM, **kwargs: Any) -> BaseTool:\n    listen_api_key = kwargs["listen_api_key"]\n    chain = APIChain.from_llm_and_api_docs(\n        llm,\n        podcast_docs.PODCAST_DOCS,\n        headers={"X-ListenAPI-Key": listen_api_key},\n    )\n    return Tool(\n        name="Podcast API",\n        description="Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.",\n        func=chain.run,\n    )\n\ndef _get_lambda_api(kwargs: Any) -> BaseTool:\n    return Tool(\n        name=kwargs["awslambda_tool_name"],\n        description=kwargs["awslambda_tool_description"],\n        func=LambdaWrapper(kwargs).run,\n    )\n\ndef _get_wolfram_alpha(kwargs: Any) -> BaseTool:\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(kwargs))\n\ndef _get_google_search(kwargs: Any) -> BaseTool:\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_wikipedia(kwargs: Any) -> BaseTool:\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(kwargs))\n\ndef _get_arxiv(kwargs: Any) -> BaseTool:\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(kwargs))\n\ndef _get_google_serper(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Serper Search",\n        func=GoogleSerperAPIWrapper(kwargs).run,\n        description="A low-cost Google Search API. Useful for when you need to answer questions about current events. Input should be a search query.",\n    )\n\ndef _get_google_search_results_json(kwargs: Any) -> BaseTool:\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(kwargs))\n\ndef _get_serpapi(kwargs: Any) -> BaseTool:\n    return Tool(\n        name="Search",\n        description="A search engine. Useful for when you need to answer questions about current events. Input should be a search query.",\n        func=SerpAPIWrapper(kwargs).run,\n        coroutine=SerpAPIWrapper(**kwargs).arun,\n    )\n\ndef _get_searx_search(kwargs: Any) -> BaseTool:\n    return SearxSearchRun(wrapper=SearxSearchWrapper(kwargs))\n\ndef _get_searx_search_results_json(kwargs: Any) -> BaseTool:\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != "num_results"}\n    return SearxSearchResults(wrapper=SearxSearchWrapper(wrapper_kwargs), **kwargs)\n\ndef _get_bing_search(kwargs: Any) -> BaseTool:\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(kwargs))\n\ndef _get_ddg_search(kwargs: Any) -> BaseTool:\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(kwargs))\n\ndef _get_human_tool(kwargs: Any) -> BaseTool:\n    return HumanInputRun(kwargs)\n\ndef _get_scenexplain(kwargs: Any) -> BaseTool:\n    return SceneXplainTool(kwargs)\n\n_EXTRA_LLM_TOOLS: Dict[\n    str, Tuple[Callable[[Arg(BaseLLM, "llm"), KwArg(Any)], BaseTool], List[str]]\n] = {\n    "news-api": (_get_news_api, ["news_api_key"]),\n    "tmdb-api": (_get_tmdb_api, ["tmdb_bearer_token"]),\n    "podcast-api": (_get_podcast_api, ["listen_api_key"]),\n}\n\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\n    "wolfram-alpha": (_get_wolfram_alpha, ["wolfram_alpha_appid"]),\n    "google-search": (_get_google_search, ["google_api_key", "google_cse_id"]),\n    "google-search-results-json": (\n        _get_google_search_results_json,\n        ["google_api_key", "google_cse_id", "num_results"],\n    ),\n    "searx-search-results-json": (\n        _get_searx_search_results_json,\n        ["searx_host", "engines", "num_results", "aiosession"],\n    ),\n    "bing-search": (_get_bing_search, ["bing_subscription_key", "bing_search_url"]),\n    "ddg-search": (_get_ddg_search, []),\n    "google-serper": (_get_google_serper, ["serper_api_key"]),\n    "serpapi": (_get_serpapi, ["serpapi_api_key", "aiosession"]),\n    "searx-search": (_get_searx_search, ["searx_host", "engines", "aiosession"]),\n    "wikipedia": (_get_wikipedia, ["top_k_results", "lang"]),\n    "arxiv": (\n        _get_arxiv,\n        ["top_k_results", "load_max_docs", "load_all_available_meta"],\n    ),\n    "human": (_get_human_tool, ["prompt_func", "input_func"]),\n    "awslambda": (\n        _get_lambda_api,\n        ["awslambda_tool_name", "awslambda_tool_description", "function_name"],\n    ),\n    "sceneXplain": (_get_scenexplain, []),\n}\n\ndef load_tools(\n    tool_names: List[str],\n    llm: Optional[BaseLLM] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    **kwargs: Any,\n) -> List[BaseTool]:\n    """Load tools based on their name.\n\ndef get_all_tool_names() -> List[str]:\n    """Get a list of all possible tool names."""\n    return (\n        list(_BASE_TOOLS)\n        + list(_EXTRA_OPTIONAL_TOOLS)\n        + list(_EXTRA_LLM_TOOLS)\n        + list(_LLM_TOOLS)\n    )\n\n```', metadata={'source': 'embeddings\\agents\\load_tools.md'})] - Line 53
2023-05-03 22:22:56,840 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:56,840 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:56,849 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.schema import AgentAction\n\nclass AgentScratchPadChatPromptTemplate(ChatPromptTemplate):\n    def _construct_agent_scratchpad(\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\n    ) -> str:\n        if len(intermediate_steps) == 0:\n            return ""\n        thoughts = ""\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f"\\nObservation: {observation}\\nThought: "\n        return (\n            f"This was your previous work "\n            f"(but I haven\'t seen any of it! I only see what "\n            f"you return as final answer):\\n{thoughts}"\n        )\n\n```', metadata={'source': 'embeddings\\agents\\schema.md'})] - Line 53
2023-05-03 22:22:58,521 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:58,521 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:58,529 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for tools."""\nfrom typing import Optional\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain.tools.base import BaseTool, Tool, tool\n\nclass InvalidTool(BaseTool):\n    """Tool that is run when invalid tool name is encountered by agent."""\n\nall = ["InvalidTool", "BaseTool", "tool", "Tool"]\n\n```', metadata={'source': 'embeddings\\agents\\tools.md'})] - Line 53
2023-05-03 22:22:59,694 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:22:59,694 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:22:59,704 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Dict, Type\n\nfrom langchain.agents.agent import BaseSingleActionAgent\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.chat.base import ChatAgent\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.mrkl.base import ZeroShotAgent\nfrom langchain.agents.react.base import ReActDocstoreAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchAgent\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\n\nAGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] = {\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n}\n\n```', metadata={'source': 'embeddings\\agents\\types.md'})] - Line 53
2023-05-03 22:23:01,040 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:01,040 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:01,046 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Sequence\n\nfrom langchain.tools.base import BaseTool\n\ndef validate_tools_single_input(class_name: str, tools: Sequence[BaseTool]) -> None:\n    """Validate tools for single input."""\n    for tool in tools:\n        if not tool.is_single_input:\n            raise ValueError(\n                f"{class_name} does not support multi-input tool {tool.name}."\n            )\n\n```', metadata={'source': 'embeddings\\agents\\utils.md'})] - Line 53
2023-05-03 22:23:02,923 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:02,923 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:02,932 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Interface for agents."""\nfrom langchain.agents.agent import (\n    Agent,\n    AgentExecutor,\n    AgentOutputParser,\n    BaseMultiActionAgent,\n    BaseSingleActionAgent,\n    LLMSingleActionAgent,\n)\nfrom langchain.agents.agent_toolkits import (\n    create_csv_agent,\n    create_json_agent,\n    create_openapi_agent,\n    create_pandas_dataframe_agent,\n    create_pbi_agent,\n    create_pbi_chat_agent,\n    create_sql_agent,\n    create_vectorstore_agent,\n    create_vectorstore_router_agent,\n)\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.conversational.base import ConversationalAgent\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\nfrom langchain.agents.initialize import initialize_agent\nfrom langchain.agents.load_tools import get_all_tool_names, load_tools\nfrom langchain.agents.loading import load_agent\nfrom langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent\nfrom langchain.agents.react.base import ReActChain, ReActTextWorldAgent\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchChain\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\nfrom langchain.agents.tools import Tool, tool\n\nall = [\n    "Agent",\n    "AgentExecutor",\n    "AgentOutputParser",\n    "AgentType",\n    "BaseMultiActionAgent",\n    "BaseSingleActionAgent",\n    "ConversationalAgent",\n    "ConversationalChatAgent",\n    "LLMSingleActionAgent",\n    "MRKLChain",\n    "ReActChain",\n    "ReActTextWorldAgent",\n    "SelfAskWithSearchChain",\n    "StructuredChatAgent",\n    "Tool",\n    "ZeroShotAgent",\n    "create_csv_agent",\n    "create_json_agent",\n    "create_openapi_agent",\n    "create_pandas_dataframe_agent",\n    "create_pbi_agent",\n    "create_pbi_chat_agent",\n    "create_sql_agent",\n    "create_vectorstore_agent",\n    "create_vectorstore_router_agent",\n    "get_all_tool_names",\n    "initialize_agent",\n    "load_agent",\n    "load_tools",\n    "tool",\n]\n\n```', metadata={'source': 'embeddings\\agents\\__init__.md'})] - Line 53
2023-05-03 22:23:04,307 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:04,307 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:04,320 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that does self ask with search."""\nfrom typing import Any, Sequence, Union\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\nfrom langchain.agents.agent_types import AgentType\nfrom langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser\nfrom langchain.agents.self_ask_with_search.prompt import PROMPT\nfrom langchain.agents.tools import Tool\nfrom langchain.agents.utils import validate_tools_single_input\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.tools.base import BaseTool\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nclass SelfAskWithSearchAgent(Agent):\n    """Agent for the self-ask-with-search paper."""\n\nclass SelfAskWithSearchChain(AgentExecutor):\n    """Chain that does self ask with search.\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\base.md'})] - Line 53
2023-05-03 22:23:05,613 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:05,614 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:05,622 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom typing import Union\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nclass SelfAskOutputParser(AgentOutputParser):\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n        followup = "Follow up:"\n        last_line = text.split("\\n")[-1]\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\output_parser.md'})] - Line 53
2023-05-03 22:23:06,870 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:06,870 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:06,880 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n_DEFAULT_TEMPLATE = """Question: Who lived longer, Muhammad Ali or Alan Turing?\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n\nQuestion: When was the founder of craigslist born?\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\n\nQuestion: Who was the maternal grandfather of George Washington?\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\n\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Jaws?\nIntermediate answer: The director of Jaws is Steven Spielberg.\nFollow up: Where is Steven Spielberg from?\nIntermediate answer: The United States.\nFollow up: Who is the director of Casino Royale?\nIntermediate answer: The director of Casino Royale is Martin Campbell.\nFollow up: Where is Martin Campbell from?\nIntermediate answer: New Zealand.\nSo the final answer is: No\n\nQuestion: {input}\nAre followup questions needed here:{agent_scratchpad}"""\nPROMPT = PromptTemplate(\n    input_variables=["input", "agent_scratchpad"], template=_DEFAULT_TEMPLATE\n)\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\prompt.md'})] - Line 53
2023-05-03 22:23:08,561 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:08,562 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:08,567 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that does self ask with search.\n\nHeavily borrowed from https://github.com/ofirpress/self-ask\n"""\n\n```', metadata={'source': 'embeddings\\agents\\self_ask_with_search\\__init__.md'})] - Line 53
2023-05-03 22:23:09,947 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:09,948 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:09,959 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport re\nfrom typing import Any, List, Optional, Sequence, Tuple\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import Agent, AgentOutputParser\nfrom langchain.agents.structured_chat.output_parser import (\n    StructuredChatOutputParserWithRetries,\n)\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.schema import AgentAction\nfrom langchain.tools import BaseTool\n\nclass StructuredChatAgent(Agent):\n    output_parser: AgentOutputParser = Field(\n        default_factory=StructuredChatOutputParserWithRetries\n    )\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\base.md'})] - Line 53
2023-05-03 22:23:11,215 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:11,215 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:11,228 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations\n\nimport json\nimport logging\nimport re\nfrom typing import Optional, Union\n\nfrom pydantic import Field\n\nfrom langchain.agents.agent import AgentOutputParser\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.output_parsers import OutputFixingParser\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\n\nlogger = logging.getLogger(name)\n\nclass StructuredChatOutputParser(AgentOutputParser):\n    def get_format_instructions(self) -> str:\n        return FORMAT_INSTRUCTIONS\n\nclass StructuredChatOutputParserWithRetries(AgentOutputParser):\n    base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)\n    output_fixing_parser: Optional[OutputFixingParser] = None\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\output_parser.md'})] - Line 53
2023-05-03 22:23:13,205 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:13,206 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:13,214 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\nflake8: noqa\n\nPREFIX = """Respond to the human as helpfully and accurately as possible. You have access to the following tools:"""\nFORMAT_INSTRUCTIONS = """Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\nValid "action" values: "Final Answer" or {tool_names}\n\nProvide only ONE action per $JSON_BLOB, as shown:\n\n{{{{\n  "action": $TOOL_NAME,\n  "action_input": $INPUT\n}}}}\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n$JSON_BLOB\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n{{{{\n  "action": "Final Answer",\n  "action_input": "Final response to human"\n}}}}"""\nSUFFIX = """Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:$JSON_BLOBthen Observation:.\nThought:"""\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\prompt.md'})] - Line 53
2023-05-03 22:23:14,761 - ye_logger_of_yor - INFO - creating embedding - Line 84
2023-05-03 22:23:14,761 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:23:14,764 - ye_logger_of_yor - INFO - [Document(page_content='```python\n\n```', metadata={'source': 'embeddings\\agents\\structured_chat\\__init__.md'})] - Line 53
2023-05-03 22:24:44,848 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:24:44,849 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:24:44,849 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:24:45,813 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:24:45,814 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:24:45,814 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:24:45,814 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:24:45,814 - ye_logger_of_yor - INFO - create_embedding function - Line 83
2023-05-03 22:24:45,814 - ye_logger_of_yor - INFO - load_embedding function - Line 102
2023-05-03 22:24:45,815 - ye_logger_of_yor - INFO - base_retriever function - Line 108
2023-05-03 22:24:45,815 - ye_logger_of_yor - INFO - retriever function - Line 116
2023-05-03 22:24:45,815 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 127
2023-05-03 22:24:45,815 - ye_logger_of_yor - INFO - memory_search function - Line 135
2023-05-03 22:24:46,030 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:24:46,031 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:24:46,031 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:24:46,032 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:24:46,032 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:24:46,032 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:24:46,032 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:24:46,033 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:24:46,033 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:24:46,033 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:24:46,033 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:24:46,034 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:24:46,034 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:24:46,034 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:24:46,034 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:24:46,034 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:24:46,035 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:24:46,035 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:24:46,035 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:24:46,035 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:24:46,036 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:24:46,036 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:24:46,036 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:24:46,036 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:24:46,036 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:24:46,037 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:24:46,038 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:24:46,038 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:24:46,038 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:24:46,038 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:24:46,038 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:25:32,293 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:32,294 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:32,811 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base class for all language models."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom langchain.callbacks.manager import Callbacks\nfrom langchain.schema import BaseMessage, LLMResult, PromptValue, get_buffer_string\n\ndef _get_num_tokens_default_method(text: str) -> int:\n    """Get the number of tokens present in the text."""\n    # TODO: this method may not be exact.\n    # TODO: this method may differ based on model (eg codex).\n    try:\n        from transformers import GPT2TokenizerFast\n    except ImportError:\n        raise ValueError(\n            "Could not import transformers python package. "\n            "This is needed in order to calculate get_num_tokens. "\n            "Please install it with pip install transformers."\n        )\n    # create a GPT-2 tokenizer instance\n    tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")\n\nclass BaseLanguageModel(BaseModel, ABC):\n    @abstractmethod\n    def generate_prompt(\n        self,\n        prompts: List[PromptValue],\n        stop: Optional[List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -> LLMResult:\n        """Take in a list of prompt values and return an LLMResult."""\n\n```', metadata={'source': 'embeddings\\base_language.md'})] - Line 53
2023-05-03 22:25:35,523 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:35,524 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:35,552 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Beta Feature: base interface for cache."""\nimport hashlib\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, cast\n\nfrom sqlalchemy import Column, Integer, String, create_engine, select\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.orm import Session\n\ntry:\n    from sqlalchemy.orm import declarative_base\nexcept ImportError:\n    from sqlalchemy.ext.declarative import declarative_base\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.schema import Generation\nfrom langchain.vectorstores.redis import Redis as RedisVectorstore\n\nRETURN_VAL_TYPE = List[Generation]\n\ndef _hash(_input: str) -> str:\n    """Use a deterministic hashing approach."""\n    return hashlib.md5(_input.encode()).hexdigest()\n\nclass BaseCache(ABC):\n    """Base interface for cache."""\n\nclass InMemoryCache(BaseCache):\n    """Cache that stores things in memory."""\n\nBase = declarative_base()\n\nclass FullLLMCache(Base):  # type: ignore\n    """SQLite table for full LLM Cache (all generations)."""\n\nclass SQLAlchemyCache(BaseCache):\n    """Cache that uses SQAlchemy as a backend."""\n\nclass SQLiteCache(SQLAlchemyCache):\n    """Cache that uses SQLite as a backend."""\n\nclass RedisCache(BaseCache):\n    """Cache that uses Redis as a backend."""\n\nclass RedisSemanticCache(BaseCache):\n    """Cache that uses Redis as a vector-store backend."""\n\nclass GPTCache(BaseCache):\n    """Cache that uses GPTCache as a backend."""\n\n```', metadata={'source': 'embeddings\\cache.md'})] - Line 53
2023-05-03 22:25:36,838 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:36,839 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:36,862 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Transform documents"""\nfrom typing import Any, Callable, List, Sequence\n\nimport numpy as np\nfrom pydantic import BaseModel, Field\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.math_utils import cosine_similarity\nfrom langchain.schema import BaseDocumentTransformer, Document\n\nclass _DocumentWithState(Document):\n    """Wrapper for a document that includes arbitrary state."""\n\ndef get_stateful_documents(\n    documents: Sequence[Document],\n) -> Sequence[_DocumentWithState]:\n    return [_DocumentWithState.from_document(doc) for doc in documents]\n\ndef _filter_similar_embeddings(\n    embedded_documents: List[List[float]], similarity_fn: Callable, threshold: float\n) -> List[int]:\n    """Filter redundant documents based on the similarity of their embeddings."""\n    similarity = np.tril(similarity_fn(embedded_documents, embedded_documents), k=-1)\n    redundant = np.where(similarity > threshold)\n    redundant_stacked = np.column_stack(redundant)\n    redundant_sorted = np.argsort(similarity[redundant])[::-1]\n    included_idxs = set(range(len(embedded_documents)))\n    for first_idx, second_idx in redundant_stacked[redundant_sorted]:\n        if first_idx in included_idxs and second_idx in included_idxs:\n            # Default to dropping the second document of any highly similar pair.\n            included_idxs.remove(second_idx)\n    return list(sorted(included_idxs))\n\ndef _get_embeddings_from_stateful_docs(\n    embeddings: Embeddings, documents: Sequence[_DocumentWithState]\n) -> List[List[float]]:\n    if len(documents) and "embedded_doc" in documents[0].state:\n        embedded_documents = [doc.state["embedded_doc"] for doc in documents]\n    else:\n        embedded_documents = embeddings.embed_documents(\n            [d.page_content for d in documents]\n        )\n        for doc, embedding in zip(documents, embedded_documents):\n            doc.state["embedded_doc"] = embedding\n    return embedded_documents\n\nclass EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n    """Filter that drops redundant documents by comparing their embeddings."""\n\n```', metadata={'source': 'embeddings\\document_transformers.md'})] - Line 53
2023-05-03 22:25:38,216 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:38,217 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:38,223 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utility functions for working with prompts."""\nfrom typing import List\n\nfrom langchain.chains.llm import LLMChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\n\nTEST_GEN_TEMPLATE_SUFFIX = "Add another example."\n\ndef generate_example(\n    examples: List[dict], llm: BaseLLM, prompt_template: PromptTemplate\n) -> str:\n    """Return another example given a list of examples for a prompt."""\n    prompt = FewShotPromptTemplate(\n        examples=examples,\n        suffix=TEST_GEN_TEMPLATE_SUFFIX,\n        input_variables=[],\n        example_prompt=prompt_template,\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    return chain.predict()\n\n```', metadata={'source': 'embeddings\\example_generator.md'})] - Line 53
2023-05-03 22:25:40,270 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:40,271 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:40,277 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Utilities for formatting strings."""\nfrom string import Formatter\nfrom typing import Any, List, Mapping, Sequence, Union\n\nclass StrictFormatter(Formatter):\n    """A subclass of formatter that checks for extra keys."""\n\nformatter = StrictFormatter()\n\n```', metadata={'source': 'embeddings\\formatting.md'})] - Line 53
2023-05-03 22:25:42,032 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:42,032 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:42,037 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Handle chained inputs."""\nfrom typing import Dict, List, Optional\n\n_TEXT_COLOR_MAPPING = {\n    "blue": "36;1",\n    "yellow": "33;1",\n    "pink": "38;5;200",\n    "green": "32;1",\n    "red": "31;1",\n}\n\ndef get_color_mapping(\n    items: List[str], excluded_colors: Optional[List] = None\n) -> Dict[str, str]:\n    """Get mapping for items to a support color."""\n    colors = list(_TEXT_COLOR_MAPPING.keys())\n    if excluded_colors is not None:\n        colors = [c for c in colors if c not in excluded_colors]\n    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}\n    return color_mapping\n\ndef get_colored_text(text: str, color: str) -> str:\n    """Get colored text."""\n    color_str = _TEXT_COLOR_MAPPING[color]\n    return f"\\u001b[{color_str}m\\033[1;3m{text}\\u001b[0m"\n\ndef print_text(text: str, color: Optional[str] = None, end: str = "") -> None:\n    """Print text with highlighting and no end characters."""\n    if color is None:\n        text_to_print = text\n    else:\n        text_to_print = get_colored_text(text, color)\n    print(text_to_print, end=end)\n\n```', metadata={'source': 'embeddings\\input.md'})] - Line 53
2023-05-03 22:25:43,365 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:43,365 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:43,371 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Math utils."""\nfrom typing import List, Union\n\nimport numpy as np\n\nMatrix = Union[List[List[float]], List[np.ndarray], np.ndarray]\n\ndef cosine_similarity(X: Matrix, Y: Matrix) -> np.ndarray:\n    """Row-wise cosine similarity between two equal-width matrices."""\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f"Number of columns in X and Y must be the same. X has shape {X.shape} "\n            f"and Y has shape {Y.shape}."\n        )\n\n```', metadata={'source': 'embeddings\\math_utils.md'})] - Line 53
2023-05-03 22:25:45,485 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:45,485 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:45,493 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Experiment with different models."""\nfrom future import annotations\n\nfrom typing import List, Optional, Sequence\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.input import get_color_mapping, print_text\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.prompt import PromptTemplate\n\nclass ModelLaboratory:\n    """Experiment with different models."""\n\n```', metadata={'source': 'embeddings\\model_laboratory.md'})] - Line 53
2023-05-03 22:25:46,928 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:46,928 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:46,932 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatibility."""\nfrom langchain.utilities.python import PythonREPL\n\nall = ["PythonREPL"]\n\n```', metadata={'source': 'embeddings\\python.md'})] - Line 53
2023-05-03 22:25:48,363 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:48,363 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:48,373 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Lightweight wrapper around requests library, with async support."""\nfrom contextlib import asynccontextmanager\nfrom typing import Any, AsyncGenerator, Dict, Optional\n\nimport aiohttp\nimport requests\nfrom pydantic import BaseModel, Extra\n\nclass Requests(BaseModel):\n    """Wrapper around requests to handle auth and async.\n\nclass TextRequestsWrapper(BaseModel):\n    """Lightweight wrapper around requests library.\n\nFor backwards compatibility\n\nRequestsWrapper = TextRequestsWrapper\n\n```', metadata={'source': 'embeddings\\requests.md'})] - Line 53
2023-05-03 22:25:49,859 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:49,859 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:49,907 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Common schema objects."""\nfrom future import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, Extra, Field, root_validator\n\ndef get_buffer_string(\n    messages: List[BaseMessage], human_prefix: str = "Human", ai_prefix: str = "AI"\n) -> str:\n    """Get buffer string of messages."""\n    string_messages = []\n    for m in messages:\n        if isinstance(m, HumanMessage):\n            role = human_prefix\n        elif isinstance(m, AIMessage):\n            role = ai_prefix\n        elif isinstance(m, SystemMessage):\n            role = "System"\n        elif isinstance(m, ChatMessage):\n            role = m.role\n        else:\n            raise ValueError(f"Got unsupported message type: {m}")\n        string_messages.append(f"{role}: {m.content}")\n    return "\\n".join(string_messages)\n\nclass AgentAction(NamedTuple):\n    """Agent\'s action to take."""\n\nclass AgentFinish(NamedTuple):\n    """Agent\'s return value."""\n\nclass Generation(BaseModel):\n    """Output of a single generation."""\n\nclass BaseMessage(BaseModel):\n    """Message object."""\n\nclass HumanMessage(BaseMessage):\n    """Type of message that is spoken by the human."""\n\nclass AIMessage(BaseMessage):\n    """Type of message that is spoken by the AI."""\n\nclass SystemMessage(BaseMessage):\n    """Type of message that is a system message."""\n\nclass ChatMessage(BaseMessage):\n    """Type of message with arbitrary speaker."""\n\ndef _message_to_dict(message: BaseMessage) -> dict:\n    return {"type": message.type, "data": message.dict()}\n\ndef messages_to_dict(messages: List[BaseMessage]) -> List[dict]:\n    return [_message_to_dict(m) for m in messages]\n\ndef _message_from_dict(message: dict) -> BaseMessage:\n    _type = message["type"]\n    if _type == "human":\n        return HumanMessage(message["data"])\n    elif _type == "ai":\n        return AIMessage(message["data"])\n    elif _type == "system":\n        return SystemMessage(message["data"])\n    elif _type == "chat":\n        return ChatMessage(message["data"])\n    else:\n        raise ValueError(f"Got unexpected type: {_type}")\n\ndef messages_from_dict(messages: List[dict]) -> List[BaseMessage]:\n    return [_message_from_dict(m) for m in messages]\n\nclass ChatGeneration(Generation):\n    """Output of a single generation."""\n\nclass ChatResult(BaseModel):\n    """Class that contains all relevant information for a Chat Result."""\n\nclass LLMResult(BaseModel):\n    """Class that contains all relevant information for an LLM Result."""\n\nclass PromptValue(BaseModel, ABC):\n    @abstractmethod\n    def to_string(self) -> str:\n        """Return prompt as string."""\n\nclass BaseMemory(BaseModel, ABC):\n    """Base interface for memory in chains."""\n\nclass BaseChatMessageHistory(ABC):\n    """Base interface for chat message history\n    See ChatMessageHistory for default implementation.\n    """\n\nclass Document(BaseModel):\n    """Interface for interacting with a document."""\n\nclass BaseRetriever(ABC):\n    @abstractmethod\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        """Get documents relevant for a query.\n\nFor backwards compatibility\n\nMemory = BaseMemory\n\nT = TypeVar("T")\n\nclass BaseOutputParser(BaseModel, ABC, Generic[T]):\n    """Class to parse the output of an LLM call.\n\nclass OutputParserException(Exception):\n    """Exception that output parsers should raise to signify a parsing error.\n\nclass BaseDocumentTransformer(ABC):\n    """Base interface for transforming documents."""\n\n```', metadata={'source': 'embeddings\\schema.md'})] - Line 53
2023-05-03 22:25:51,837 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:51,838 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:51,842 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""For backwards compatiblity."""\nfrom langchain.utilities.serpapi import SerpAPIWrapper\n\nall = ["SerpAPIWrapper"]\n\n```', metadata={'source': 'embeddings\\serpapi.md'})] - Line 53
2023-05-03 22:25:53,058 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:53,058 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:53,063 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Script to run langchain-server locally using docker-compose."""\nimport shutil\nimport subprocess\nfrom pathlib import Path\n\ndef main() -> None:\n    """Run the langchain server locally."""\n    p = Path(file).absolute().parent / "docker-compose.yaml"\n\nif name == "main":\n    main()\n\n```', metadata={'source': 'embeddings\\server.md'})] - Line 53
2023-05-03 22:25:54,299 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:54,299 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:54,312 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""SQLAlchemy wrapper around a database."""\nfrom future import annotations\n\nimport warnings\nfrom typing import Any, Iterable, List, Optional\n\nfrom sqlalchemy import MetaData, Table, create_engine, inspect, select, text\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.exc import ProgrammingError, SQLAlchemyError\nfrom sqlalchemy.schema import CreateTable\n\ndef _format_index(index: dict) -> str:\n    return (\n        f\'Name: {index["name"]}, Unique: {index["unique"]},\'\n        f\' Columns: {str(index["column_names"])}\'\n    )\n\nclass SQLDatabase:\n    """SQLAlchemy wrapper around a database."""\n\n```', metadata={'source': 'embeddings\\sql_database.md'})] - Line 53
2023-05-03 22:25:56,320 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:56,321 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:56,345 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for splitting text."""\nfrom future import annotations\n\nimport copy\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Union,\n)\n\nfrom langchain.docstore.document import Document\nfrom langchain.schema import BaseDocumentTransformer\n\nlogger = logging.getLogger(name)\n\nclass TextSplitter(BaseDocumentTransformer, ABC):\n    """Interface for splitting text into chunks."""\n\nclass CharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters."""\n\nclass TokenTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at tokens."""\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at characters.\n\nclass NLTKTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using NLTK."""\n\nclass SpacyTextSplitter(TextSplitter):\n    """Implementation of splitting text that looks at sentences using Spacy."""\n\nclass MarkdownTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Markdown-formatted headings."""\n\nclass LatexTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Latex-formatted layout elements."""\n\nclass PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\n    """Attempts to split the text along Python syntax."""\n\n```', metadata={'source': 'embeddings\\text_splitter.md'})] - Line 53
2023-05-03 22:25:57,628 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:57,629 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:57,641 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Generic utility functions."""\nimport os\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\ndef get_from_dict_or_env(\n    data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None\n) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if key in data and data[key]:\n        return data[key]\n    else:\n        return get_from_env(key, env_key, default=default)\n\ndef get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:\n    """Get a value from a dictionary or an environment variable."""\n    if env_key in os.environ and os.environ[env_key]:\n        return os.environ[env_key]\n    elif default is not None:\n        return default\n    else:\n        raise ValueError(\n            f"Did not find {key}, please add an environment variable"\n            f" {env_key} which contains it, or pass"\n            f"  {key} as a named parameter."\n        )\n\ndef xor_args(*arg_groups: Tuple[str, ...]) -> Callable:\n    """Validate specified keyword args are mutually exclusive."""\n\ndef stringify_value(val: Any) -> str:\n    if isinstance(val, str):\n        return val\n    elif isinstance(val, dict):\n        return "\\n" + stringify_dict(val)\n    elif isinstance(val, list):\n        return "\\n".join(stringify_value(v) for v in val)\n    else:\n        return str(val)\n\ndef stringify_dict(data: dict) -> str:\n    text = ""\n    for key, value in data.items():\n        text += key + ": " + stringify_value(value) + "\\n"\n    return text\n\n```', metadata={'source': 'embeddings\\utils.md'})] - Line 53
2023-05-03 22:25:59,013 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:25:59,013 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:25:59,035 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Main entrypoint into package."""\n\nfrom importlib import metadata\nfrom typing import Optional\n\nfrom langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\nfrom langchain.cache import BaseCache\nfrom langchain.chains import (\n    ConversationChain,\n    LLMBashChain,\n    LLMChain,\n    LLMCheckerChain,\n    LLMMathChain,\n    PALChain,\n    QAWithSourcesChain,\n    SQLDatabaseChain,\n    VectorDBQA,\n    VectorDBQAWithSourcesChain,\n)\nfrom langchain.docstore import InMemoryDocstore, Wikipedia\nfrom langchain.llms import (\n    Anthropic,\n    Banana,\n    CerebriumAI,\n    Cohere,\n    ForefrontAI,\n    GooseAI,\n    HuggingFaceHub,\n    LlamaCpp,\n    Modal,\n    OpenAI,\n    Petals,\n    PipelineAI,\n    SagemakerEndpoint,\n    StochasticAI,\n    Writer,\n)\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.prompts import (\n    BasePromptTemplate,\n    FewShotPromptTemplate,\n    Prompt,\n    PromptTemplate,\n)\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\nfrom langchain.utilities.powerbi import PowerBIDataset\nfrom langchain.utilities.searx_search import SearxSearchWrapper\nfrom langchain.utilities.serpapi import SerpAPIWrapper\nfrom langchain.utilities.wikipedia import WikipediaAPIWrapper\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\nfrom langchain.vectorstores import FAISS, ElasticVectorSearch\n\ntry:\n    version = metadata.version(package)\nexcept metadata.PackageNotFoundError:\n    # Case where package metadata is not available.\n    version = ""\ndel metadata  # optional, avoids polluting the results of dir(package)\n\nverbose: bool = False\nllm_cache: Optional[BaseCache] = None\n\nFor backwards compatibility\n\nSerpAPIChain = SerpAPIWrapper\n\nall = [\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMMathChain",\n    "ArxivAPIWrapper",\n    "SelfAskWithSearchChain",\n    "SerpAPIWrapper",\n    "SerpAPIChain",\n    "SearxSearchWrapper",\n    "GoogleSearchAPIWrapper",\n    "GoogleSerperAPIWrapper",\n    "WolframAlphaAPIWrapper",\n    "WikipediaAPIWrapper",\n    "Anthropic",\n    "Banana",\n    "CerebriumAI",\n    "Cohere",\n    "ForefrontAI",\n    "GooseAI",\n    "Modal",\n    "OpenAI",\n    "Petals",\n    "PipelineAI",\n    "StochasticAI",\n    "Writer",\n    "BasePromptTemplate",\n    "Prompt",\n    "FewShotPromptTemplate",\n    "PromptTemplate",\n    "ReActChain",\n    "Wikipedia",\n    "HuggingFaceHub",\n    "SagemakerEndpoint",\n    "HuggingFacePipeline",\n    "SQLDatabase",\n    "SQLDatabaseChain",\n    "PowerBIDataset",\n    "FAISS",\n    "MRKLChain",\n    "VectorDBQA",\n    "ElasticVectorSearch",\n    "InMemoryDocstore",\n    "ConversationChain",\n    "VectorDBQAWithSourcesChain",\n    "QAWithSourcesChain",\n    "PALChain",\n    "LlamaCpp",\n]\n\n```', metadata={'source': 'embeddings\\__init__.md'})] - Line 53
2023-05-03 22:28:58,666 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:28:58,667 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:28:58,667 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:28:59,642 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:28:59,642 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:28:59,642 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:28:59,642 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:28:59,642 - ye_logger_of_yor - INFO - create_embedding function - Line 83
2023-05-03 22:28:59,643 - ye_logger_of_yor - INFO - load_embedding function - Line 102
2023-05-03 22:28:59,643 - ye_logger_of_yor - INFO - base_retriever function - Line 108
2023-05-03 22:28:59,643 - ye_logger_of_yor - INFO - retriever function - Line 116
2023-05-03 22:28:59,643 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 127
2023-05-03 22:28:59,643 - ye_logger_of_yor - INFO - memory_search function - Line 135
2023-05-03 22:28:59,848 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:28:59,848 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:28:59,849 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:28:59,850 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:28:59,850 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:28:59,850 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:28:59,850 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:28:59,850 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:28:59,851 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:28:59,852 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:28:59,853 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:29:08,327 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 22:29:20,790 - ye_logger_of_yor - INFO - creating mass embedding - Line 62
2023-05-03 22:29:20,790 - ye_logger_of_yor - INFO - creating embedding - Line 86
2023-05-03 22:29:20,791 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:30:32,554 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:30:32,555 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:30:32,555 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:30:33,487 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:30:33,487 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:30:33,487 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:30:33,487 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - create_embedding function - Line 83
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - load_embedding function - Line 102
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - base_retriever function - Line 108
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - retriever function - Line 116
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 127
2023-05-03 22:30:33,488 - ye_logger_of_yor - INFO - memory_search function - Line 135
2023-05-03 22:30:33,692 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:30:33,693 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:30:33,693 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:30:33,693 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:30:33,694 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:30:33,695 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:30:33,696 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:30:33,697 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:30:33,698 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:30:33,699 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:35:15,813 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:35:15,813 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:35:15,813 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:35:16,830 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:35:16,830 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:35:16,830 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:35:16,830 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - create_embedding function - Line 81
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - load_embedding function - Line 100
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - base_retriever function - Line 106
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - retriever function - Line 114
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 125
2023-05-03 22:35:16,831 - ye_logger_of_yor - INFO - memory_search function - Line 133
2023-05-03 22:35:17,049 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:35:17,049 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:35:17,049 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:35:17,049 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:35:17,050 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:35:17,051 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:35:17,051 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:35:17,051 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:35:17,051 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:35:17,052 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:35:17,053 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:35:17,054 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:35:17,055 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:35:17,055 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:35:17,055 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:35:17,055 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:35:17,055 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:36:02,692 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:36:02,692 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:36:02,692 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:36:32,608 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:36:32,609 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:36:32,609 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:37:21,764 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:37:21,764 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:37:21,764 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:37:22,845 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:37:22,845 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 59
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - create_embedding function - Line 81
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - load_embedding function - Line 100
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - base_retriever function - Line 106
2023-05-03 22:37:22,846 - ye_logger_of_yor - INFO - retriever function - Line 114
2023-05-03 22:37:22,847 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 125
2023-05-03 22:37:22,847 - ye_logger_of_yor - INFO - memory_search function - Line 133
2023-05-03 22:37:23,061 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:37:23,062 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:37:23,062 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:37:23,062 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:37:23,062 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:37:23,062 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:37:23,063 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:37:23,064 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:37:23,064 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:37:23,064 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:37:23,064 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:37:23,064 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:37:23,065 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:37:23,066 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:37:23,067 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:37:23,067 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:37:23,067 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:38:42,791 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:38:42,791 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:38:42,791 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:38:43,922 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:38:43,923 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:38:43,923 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:38:43,923 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 22:38:43,924 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 22:38:44,188 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:38:44,188 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:38:44,189 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:38:44,189 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:38:44,189 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:38:44,189 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:38:44,189 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:38:44,190 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:38:44,190 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:38:44,190 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:38:44,190 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:38:44,191 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:38:44,191 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:38:44,191 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:38:44,191 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:38:44,191 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:38:44,192 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:38:44,192 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:38:44,192 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:38:44,192 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:38:44,193 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:38:44,194 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:38:44,194 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:38:44,194 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:38:44,194 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:38:44,194 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:38:44,195 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:38:44,195 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:38:44,195 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:38:44,195 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:38:44,196 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:38:44,196 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:38:44,196 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:39:52,296 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:39:52,297 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:39:52,297 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:39:53,312 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:39:53,312 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:39:53,312 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 22:39:53,313 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 22:39:53,314 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 22:39:53,525 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:39:53,526 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:39:53,526 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:39:53,527 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:39:53,528 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:39:53,529 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:39:53,530 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:39:53,531 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:39:53,531 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:39:53,531 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:39:53,531 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:39:53,531 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:39:53,532 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:39:53,532 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:39:53,532 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:45:12,338 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:45:12,338 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:45:12,338 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:45:13,311 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:45:13,312 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:45:13,312 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:45:13,312 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 22:45:13,312 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 22:45:13,313 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 22:45:13,313 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 22:45:13,313 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 22:45:13,313 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 22:45:13,313 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 22:45:13,517 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:45:13,518 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:45:13,518 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:45:13,518 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:45:13,519 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:45:13,520 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:45:13,521 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:45:13,522 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:45:13,523 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:45:13,523 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:45:13,523 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:45:13,523 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:45:13,524 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:45:13,524 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:45:13,524 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:45:13,524 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:45:27,813 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:27,813 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:28,863 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base callback handler that can be used to handle callbacks in langchain."""\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class LLMManagerMixin:\n    """Mixin for LLM callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ChainManagerMixin:\n    """Mixin for chain callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ToolManagerMixin:\n    """Mixin for tool callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerMixin:\n    """Mixin for callback manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RunManagerMixin:\n    """Mixin for run manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseCallbackHandler(\n    LLMManagerMixin,\n    ChainManagerMixin,\n    ToolManagerMixin,\n    CallbackManagerMixin,\n    RunManagerMixin,\n):\n    """Base callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackHandler(BaseCallbackHandler):\n    """Async callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCallbackManager(CallbackManagerMixin):\n    """Base callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:31,006 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:31,007 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:31,018 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n    load_json,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_clearml() -> Any:\n    try:\n        import clearml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the clearml callback manager you need to have the clearml python "\n            "package installed. Please install it with pip install clearml"\n        )\n    return clearml', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ClearMLCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to ClearML.', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:32,547 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:32,547 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:32,570 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, Generation, LLMResult', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='LANGCHAIN_MODEL_NAME = "langchain-model"', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def import_comet_ml() -> Any:\n    try:\n        import comet_ml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the comet_ml callback manager you need to have the "\n            "comet_ml python package installed. Please install it with"\n            " pip install comet_ml"\n        )\n    return comet_ml', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _get_experiment(\n    workspace: Optional[str] = None, project_name: Optional[str] = None\n) -> Any:\n    comet_ml = import_comet_ml()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _fetch_text_complexity_metrics(text: str) -> dict:\n    textstat = import_textstat()\n    text_complexity_metrics = {\n        "flesch_reading_ease": textstat.flesch_reading_ease(text),\n        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),\n        "smog_index": textstat.smog_index(text),\n        "coleman_liau_index": textstat.coleman_liau_index(text),\n        "automated_readability_index": textstat.automated_readability_index(text),\n        "dale_chall_readability_score": textstat.dale_chall_readability_score(text),\n        "difficult_words": textstat.difficult_words(text),\n        "linsear_write_formula": textstat.linsear_write_formula(text),\n        "gunning_fog": textstat.gunning_fog(text),\n        "text_standard": textstat.text_standard(text),\n        "fernandez_huerta": textstat.fernandez_huerta(text),\n        "szigriszt_pazos": textstat.szigriszt_pazos(text),\n        "gutierrez_polini": textstat.gutierrez_polini(text),\n        "crawford": textstat.crawford(text),\n        "gulpease_index": textstat.gulpease_index(text),\n        "osman": textstat.osman(text),\n    }\n    return text_complexity_metrics', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _summarize_metrics_for_generated_outputs(metrics: Sequence) -> dict:\n    pd = import_pandas()\n    metrics_df = pd.DataFrame(metrics)\n    metrics_summary = metrics_df.describe()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class CometCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Comet.', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:34,020 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:34,020 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:34,070 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nimport functools\nimport os\nimport warnings\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, Generator, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID, uuid4', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n    ChainManagerMixin,\n    LLMManagerMixin,\n    RunManagerMixin,\n    ToolManagerMixin,\n)\nfrom langchain.callbacks.openai_info import OpenAICallbackHandler\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.callbacks.tracers.base import TracerSession\nfrom langchain.callbacks.tracers.langchain import LangChainTracer, LangChainTracerV2\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Callbacks = Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='openai_callback_var: ContextVar[Optional[OpenAICallbackHandler]] = ContextVar(\n    "openai_callback", default=None\n)\ntracing_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(\n    "tracing_callback", default=None\n)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='@contextmanager\ndef get_openai_callback() -> Generator[OpenAICallbackHandler, None, None]:\n    """Get OpenAI callback handler in a context manager."""\n    cb = OpenAICallbackHandler()\n    openai_callback_var.set(cb)\n    yield cb\n    openai_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get Tracer in a context manager."""\n    cb = LangChainTracer()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_v2_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get the experimental tracer handler in a context manager."""\n    # Issue a warning that this is experimental\n    warnings.warn(\n        "The experimental tracing v2 is in development. "\n        "This is not yet stable and may change in the future."\n    )\n    cb = LangChainTracerV2()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _handle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    for handler in handlers:\n        try:\n            if ignore_condition_name is None or not getattr(\n                handler, ignore_condition_name\n            ):\n                getattr(handler, event_name)(args, **kwargs)\n        except Exception as e:\n            # TODO: switch this to use logging\n            print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event_for_handler(\n    handler: BaseCallbackHandler,\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    try:\n        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):\n            event = getattr(handler, event_name)\n            if asyncio.iscoroutinefunction(event):\n                await event(args, kwargs)\n            else:\n                await asyncio.get_event_loop().run_in_executor(\n                    None, functools.partial(event, *args, kwargs)\n                )\n    except Exception as e:\n        # TODO: switch this to use logging\n        print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    """Generic event handler for AsyncCallbackManager."""\n    await asyncio.gather(\n        (\n            _ahandle_event_for_handler(\n                handler, event_name, ignore_condition_name, args, *kwargs\n            )\n            for handler in handlers\n        )\n    )', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='BRM = TypeVar("BRM", bound="BaseRunManager")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseRunManager(RunManagerMixin):\n    """Base class for run manager (a bound callback manager)."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class RunManager(BaseRunManager):\n    """Sync Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncRunManager(BaseRunManager):\n    """Async Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForLLMRun(RunManager, LLMManagerMixin):\n    """Callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):\n    """Async callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForChainRun(RunManager, ChainManagerMixin):\n    """Callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForChainRun(AsyncRunManager, ChainManagerMixin):\n    """Async callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForToolRun(RunManager, ToolManagerMixin):\n    """Callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForToolRun(AsyncRunManager, ToolManagerMixin):\n    """Async callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManager(BaseCallbackManager):\n    """Callback manager that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackManager(BaseCallbackManager):\n    """Async callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='T = TypeVar("T", CallbackManager, AsyncCallbackManager)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def configure(\n    callback_manager_cls: Type[T],\n    inheritable_callbacks: Callbacks = None,\n    local_callbacks: Callbacks = None,\n    verbose: bool = False,\n) -> T:\n    """Configure the callback manager."""\n    callback_manager = callback_manager_cls([])\n    if inheritable_callbacks or local_callbacks:\n        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:\n            inheritable_callbacks = inheritable_callbacks or []\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks_.copy(),\n                inheritable_handlers=inheritable_callbacks_.copy(),\n            )\n        else:\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks.handlers,\n                inheritable_handlers=inheritable_callbacks.inheritable_handlers,\n                parent_run_id=inheritable_callbacks.parent_run_id,\n            )\n        local_handlers_ = (\n            local_callbacks\n            if isinstance(local_callbacks, list)\n            else (local_callbacks.handlers if local_callbacks else [])\n        )\n        for handler in local_handlers_:\n            callback_manager.add_handler(handler, False)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:36,339 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:36,339 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:36,349 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='MODEL_COST_PER_1K_TOKENS = {\n    "gpt-4": 0.03,\n    "gpt-4-0314": 0.03,\n    "gpt-4-completion": 0.06,\n    "gpt-4-0314-completion": 0.06,\n    "gpt-4-32k": 0.06,\n    "gpt-4-32k-0314": 0.06,\n    "gpt-4-32k-completion": 0.12,\n    "gpt-4-32k-0314-completion": 0.12,\n    "gpt-3.5-turbo": 0.002,\n    "gpt-3.5-turbo-0301": 0.002,\n    "text-ada-001": 0.0004,\n    "ada": 0.0004,\n    "text-babbage-001": 0.0005,\n    "babbage": 0.0005,\n    "text-curie-001": 0.002,\n    "curie": 0.002,\n    "text-davinci-003": 0.02,\n    "text-davinci-002": 0.02,\n    "code-davinci-002": 0.02,\n}', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def get_openai_token_cost_for_model(\n    model_name: str, num_tokens: int, is_completion: bool = False\n) -> float:\n    suffix = "-completion" if is_completion and model_name.startswith("gpt-4") else ""\n    model = model_name.lower() + suffix\n    if model not in MODEL_COST_PER_1K_TOKENS:\n        raise ValueError(\n            f"Unknown model: {model_name}. Please provide a valid OpenAI model name."\n            "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())\n        )\n    return MODEL_COST_PER_1K_TOKENS[model] * num_tokens / 1000', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class OpenAICallbackHandler(BaseCallbackHandler):\n    """Callback Handler that tracks OpenAI info."""', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:37,740 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:37,740 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:37,748 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.input import print_text\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class StdOutCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that prints to std out."""', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:39,279 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:39,280 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:39,290 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nfrom typing import Any, AsyncIterator, Dict, List, Literal, Union, cast', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import AsyncCallbackHandler\nfrom langchain.schema import LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="TODO If used by two LLM runs in parallel this won't work as expected", metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class AsyncIteratorCallbackHandler(AsyncCallbackHandler):\n    """Callback handler that returns an async iterator."""', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:41,013 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:41,013 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:41,018 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler streams to stdout on new llm token."""\nimport sys\nfrom typing import Any, Dict, List, Union', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n    """Callback handler for streaming. Only works with LLMs that support streaming."""', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:42,479 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:42,479 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:42,486 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that logs to streamlit."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import streamlit as st', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamlitCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that logs to streamlit."""', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:44,027 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:44,027 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:44,044 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport hashlib\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, Tuple, Union', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_spacy() -> Any:\n    try:\n        import spacy\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the spacy python "\n            "package installed. Please install it with pip install spacy"\n        )\n    return spacy', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_pandas() -> Any:\n    try:\n        import pandas\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the pandas python "\n            "package installed. Please install it with pip install pandas"\n        )\n    return pandas', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_textstat() -> Any:\n    try:\n        import textstat\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the textstat python "\n            "package installed. Please install it with pip install textstat"\n        )\n    return textstat', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = ""\n) -> Iterable[Tuple[str, Any]]:\n    """\n    Generator that yields flattened items from a nested dictionary for a flat dict.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"\n) -> Dict[str, Any]:\n    """Flattens a nested dictionary into a flat dictionary.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def hash_string(s: str) -> str:\n    """Hash a string using sha1.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json(json_path: Union[str, Path]) -> str:\n    """Load json file to a string.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BaseMetadataCallbackHandler:\n    """This class handles the metadata and associated function states for callbacks.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:46,163 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:46,163 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:46,182 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport json\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_wandb() -> Any:\n    try:\n        import wandb  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the wandb callback manager you need to have the wandb python "\n            "package installed. Please install it with pip install wandb"\n        )\n    return wandb', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json_to_dict(json_path: Union[str, Path]) -> dict:\n    """Load json file to a dictionary.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def analyze_text(\n    text: str,\n    complexity_metrics: bool = True,\n    visualize: bool = True,\n    nlp: Any = None,\n    output_dir: Optional[Union[str, Path]] = None,\n) -> dict:\n    """Analyze text using textstat and spacy.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def construct_html_from_prompt_and_generation(prompt: str, generation: str) -> Any:\n    """Construct an html element from a prompt and a generation.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class WandbCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Weights and Biases.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:47,787 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:47,787 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:47,799 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface that all chains should implement."""\nimport inspect\nimport json\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml\nfrom pydantic import BaseModel, Field, root_validator, validator', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_verbosity() -> bool:\n    return langchain.verbose', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class Chain(BaseModel, ABC):\n    """Base interface that all chains should implement."""', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:49,315 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:49,315 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:49,325 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that just formats a prompt and calls an LLM."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import LLMResult, PromptValue', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMChain(Chain):\n    """Chain to run queries against LLMs.', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:51,167 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:51,168 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:51,177 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that hits a URL and then uses an LLM to parse results."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_HEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"  # noqa: E501\n}', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMRequestsChain(Chain):\n    """Chain that hits a URL and then uses an LLM to parse results."""', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:52,598 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:52,599 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:52,642 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading chains."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, Union', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.api.base import APIChain\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\nfrom langchain.llms.loading import load_llm, load_llm_from_config\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\nfrom langchain.utilities.loading import try_load_from_hub', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/"', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\n    """Load LLM chain from config dict."""\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_hyde_chain(config: dict, kwargs: Any) -> HypotheticalDocumentEmbedder:\n    """Load hypothetical document embedder chain from config dict."""\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "embeddings" in kwargs:\n        embeddings = kwargs.pop("embeddings")\n    else:\n        raise ValueError("embeddings must be present.")\n    return HypotheticalDocumentEmbedder(\n        llm_chain=llm_chain, base_embeddings=embeddings, config\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_reduce_documents_chain(\n    config: dict, **kwargs: Any\n) -> MapReduceDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_bash_chain(config: dict, kwargs: Any) -> LLMBashChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMBashChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_checker_chain(config: dict, kwargs: Any) -> LLMCheckerChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "create_draft_answer_prompt" in config:\n        create_draft_answer_prompt_config = config.pop("create_draft_answer_prompt")\n        create_draft_answer_prompt = load_prompt_from_config(\n            create_draft_answer_prompt_config\n        )\n    elif "create_draft_answer_prompt_path" in config:\n        create_draft_answer_prompt = load_prompt(\n            config.pop("create_draft_answer_prompt_path")\n        )\n    if "list_assertions_prompt" in config:\n        list_assertions_prompt_config = config.pop("list_assertions_prompt")\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\n    elif "list_assertions_prompt_path" in config:\n        list_assertions_prompt = load_prompt(config.pop("list_assertions_prompt_path"))\n    if "check_assertions_prompt" in config:\n        check_assertions_prompt_config = config.pop("check_assertions_prompt")\n        check_assertions_prompt = load_prompt_from_config(\n            check_assertions_prompt_config\n        )\n    elif "check_assertions_prompt_path" in config:\n        check_assertions_prompt = load_prompt(\n            config.pop("check_assertions_prompt_path")\n        )\n    if "revised_answer_prompt" in config:\n        revised_answer_prompt_config = config.pop("revised_answer_prompt")\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\n    elif "revised_answer_prompt_path" in config:\n        revised_answer_prompt = load_prompt(config.pop("revised_answer_prompt_path"))\n    return LLMCheckerChain(\n        llm=llm,\n        create_draft_answer_prompt=create_draft_answer_prompt,\n        list_assertions_prompt=list_assertions_prompt,\n        check_assertions_prompt=check_assertions_prompt,\n        revised_answer_prompt=revised_answer_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_math_chain(config: dict, kwargs: Any) -> LLMMathChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMMathChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_documents_chain(\n    config: dict, kwargs: Any\n) -> MapRerankDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")\n    return MapRerankDocumentsChain(llm_chain=llm_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_pal_chain(config: dict, kwargs: Any) -> PALChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    else:\n        raise ValueError("One of prompt or prompt_path must be present.")\n    return PALChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_documents_chain(config: dict, kwargs: Any) -> RefineDocumentsChain:\n    if "initial_llm_chain" in config:\n        initial_llm_chain_config = config.pop("initial_llm_chain")\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\n    elif "initial_llm_chain_path" in config:\n        initial_llm_chain = load_chain(config.pop("initial_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of initial_llm_chain or initial_llm_chain_config must be present."\n        )\n    if "refine_llm_chain" in config:\n        refine_llm_chain_config = config.pop("refine_llm_chain")\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\n    elif "refine_llm_chain_path" in config:\n        refine_llm_chain = load_chain(config.pop("refine_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of refine_llm_chain or refine_llm_chain_config must be present."\n        )\n    if "document_prompt" in config:\n        prompt_config = config.pop("document_prompt")\n        document_prompt = load_prompt_from_config(prompt_config)\n    elif "document_prompt_path" in config:\n        document_prompt = load_prompt(config.pop("document_prompt_path"))\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_llm_chain,\n        refine_llm_chain=refine_llm_chain,\n        document_prompt=document_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_qa_with_sources_chain(config: dict, kwargs: Any) -> QAWithSourcesChain:\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_sql_database_chain(config: dict, kwargs: Any) -> SQLDatabaseChain:\n    if "database" in kwargs:\n        database = kwargs.pop("database")\n    else:\n        raise ValueError("database must be present.")\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa_with_sources_chain(\n    config: dict, kwargs: Any\n) -> VectorDBQAWithSourcesChain:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQAWithSourcesChain(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa(config: dict, kwargs: Any) -> VectorDBQA:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQA(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_api_chain(config: dict, kwargs: Any) -> APIChain:\n    if "api_request_chain" in config:\n        api_request_chain_config = config.pop("api_request_chain")\n        api_request_chain = load_chain_from_config(api_request_chain_config)\n    elif "api_request_chain_path" in config:\n        api_request_chain = load_chain(config.pop("api_request_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_request_chain or api_request_chain_path must be present."\n        )\n    if "api_answer_chain" in config:\n        api_answer_chain_config = config.pop("api_answer_chain")\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\n    elif "api_answer_chain_path" in config:\n        api_answer_chain = load_chain(config.pop("api_answer_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_answer_chain or api_answer_chain_path must be present."\n        )\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n    else:\n        raise ValueError("requests_wrapper must be present.")\n    return APIChain(\n        api_request_chain=api_request_chain,\n        api_answer_chain=api_answer_chain,\n        requests_wrapper=requests_wrapper,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_requests_chain(config: dict, kwargs: Any) -> LLMRequestsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n        return LLMRequestsChain(\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, config\n        )\n    else:\n        return LLMRequestsChain(llm_chain=llm_chain, **config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='type_to_loader_dict = {\n    "api_chain": _load_api_chain,\n    "hyde_chain": _load_hyde_chain,\n    "llm_chain": _load_llm_chain,\n    "llm_bash_chain": _load_llm_bash_chain,\n    "llm_checker_chain": _load_llm_checker_chain,\n    "llm_math_chain": _load_llm_math_chain,\n    "llm_requests_chain": _load_llm_requests_chain,\n    "pal_chain": _load_pal_chain,\n    "qa_with_sources_chain": _load_qa_with_sources_chain,\n    "stuff_documents_chain": _load_stuff_documents_chain,\n    "map_reduce_documents_chain": _load_map_reduce_documents_chain,\n    "map_rerank_documents_chain": _load_map_rerank_documents_chain,\n    "refine_documents_chain": _load_refine_documents_chain,\n    "sql_database_chain": _load_sql_database_chain,\n    "vector_db_qa_with_sources_chain": _load_vector_db_qa_with_sources_chain,\n    "vector_db_qa": _load_vector_db_qa,\n}', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\n    """Load chain from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify a chain Type in config")\n    config_type = config.pop("_type")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_chain(path: Union[str, Path], kwargs: Any) -> Chain:\n    """Unified method for loading a chain from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_chain_from_file, "chains", {"json", "yaml"}, kwargs\n    ):\n        return hub_result\n    else:\n        return _load_chain_from_file(path, **kwargs)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\n    """Load chain from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:54,426 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:54,426 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:54,437 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Map-reduce chain.', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Splits up a document, sends the smaller parts to the LLM with one prompt,\nthen combines the results with another one.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import TextSplitter', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceChain(Chain):\n    """Map-reduce chain."""', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:56,301 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:56,302 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:56,310 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Pass input through a moderation endpoint."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import root_validator', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.utils import get_from_dict_or_env', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class OpenAIModerationChain(Chain):\n    """Pass input through a moderation endpoint.', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:57,843 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:57,844 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:57,862 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Tuple', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import BaseModel, Field', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BasePromptSelector(BaseModel, ABC):\n    @abstractmethod\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\n        """Get default prompt for a language model."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConditionalPromptSelector(BasePromptSelector):\n    """Prompt collection that goes through conditionals."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def is_llm(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseLLM)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def is_chat_model(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseChatModel)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:45:59,549 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:45:59,549 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:45:59,559 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain pipeline where the outputs of one step feed directly into next."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_color_mapping', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class SequentialChain(Chain):\n    """Chain where the outputs of one chain feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class SimpleSequentialChain(Chain):\n    """Simple chain where the outputs of one step feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:46:01,622 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:46:01,622 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:46:01,629 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that runs an arbitrary python function."""\nfrom typing import Callable, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class TransformChain(Chain):\n    """Chain transform chain output.', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:46:03,267 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:46:03,267 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:46:03,277 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chains are easily reusable components which can be linked together."""\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.conversation.base import ConversationChain\nfrom langchain.chains.conversational_retrieval.base import (\n    ChatVectorDBChain,\n    ConversationalRetrievalChain,\n)\nfrom langchain.chains.graph_qa.base import GraphQAChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\nfrom langchain.chains.loading import load_chain\nfrom langchain.chains.mapreduce import MapReduceChain\nfrom langchain.chains.moderation import OpenAIModerationChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_generation.base import QAGenerationChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\nfrom langchain.chains.sql_database.base import (\n    SQLDatabaseChain,\n    SQLDatabaseSequentialChain,\n)\nfrom langchain.chains.transform import TransformChain', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='all = [\n    "ConversationChain",\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMSummarizationCheckerChain",\n    "LLMMathChain",\n    "PALChain",\n    "QAWithSourcesChain",\n    "SQLDatabaseChain",\n    "SequentialChain",\n    "SimpleSequentialChain",\n    "VectorDBQA",\n    "VectorDBQAWithSourcesChain",\n    "APIChain",\n    "LLMRequestsChain",\n    "TransformChain",\n    "MapReduceChain",\n    "OpenAIModerationChain",\n    "SQLDatabaseSequentialChain",\n    "load_chain",\n    "AnalyzeDocumentChain",\n    "HypotheticalDocumentEmbedder",\n    "ChatVectorDBChain",\n    "GraphQAChain",\n    "ConstitutionalChain",\n    "QAGenerationChain",\n    "RetrievalQA",\n    "RetrievalQAWithSourcesChain",\n    "ConversationalRetrievalChain",\n    "OpenAPIEndpointChain",\n]', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:46:04,895 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:46:04,895 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:46:04,905 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that makes API calls and summarizes the responses to answer a question."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Field, root_validator', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT, API_URL_PROMPT\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import BasePromptTemplate\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class APIChain(Chain):\n    """Chain that makes API calls and summarizes the responses to answer a question."""', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\api\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:46:06,909 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:46:06,910 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:46:06,920 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='NEWS_DOCS = """API documentation:\nEndpoint: https://newsapi.org\nTop headlines /v2/top-headlines', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='This endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='This endpoint is great for retrieving headlines for use with news tickers or similar.\nRequest parameters', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Response object\n    status | string | If the request was successful or not. Options: ok, error. In the case of error a code and message property will be populated.\n    totalResults | int | The total number of results available for your request.\n    articles | array[article] | The results of the request.\n    source | object | The identifier id and a display name name for the source this article came from.\n    author | string | The author of the article\n    title | string | The headline or title of the article.\n    description | string | A description or snippet from the article.\n    url | string | The direct URL to the article.\n    urlToImage | string | The URL to a relevant image for the article.\n    publishedAt | string | The date and time that the article was published, in UTC (+000)\n    content | string | The unformatted content of the article, where available. This is truncated to 200 chars.', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Use page size: 2\n"""', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\api\\news_docs.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:46:08,651 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:46:08,652 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:46:48,002 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:46:48,002 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:46:48,002 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:46:48,934 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:46:48,934 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 22:46:48,935 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 22:46:48,936 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 22:46:48,936 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 22:46:49,141 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:46:49,142 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:46:49,143 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:46:49,144 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:46:49,145 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:46:49,146 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:46:49,147 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:46:49,147 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:46:49,147 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:46:49,147 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:46:49,147 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:47:00,201 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:00,202 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:01,224 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base callback handler that can be used to handle callbacks in langchain."""\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class LLMManagerMixin:\n    """Mixin for LLM callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ChainManagerMixin:\n    """Mixin for chain callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ToolManagerMixin:\n    """Mixin for tool callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerMixin:\n    """Mixin for callback manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RunManagerMixin:\n    """Mixin for run manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseCallbackHandler(\n    LLMManagerMixin,\n    ChainManagerMixin,\n    ToolManagerMixin,\n    CallbackManagerMixin,\n    RunManagerMixin,\n):\n    """Base callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackHandler(BaseCallbackHandler):\n    """Async callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCallbackManager(CallbackManagerMixin):\n    """Base callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:03,594 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:03,594 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:03,604 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n    load_json,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_clearml() -> Any:\n    try:\n        import clearml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the clearml callback manager you need to have the clearml python "\n            "package installed. Please install it with pip install clearml"\n        )\n    return clearml', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ClearMLCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to ClearML.', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:05,173 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:05,174 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:05,197 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, Generation, LLMResult', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='LANGCHAIN_MODEL_NAME = "langchain-model"', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def import_comet_ml() -> Any:\n    try:\n        import comet_ml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the comet_ml callback manager you need to have the "\n            "comet_ml python package installed. Please install it with"\n            " pip install comet_ml"\n        )\n    return comet_ml', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _get_experiment(\n    workspace: Optional[str] = None, project_name: Optional[str] = None\n) -> Any:\n    comet_ml = import_comet_ml()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _fetch_text_complexity_metrics(text: str) -> dict:\n    textstat = import_textstat()\n    text_complexity_metrics = {\n        "flesch_reading_ease": textstat.flesch_reading_ease(text),\n        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),\n        "smog_index": textstat.smog_index(text),\n        "coleman_liau_index": textstat.coleman_liau_index(text),\n        "automated_readability_index": textstat.automated_readability_index(text),\n        "dale_chall_readability_score": textstat.dale_chall_readability_score(text),\n        "difficult_words": textstat.difficult_words(text),\n        "linsear_write_formula": textstat.linsear_write_formula(text),\n        "gunning_fog": textstat.gunning_fog(text),\n        "text_standard": textstat.text_standard(text),\n        "fernandez_huerta": textstat.fernandez_huerta(text),\n        "szigriszt_pazos": textstat.szigriszt_pazos(text),\n        "gutierrez_polini": textstat.gutierrez_polini(text),\n        "crawford": textstat.crawford(text),\n        "gulpease_index": textstat.gulpease_index(text),\n        "osman": textstat.osman(text),\n    }\n    return text_complexity_metrics', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _summarize_metrics_for_generated_outputs(metrics: Sequence) -> dict:\n    pd = import_pandas()\n    metrics_df = pd.DataFrame(metrics)\n    metrics_summary = metrics_df.describe()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class CometCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Comet.', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:06,887 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:06,888 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:06,936 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nimport functools\nimport os\nimport warnings\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, Generator, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID, uuid4', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n    ChainManagerMixin,\n    LLMManagerMixin,\n    RunManagerMixin,\n    ToolManagerMixin,\n)\nfrom langchain.callbacks.openai_info import OpenAICallbackHandler\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.callbacks.tracers.base import TracerSession\nfrom langchain.callbacks.tracers.langchain import LangChainTracer, LangChainTracerV2\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Callbacks = Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='openai_callback_var: ContextVar[Optional[OpenAICallbackHandler]] = ContextVar(\n    "openai_callback", default=None\n)\ntracing_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(\n    "tracing_callback", default=None\n)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='@contextmanager\ndef get_openai_callback() -> Generator[OpenAICallbackHandler, None, None]:\n    """Get OpenAI callback handler in a context manager."""\n    cb = OpenAICallbackHandler()\n    openai_callback_var.set(cb)\n    yield cb\n    openai_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get Tracer in a context manager."""\n    cb = LangChainTracer()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_v2_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get the experimental tracer handler in a context manager."""\n    # Issue a warning that this is experimental\n    warnings.warn(\n        "The experimental tracing v2 is in development. "\n        "This is not yet stable and may change in the future."\n    )\n    cb = LangChainTracerV2()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _handle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    for handler in handlers:\n        try:\n            if ignore_condition_name is None or not getattr(\n                handler, ignore_condition_name\n            ):\n                getattr(handler, event_name)(args, **kwargs)\n        except Exception as e:\n            # TODO: switch this to use logging\n            print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event_for_handler(\n    handler: BaseCallbackHandler,\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    try:\n        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):\n            event = getattr(handler, event_name)\n            if asyncio.iscoroutinefunction(event):\n                await event(args, kwargs)\n            else:\n                await asyncio.get_event_loop().run_in_executor(\n                    None, functools.partial(event, *args, kwargs)\n                )\n    except Exception as e:\n        # TODO: switch this to use logging\n        print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    """Generic event handler for AsyncCallbackManager."""\n    await asyncio.gather(\n        (\n            _ahandle_event_for_handler(\n                handler, event_name, ignore_condition_name, args, *kwargs\n            )\n            for handler in handlers\n        )\n    )', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='BRM = TypeVar("BRM", bound="BaseRunManager")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseRunManager(RunManagerMixin):\n    """Base class for run manager (a bound callback manager)."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class RunManager(BaseRunManager):\n    """Sync Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncRunManager(BaseRunManager):\n    """Async Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForLLMRun(RunManager, LLMManagerMixin):\n    """Callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):\n    """Async callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForChainRun(RunManager, ChainManagerMixin):\n    """Callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForChainRun(AsyncRunManager, ChainManagerMixin):\n    """Async callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForToolRun(RunManager, ToolManagerMixin):\n    """Callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForToolRun(AsyncRunManager, ToolManagerMixin):\n    """Async callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManager(BaseCallbackManager):\n    """Callback manager that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackManager(BaseCallbackManager):\n    """Async callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='T = TypeVar("T", CallbackManager, AsyncCallbackManager)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def configure(\n    callback_manager_cls: Type[T],\n    inheritable_callbacks: Callbacks = None,\n    local_callbacks: Callbacks = None,\n    verbose: bool = False,\n) -> T:\n    """Configure the callback manager."""\n    callback_manager = callback_manager_cls([])\n    if inheritable_callbacks or local_callbacks:\n        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:\n            inheritable_callbacks = inheritable_callbacks or []\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks_.copy(),\n                inheritable_handlers=inheritable_callbacks_.copy(),\n            )\n        else:\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks.handlers,\n                inheritable_handlers=inheritable_callbacks.inheritable_handlers,\n                parent_run_id=inheritable_callbacks.parent_run_id,\n            )\n        local_handlers_ = (\n            local_callbacks\n            if isinstance(local_callbacks, list)\n            else (local_callbacks.handlers if local_callbacks else [])\n        )\n        for handler in local_handlers_:\n            callback_manager.add_handler(handler, False)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:08,635 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:08,635 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:08,645 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='MODEL_COST_PER_1K_TOKENS = {\n    "gpt-4": 0.03,\n    "gpt-4-0314": 0.03,\n    "gpt-4-completion": 0.06,\n    "gpt-4-0314-completion": 0.06,\n    "gpt-4-32k": 0.06,\n    "gpt-4-32k-0314": 0.06,\n    "gpt-4-32k-completion": 0.12,\n    "gpt-4-32k-0314-completion": 0.12,\n    "gpt-3.5-turbo": 0.002,\n    "gpt-3.5-turbo-0301": 0.002,\n    "text-ada-001": 0.0004,\n    "ada": 0.0004,\n    "text-babbage-001": 0.0005,\n    "babbage": 0.0005,\n    "text-curie-001": 0.002,\n    "curie": 0.002,\n    "text-davinci-003": 0.02,\n    "text-davinci-002": 0.02,\n    "code-davinci-002": 0.02,\n}', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def get_openai_token_cost_for_model(\n    model_name: str, num_tokens: int, is_completion: bool = False\n) -> float:\n    suffix = "-completion" if is_completion and model_name.startswith("gpt-4") else ""\n    model = model_name.lower() + suffix\n    if model not in MODEL_COST_PER_1K_TOKENS:\n        raise ValueError(\n            f"Unknown model: {model_name}. Please provide a valid OpenAI model name."\n            "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())\n        )\n    return MODEL_COST_PER_1K_TOKENS[model] * num_tokens / 1000', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class OpenAICallbackHandler(BaseCallbackHandler):\n    """Callback Handler that tracks OpenAI info."""', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:11,028 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:11,029 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:11,037 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.input import print_text\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class StdOutCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that prints to std out."""', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:13,091 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:13,091 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:13,103 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nfrom typing import Any, AsyncIterator, Dict, List, Literal, Union, cast', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import AsyncCallbackHandler\nfrom langchain.schema import LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="TODO If used by two LLM runs in parallel this won't work as expected", metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class AsyncIteratorCallbackHandler(AsyncCallbackHandler):\n    """Callback handler that returns an async iterator."""', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:15,320 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:15,320 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:15,326 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler streams to stdout on new llm token."""\nimport sys\nfrom typing import Any, Dict, List, Union', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n    """Callback handler for streaming. Only works with LLMs that support streaming."""', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:17,634 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:17,634 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:17,642 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that logs to streamlit."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import streamlit as st', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamlitCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that logs to streamlit."""', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:19,679 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:19,679 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:19,696 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport hashlib\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, Tuple, Union', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_spacy() -> Any:\n    try:\n        import spacy\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the spacy python "\n            "package installed. Please install it with pip install spacy"\n        )\n    return spacy', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_pandas() -> Any:\n    try:\n        import pandas\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the pandas python "\n            "package installed. Please install it with pip install pandas"\n        )\n    return pandas', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_textstat() -> Any:\n    try:\n        import textstat\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the textstat python "\n            "package installed. Please install it with pip install textstat"\n        )\n    return textstat', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = ""\n) -> Iterable[Tuple[str, Any]]:\n    """\n    Generator that yields flattened items from a nested dictionary for a flat dict.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"\n) -> Dict[str, Any]:\n    """Flattens a nested dictionary into a flat dictionary.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def hash_string(s: str) -> str:\n    """Hash a string using sha1.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json(json_path: Union[str, Path]) -> str:\n    """Load json file to a string.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BaseMetadataCallbackHandler:\n    """This class handles the metadata and associated function states for callbacks.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:21,966 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:21,967 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:21,986 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport json\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_wandb() -> Any:\n    try:\n        import wandb  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the wandb callback manager you need to have the wandb python "\n            "package installed. Please install it with pip install wandb"\n        )\n    return wandb', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json_to_dict(json_path: Union[str, Path]) -> dict:\n    """Load json file to a dictionary.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def analyze_text(\n    text: str,\n    complexity_metrics: bool = True,\n    visualize: bool = True,\n    nlp: Any = None,\n    output_dir: Optional[Union[str, Path]] = None,\n) -> dict:\n    """Analyze text using textstat and spacy.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def construct_html_from_prompt_and_generation(prompt: str, generation: str) -> Any:\n    """Construct an html element from a prompt and a generation.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class WandbCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Weights and Biases.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:24,079 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:24,080 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:24,091 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface that all chains should implement."""\nimport inspect\nimport json\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml\nfrom pydantic import BaseModel, Field, root_validator, validator', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_verbosity() -> bool:\n    return langchain.verbose', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class Chain(BaseModel, ABC):\n    """Base interface that all chains should implement."""', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:26,329 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:26,330 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:26,339 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that just formats a prompt and calls an LLM."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import LLMResult, PromptValue', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMChain(Chain):\n    """Chain to run queries against LLMs.', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:28,253 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:28,253 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:28,264 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that hits a URL and then uses an LLM to parse results."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_HEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"  # noqa: E501\n}', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMRequestsChain(Chain):\n    """Chain that hits a URL and then uses an LLM to parse results."""', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:30,030 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:30,030 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:30,072 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading chains."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, Union', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.api.base import APIChain\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\nfrom langchain.llms.loading import load_llm, load_llm_from_config\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\nfrom langchain.utilities.loading import try_load_from_hub', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/"', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\n    """Load LLM chain from config dict."""\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_hyde_chain(config: dict, kwargs: Any) -> HypotheticalDocumentEmbedder:\n    """Load hypothetical document embedder chain from config dict."""\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "embeddings" in kwargs:\n        embeddings = kwargs.pop("embeddings")\n    else:\n        raise ValueError("embeddings must be present.")\n    return HypotheticalDocumentEmbedder(\n        llm_chain=llm_chain, base_embeddings=embeddings, config\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_reduce_documents_chain(\n    config: dict, **kwargs: Any\n) -> MapReduceDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_bash_chain(config: dict, kwargs: Any) -> LLMBashChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMBashChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_checker_chain(config: dict, kwargs: Any) -> LLMCheckerChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "create_draft_answer_prompt" in config:\n        create_draft_answer_prompt_config = config.pop("create_draft_answer_prompt")\n        create_draft_answer_prompt = load_prompt_from_config(\n            create_draft_answer_prompt_config\n        )\n    elif "create_draft_answer_prompt_path" in config:\n        create_draft_answer_prompt = load_prompt(\n            config.pop("create_draft_answer_prompt_path")\n        )\n    if "list_assertions_prompt" in config:\n        list_assertions_prompt_config = config.pop("list_assertions_prompt")\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\n    elif "list_assertions_prompt_path" in config:\n        list_assertions_prompt = load_prompt(config.pop("list_assertions_prompt_path"))\n    if "check_assertions_prompt" in config:\n        check_assertions_prompt_config = config.pop("check_assertions_prompt")\n        check_assertions_prompt = load_prompt_from_config(\n            check_assertions_prompt_config\n        )\n    elif "check_assertions_prompt_path" in config:\n        check_assertions_prompt = load_prompt(\n            config.pop("check_assertions_prompt_path")\n        )\n    if "revised_answer_prompt" in config:\n        revised_answer_prompt_config = config.pop("revised_answer_prompt")\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\n    elif "revised_answer_prompt_path" in config:\n        revised_answer_prompt = load_prompt(config.pop("revised_answer_prompt_path"))\n    return LLMCheckerChain(\n        llm=llm,\n        create_draft_answer_prompt=create_draft_answer_prompt,\n        list_assertions_prompt=list_assertions_prompt,\n        check_assertions_prompt=check_assertions_prompt,\n        revised_answer_prompt=revised_answer_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_math_chain(config: dict, kwargs: Any) -> LLMMathChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMMathChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_documents_chain(\n    config: dict, kwargs: Any\n) -> MapRerankDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")\n    return MapRerankDocumentsChain(llm_chain=llm_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_pal_chain(config: dict, kwargs: Any) -> PALChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    else:\n        raise ValueError("One of prompt or prompt_path must be present.")\n    return PALChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_documents_chain(config: dict, kwargs: Any) -> RefineDocumentsChain:\n    if "initial_llm_chain" in config:\n        initial_llm_chain_config = config.pop("initial_llm_chain")\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\n    elif "initial_llm_chain_path" in config:\n        initial_llm_chain = load_chain(config.pop("initial_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of initial_llm_chain or initial_llm_chain_config must be present."\n        )\n    if "refine_llm_chain" in config:\n        refine_llm_chain_config = config.pop("refine_llm_chain")\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\n    elif "refine_llm_chain_path" in config:\n        refine_llm_chain = load_chain(config.pop("refine_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of refine_llm_chain or refine_llm_chain_config must be present."\n        )\n    if "document_prompt" in config:\n        prompt_config = config.pop("document_prompt")\n        document_prompt = load_prompt_from_config(prompt_config)\n    elif "document_prompt_path" in config:\n        document_prompt = load_prompt(config.pop("document_prompt_path"))\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_llm_chain,\n        refine_llm_chain=refine_llm_chain,\n        document_prompt=document_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_qa_with_sources_chain(config: dict, kwargs: Any) -> QAWithSourcesChain:\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_sql_database_chain(config: dict, kwargs: Any) -> SQLDatabaseChain:\n    if "database" in kwargs:\n        database = kwargs.pop("database")\n    else:\n        raise ValueError("database must be present.")\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa_with_sources_chain(\n    config: dict, kwargs: Any\n) -> VectorDBQAWithSourcesChain:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQAWithSourcesChain(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa(config: dict, kwargs: Any) -> VectorDBQA:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQA(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_api_chain(config: dict, kwargs: Any) -> APIChain:\n    if "api_request_chain" in config:\n        api_request_chain_config = config.pop("api_request_chain")\n        api_request_chain = load_chain_from_config(api_request_chain_config)\n    elif "api_request_chain_path" in config:\n        api_request_chain = load_chain(config.pop("api_request_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_request_chain or api_request_chain_path must be present."\n        )\n    if "api_answer_chain" in config:\n        api_answer_chain_config = config.pop("api_answer_chain")\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\n    elif "api_answer_chain_path" in config:\n        api_answer_chain = load_chain(config.pop("api_answer_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_answer_chain or api_answer_chain_path must be present."\n        )\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n    else:\n        raise ValueError("requests_wrapper must be present.")\n    return APIChain(\n        api_request_chain=api_request_chain,\n        api_answer_chain=api_answer_chain,\n        requests_wrapper=requests_wrapper,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_requests_chain(config: dict, kwargs: Any) -> LLMRequestsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n        return LLMRequestsChain(\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, config\n        )\n    else:\n        return LLMRequestsChain(llm_chain=llm_chain, **config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='type_to_loader_dict = {\n    "api_chain": _load_api_chain,\n    "hyde_chain": _load_hyde_chain,\n    "llm_chain": _load_llm_chain,\n    "llm_bash_chain": _load_llm_bash_chain,\n    "llm_checker_chain": _load_llm_checker_chain,\n    "llm_math_chain": _load_llm_math_chain,\n    "llm_requests_chain": _load_llm_requests_chain,\n    "pal_chain": _load_pal_chain,\n    "qa_with_sources_chain": _load_qa_with_sources_chain,\n    "stuff_documents_chain": _load_stuff_documents_chain,\n    "map_reduce_documents_chain": _load_map_reduce_documents_chain,\n    "map_rerank_documents_chain": _load_map_rerank_documents_chain,\n    "refine_documents_chain": _load_refine_documents_chain,\n    "sql_database_chain": _load_sql_database_chain,\n    "vector_db_qa_with_sources_chain": _load_vector_db_qa_with_sources_chain,\n    "vector_db_qa": _load_vector_db_qa,\n}', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\n    """Load chain from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify a chain Type in config")\n    config_type = config.pop("_type")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_chain(path: Union[str, Path], kwargs: Any) -> Chain:\n    """Unified method for loading a chain from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_chain_from_file, "chains", {"json", "yaml"}, kwargs\n    ):\n        return hub_result\n    else:\n        return _load_chain_from_file(path, **kwargs)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\n    """Load chain from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:32,798 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:32,799 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:32,808 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Map-reduce chain.', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Splits up a document, sends the smaller parts to the LLM with one prompt,\nthen combines the results with another one.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import TextSplitter', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceChain(Chain):\n    """Map-reduce chain."""', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:34,712 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:34,712 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:34,721 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Pass input through a moderation endpoint."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import root_validator', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.utils import get_from_dict_or_env', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class OpenAIModerationChain(Chain):\n    """Pass input through a moderation endpoint.', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:37,189 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:37,189 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:37,206 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Tuple', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import BaseModel, Field', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BasePromptSelector(BaseModel, ABC):\n    @abstractmethod\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\n        """Get default prompt for a language model."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConditionalPromptSelector(BasePromptSelector):\n    """Prompt collection that goes through conditionals."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def is_llm(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseLLM)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def is_chat_model(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseChatModel)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:39,356 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:39,356 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:39,367 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain pipeline where the outputs of one step feed directly into next."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_color_mapping', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class SequentialChain(Chain):\n    """Chain where the outputs of one chain feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class SimpleSequentialChain(Chain):\n    """Simple chain where the outputs of one step feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:41,680 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:41,680 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:41,688 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that runs an arbitrary python function."""\nfrom typing import Callable, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class TransformChain(Chain):\n    """Chain transform chain output.', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:44,004 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:44,004 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:44,014 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chains are easily reusable components which can be linked together."""\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.conversation.base import ConversationChain\nfrom langchain.chains.conversational_retrieval.base import (\n    ChatVectorDBChain,\n    ConversationalRetrievalChain,\n)\nfrom langchain.chains.graph_qa.base import GraphQAChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\nfrom langchain.chains.loading import load_chain\nfrom langchain.chains.mapreduce import MapReduceChain\nfrom langchain.chains.moderation import OpenAIModerationChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_generation.base import QAGenerationChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\nfrom langchain.chains.sql_database.base import (\n    SQLDatabaseChain,\n    SQLDatabaseSequentialChain,\n)\nfrom langchain.chains.transform import TransformChain', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='all = [\n    "ConversationChain",\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMSummarizationCheckerChain",\n    "LLMMathChain",\n    "PALChain",\n    "QAWithSourcesChain",\n    "SQLDatabaseChain",\n    "SequentialChain",\n    "SimpleSequentialChain",\n    "VectorDBQA",\n    "VectorDBQAWithSourcesChain",\n    "APIChain",\n    "LLMRequestsChain",\n    "TransformChain",\n    "MapReduceChain",\n    "OpenAIModerationChain",\n    "SQLDatabaseSequentialChain",\n    "load_chain",\n    "AnalyzeDocumentChain",\n    "HypotheticalDocumentEmbedder",\n    "ChatVectorDBChain",\n    "GraphQAChain",\n    "ConstitutionalChain",\n    "QAGenerationChain",\n    "RetrievalQA",\n    "RetrievalQAWithSourcesChain",\n    "ConversationalRetrievalChain",\n    "OpenAPIEndpointChain",\n]', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:45,979 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:45,979 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:45,992 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PODCAST_DOCS = """API documentation:\nEndpoint: https://listen-api.listennotes.com/api/v2\nGET /search', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='This API is for searching podcasts or episodes.', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Query parameters table:\nq | string | Search term, e.g., person, place, topic... You can use double quotes to do verbatim match, e.g., "game of thrones". Otherwise, it\'s fuzzy search. | required\ntype | string | What type of contents do you want to search for? Available values: episode, podcast, curated. default: episode | optional\npage_size | integer | The maximum number of search results per page. A valid value should be an integer between 1 and 10 (inclusive). default: 3 | optional\nlanguage | string | Limit search results to a specific language, e.g., English, Chinese ... If not specified, it\'ll be any language. It works only when type is episode or podcast. | optional\nregion | string | Limit search results to a specific region (e.g., us, gb, in...). If not specified, it\'ll be any region. It works only when type is episode or podcast. | optional\nlen_min | integer | Minimum audio length in minutes. Applicable only when type parameter is episode or podcast. If type parameter is episode, it\'s for audio length of an episode. If type parameter is podcast, it\'s for average audio length of all episodes in a podcast. | optional\nlen_max | integer | Maximum audio length in minutes. Applicable only when type parameter is episode or podcast. If type parameter is episode, it\'s for audio length of an episode. If type parameter is podcast, it\'s for average audio length of all episodes in a podcast. | optional', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Response schema (JSON object):\nnext_offset | integer | optional\ntotal | integer | optional\nresults | array[object] (Episode / Podcast List Result Object)', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Each object in the "results" key has the following schema:\nlistennotes_url | string | optional\nid | integer | optional\ntitle_highlighted | string | optional', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Use page_size: 3\n"""', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\api\\podcast_docs.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:48,466 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:48,466 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:48,484 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='API_URL_PROMPT_TEMPLATE = """You are given the below API Documentation:\n{api_docs}\nUsing this documentation, generate the full API url to call for answering the user question.\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Question:{question}\nAPI url:"""', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='API_URL_PROMPT = PromptTemplate(\n    input_variables=[\n        "api_docs",\n        "question",\n    ],\n    template=API_URL_PROMPT_TEMPLATE,\n)', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='API_RESPONSE_PROMPT_TEMPLATE = (\n    API_URL_PROMPT_TEMPLATE\n    + """ {api_url}', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Here is the response from the API:', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='{api_response}', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Summarize this response to answer the original question.', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Summary:"""\n)', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='API_RESPONSE_PROMPT = PromptTemplate(\n    input_variables=["api_docs", "question", "api_url", "api_response"],\n    template=API_RESPONSE_PROMPT_TEMPLATE,\n)', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\api\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:50,506 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:50,507 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:50,519 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='TMDB_DOCS = """API documentation:\nEndpoint: https://api.themoviedb.org/3\nGET /search/movie', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='This API is for searching movies.', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Query parameters table:\nlanguage | string | Pass a ISO 639-1 value to display translated data for the fields that support it. minLength: 2, pattern: ([a-z]{2})-([A-Z]{2}), default: en-US | optional\nquery | string | Pass a text query to search. This value should be URI encoded. minLength: 1 | required\npage | integer | Specify which page to query. minimum: 1, maximum: 1000, default: 1 | optional\ninclude_adult | boolean | Choose whether to inlcude adult (pornography) content in the results. default | optional\nregion | string | Specify a ISO 3166-1 code to filter release dates. Must be uppercase. pattern: ^[A-Z]{2}$ | optional\nyear | integer  | optional\nprimary_release_year | integer | optional', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Response schema (JSON object):\npage | integer | optional\ntotal_results | integer | optional\ntotal_pages | integer | optional\nresults | array[object] (Movie List Result Object)', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Each object in the "results" key has the following schema:\nposter_path | string or null | optional\nadult | boolean | optional\noverview | string | optional\nrelease_date | string | optional\ngenre_ids | array[integer] | optional\nid | integer | optional\noriginal_title | string | optional\noriginal_language | string | optional\ntitle | string | optional\nbackdrop_path | string or null | optional\npopularity | number | optional\nvote_count | integer | optional\nvideo | boolean | optional\nvote_average | number | optional"""', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\api\\tmdb_docs.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:53,304 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:53,304 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:53,312 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Chat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:"""\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='prompt_template = """Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='{context}', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Question: {question}\nHelpful Answer:"""\nQA_PROMPT = PromptTemplate(\n    template=prompt_template, input_variables=["context", "question"]\n)', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\chat_vector_db\\prompts.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:55,504 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:55,504 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:55,507 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\chat_vector_db\\__init__.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\chat_vector_db\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:57,964 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:57,964 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:57,979 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface for chains combining documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Field', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def format_document(doc: Document, prompt: BasePromptTemplate) -> str:\n    """Format a document into a string based on a prompt template."""\n    base_info = {"page_content": doc.page_content}\n    base_info.update(doc.metadata)\n    missing_metadata = set(prompt.input_variables).difference(base_info)\n    if len(missing_metadata) > 0:\n        required_metadata = [\n            iv for iv in prompt.input_variables if iv != "page_content"\n        ]\n        raise ValueError(\n            f"Document prompt requires documents to have metadata variables: "\n            f"{required_metadata}. Received document with missing metadata: "\n            f"{list(missing_metadata)}."\n        )\n    document_info = {k: base_info[k] for k in prompt.input_variables}\n    return prompt.format(**document_info)', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCombineDocumentsChain(Chain, ABC):\n    """Base interface for chains combining documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AnalyzeDocumentChain(Chain):\n    """Chain that splits documents, then analyzes it in pieces."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:47:59,901 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:47:59,902 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:47:59,921 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by mapping a chain over them first, then combining results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class CombineDocsProtocol(Protocol):\n    """Interface for the combine_docs method."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _split_list_of_docs(\n    docs: List[Document], length_func: Callable, token_max: int, kwargs: Any\n) -> List[List[Document]]:\n    new_result_doc_list = []\n    _sub_result_docs = []\n    for doc in docs:\n        _sub_result_docs.append(doc)\n        _num_tokens = length_func(_sub_result_docs, kwargs)\n        if _num_tokens > token_max:\n            if len(_sub_result_docs) == 1:\n                raise ValueError(\n                    "A single document was longer than the context length,"\n                    " we cannot handle this."\n                )\n            if len(_sub_result_docs) == 2:\n                raise ValueError(\n                    "A single document was so long it could not be combined "\n                    "with another document, we cannot handle this."\n                )\n            new_result_doc_list.append(_sub_result_docs[:-1])\n            _sub_result_docs = _sub_result_docs[-1:]\n    new_result_doc_list.append(_sub_result_docs)\n    return new_result_doc_list', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _collapse_docs(\n    docs: List[Document],\n    combine_document_func: CombineDocsProtocol,\n    kwargs: Any,\n) -> Document:\n    result = combine_document_func(docs, kwargs)\n    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}\n    for doc in docs[1:]:\n        for k, v in doc.metadata.items():\n            if k in combined_metadata:\n                combined_metadata[k] += f", {v}"\n            else:\n                combined_metadata[k] = str(v)\n    return Document(page_content=result, metadata=combined_metadata)', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceDocumentsChain(BaseCombineDocumentsChain):\n    """Combining documents by mapping a chain over them, then combining results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:02,528 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:02,528 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:02,541 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by mapping a chain over them first, then reranking results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union, cast', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.output_parsers.regex import RegexParser', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapRerankDocumentsChain(BaseCombineDocumentsChain):\n    """Combining documents by mapping a chain over them, then reranking results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:05,320 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:05,320 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:05,336 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by doing a first pass and then refining on more documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Dict, List, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import (\n    BaseCombineDocumentsChain,\n    format_document,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_default_document_prompt() -> PromptTemplate:\n    return PromptTemplate(input_variables=["page_content"], template="{page_content}")', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RefineDocumentsChain(BaseCombineDocumentsChain):\n    """Combine documents by doing a first pass and then refining on more documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:08,155 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:08,155 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:08,163 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that combines documents by stuffing into context."""', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import (\n    BaseCombineDocumentsChain,\n    format_document,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_default_document_prompt() -> PromptTemplate:\n    return PromptTemplate(input_variables=["page_content"], template="{page_content}")', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StuffDocumentsChain(BaseCombineDocumentsChain):\n    """Chain that combines documents by stuffing into context."""', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:10,426 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:10,426 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:10,430 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Different ways to combine documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:12,587 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:12,587 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:12,600 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for applying constitutional principles to the outputs of another chain."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\nfrom langchain.chains.constitutional_ai.prompts import CRITIQUE_PROMPT, REVISION_PROMPT\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConstitutionalChain(Chain):\n    """Chain for applying constitutional principles.', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:15,112 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:15,112 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:15,123 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Models for the Constitutional AI chain."""\nfrom pydantic import BaseModel', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConstitutionalPrinciple(BaseModel):\n    """Class for a constitutional principle."""', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:17,222 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:17,222 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:17,229 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Dict', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PRINCIPLES: Dict[str, ConstitutionalPrinciple] = {}', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:48:20,060 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:48:20,060 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:48:58,596 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 22:48:58,596 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 22:48:58,596 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 22:48:59,598 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 22:48:59,599 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 22:48:59,599 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 22:48:59,599 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 22:48:59,599 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 22:48:59,599 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 22:48:59,600 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 22:48:59,600 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 22:48:59,600 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 22:48:59,600 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 22:48:59,803 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 22:48:59,805 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 22:48:59,805 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 22:48:59,806 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 22:48:59,807 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 22:48:59,808 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 22:48:59,809 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 22:48:59,810 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 22:48:59,810 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 22:49:09,584 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:09,585 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:10,648 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base callback handler that can be used to handle callbacks in langchain."""\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class LLMManagerMixin:\n    """Mixin for LLM callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ChainManagerMixin:\n    """Mixin for chain callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ToolManagerMixin:\n    """Mixin for tool callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerMixin:\n    """Mixin for callback manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RunManagerMixin:\n    """Mixin for run manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseCallbackHandler(\n    LLMManagerMixin,\n    ChainManagerMixin,\n    ToolManagerMixin,\n    CallbackManagerMixin,\n    RunManagerMixin,\n):\n    """Base callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackHandler(BaseCallbackHandler):\n    """Async callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCallbackManager(CallbackManagerMixin):\n    """Base callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:13,336 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:13,337 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:13,348 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n    load_json,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_clearml() -> Any:\n    try:\n        import clearml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the clearml callback manager you need to have the clearml python "\n            "package installed. Please install it with pip install clearml"\n        )\n    return clearml', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ClearMLCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to ClearML.', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:15,920 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:15,920 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:15,947 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, Generation, LLMResult', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='LANGCHAIN_MODEL_NAME = "langchain-model"', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def import_comet_ml() -> Any:\n    try:\n        import comet_ml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the comet_ml callback manager you need to have the "\n            "comet_ml python package installed. Please install it with"\n            " pip install comet_ml"\n        )\n    return comet_ml', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _get_experiment(\n    workspace: Optional[str] = None, project_name: Optional[str] = None\n) -> Any:\n    comet_ml = import_comet_ml()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _fetch_text_complexity_metrics(text: str) -> dict:\n    textstat = import_textstat()\n    text_complexity_metrics = {\n        "flesch_reading_ease": textstat.flesch_reading_ease(text),\n        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),\n        "smog_index": textstat.smog_index(text),\n        "coleman_liau_index": textstat.coleman_liau_index(text),\n        "automated_readability_index": textstat.automated_readability_index(text),\n        "dale_chall_readability_score": textstat.dale_chall_readability_score(text),\n        "difficult_words": textstat.difficult_words(text),\n        "linsear_write_formula": textstat.linsear_write_formula(text),\n        "gunning_fog": textstat.gunning_fog(text),\n        "text_standard": textstat.text_standard(text),\n        "fernandez_huerta": textstat.fernandez_huerta(text),\n        "szigriszt_pazos": textstat.szigriszt_pazos(text),\n        "gutierrez_polini": textstat.gutierrez_polini(text),\n        "crawford": textstat.crawford(text),\n        "gulpease_index": textstat.gulpease_index(text),\n        "osman": textstat.osman(text),\n    }\n    return text_complexity_metrics', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _summarize_metrics_for_generated_outputs(metrics: Sequence) -> dict:\n    pd = import_pandas()\n    metrics_df = pd.DataFrame(metrics)\n    metrics_summary = metrics_df.describe()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class CometCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Comet.', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:18,100 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:18,100 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:18,151 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nimport functools\nimport os\nimport warnings\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, Generator, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID, uuid4', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n    ChainManagerMixin,\n    LLMManagerMixin,\n    RunManagerMixin,\n    ToolManagerMixin,\n)\nfrom langchain.callbacks.openai_info import OpenAICallbackHandler\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.callbacks.tracers.base import TracerSession\nfrom langchain.callbacks.tracers.langchain import LangChainTracer, LangChainTracerV2\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Callbacks = Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='openai_callback_var: ContextVar[Optional[OpenAICallbackHandler]] = ContextVar(\n    "openai_callback", default=None\n)\ntracing_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(\n    "tracing_callback", default=None\n)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='@contextmanager\ndef get_openai_callback() -> Generator[OpenAICallbackHandler, None, None]:\n    """Get OpenAI callback handler in a context manager."""\n    cb = OpenAICallbackHandler()\n    openai_callback_var.set(cb)\n    yield cb\n    openai_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get Tracer in a context manager."""\n    cb = LangChainTracer()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_v2_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get the experimental tracer handler in a context manager."""\n    # Issue a warning that this is experimental\n    warnings.warn(\n        "The experimental tracing v2 is in development. "\n        "This is not yet stable and may change in the future."\n    )\n    cb = LangChainTracerV2()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _handle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    for handler in handlers:\n        try:\n            if ignore_condition_name is None or not getattr(\n                handler, ignore_condition_name\n            ):\n                getattr(handler, event_name)(args, **kwargs)\n        except Exception as e:\n            # TODO: switch this to use logging\n            print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event_for_handler(\n    handler: BaseCallbackHandler,\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    try:\n        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):\n            event = getattr(handler, event_name)\n            if asyncio.iscoroutinefunction(event):\n                await event(args, kwargs)\n            else:\n                await asyncio.get_event_loop().run_in_executor(\n                    None, functools.partial(event, *args, kwargs)\n                )\n    except Exception as e:\n        # TODO: switch this to use logging\n        print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    """Generic event handler for AsyncCallbackManager."""\n    await asyncio.gather(\n        (\n            _ahandle_event_for_handler(\n                handler, event_name, ignore_condition_name, args, *kwargs\n            )\n            for handler in handlers\n        )\n    )', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='BRM = TypeVar("BRM", bound="BaseRunManager")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseRunManager(RunManagerMixin):\n    """Base class for run manager (a bound callback manager)."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class RunManager(BaseRunManager):\n    """Sync Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncRunManager(BaseRunManager):\n    """Async Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForLLMRun(RunManager, LLMManagerMixin):\n    """Callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):\n    """Async callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForChainRun(RunManager, ChainManagerMixin):\n    """Callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForChainRun(AsyncRunManager, ChainManagerMixin):\n    """Async callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForToolRun(RunManager, ToolManagerMixin):\n    """Callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForToolRun(AsyncRunManager, ToolManagerMixin):\n    """Async callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManager(BaseCallbackManager):\n    """Callback manager that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackManager(BaseCallbackManager):\n    """Async callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='T = TypeVar("T", CallbackManager, AsyncCallbackManager)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def configure(\n    callback_manager_cls: Type[T],\n    inheritable_callbacks: Callbacks = None,\n    local_callbacks: Callbacks = None,\n    verbose: bool = False,\n) -> T:\n    """Configure the callback manager."""\n    callback_manager = callback_manager_cls([])\n    if inheritable_callbacks or local_callbacks:\n        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:\n            inheritable_callbacks = inheritable_callbacks or []\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks_.copy(),\n                inheritable_handlers=inheritable_callbacks_.copy(),\n            )\n        else:\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks.handlers,\n                inheritable_handlers=inheritable_callbacks.inheritable_handlers,\n                parent_run_id=inheritable_callbacks.parent_run_id,\n            )\n        local_handlers_ = (\n            local_callbacks\n            if isinstance(local_callbacks, list)\n            else (local_callbacks.handlers if local_callbacks else [])\n        )\n        for handler in local_handlers_:\n            callback_manager.add_handler(handler, False)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:20,164 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:20,164 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:20,173 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='MODEL_COST_PER_1K_TOKENS = {\n    "gpt-4": 0.03,\n    "gpt-4-0314": 0.03,\n    "gpt-4-completion": 0.06,\n    "gpt-4-0314-completion": 0.06,\n    "gpt-4-32k": 0.06,\n    "gpt-4-32k-0314": 0.06,\n    "gpt-4-32k-completion": 0.12,\n    "gpt-4-32k-0314-completion": 0.12,\n    "gpt-3.5-turbo": 0.002,\n    "gpt-3.5-turbo-0301": 0.002,\n    "text-ada-001": 0.0004,\n    "ada": 0.0004,\n    "text-babbage-001": 0.0005,\n    "babbage": 0.0005,\n    "text-curie-001": 0.002,\n    "curie": 0.002,\n    "text-davinci-003": 0.02,\n    "text-davinci-002": 0.02,\n    "code-davinci-002": 0.02,\n}', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def get_openai_token_cost_for_model(\n    model_name: str, num_tokens: int, is_completion: bool = False\n) -> float:\n    suffix = "-completion" if is_completion and model_name.startswith("gpt-4") else ""\n    model = model_name.lower() + suffix\n    if model not in MODEL_COST_PER_1K_TOKENS:\n        raise ValueError(\n            f"Unknown model: {model_name}. Please provide a valid OpenAI model name."\n            "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())\n        )\n    return MODEL_COST_PER_1K_TOKENS[model] * num_tokens / 1000', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class OpenAICallbackHandler(BaseCallbackHandler):\n    """Callback Handler that tracks OpenAI info."""', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:22,364 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:22,364 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:22,372 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.input import print_text\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class StdOutCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that prints to std out."""', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:24,645 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:24,646 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:24,656 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nfrom typing import Any, AsyncIterator, Dict, List, Literal, Union, cast', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import AsyncCallbackHandler\nfrom langchain.schema import LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="TODO If used by two LLM runs in parallel this won't work as expected", metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class AsyncIteratorCallbackHandler(AsyncCallbackHandler):\n    """Callback handler that returns an async iterator."""', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:27,009 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:27,009 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:27,015 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler streams to stdout on new llm token."""\nimport sys\nfrom typing import Any, Dict, List, Union', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n    """Callback handler for streaming. Only works with LLMs that support streaming."""', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:28,784 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:28,784 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:28,792 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that logs to streamlit."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import streamlit as st', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamlitCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that logs to streamlit."""', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:31,172 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:31,172 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:31,190 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport hashlib\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, Tuple, Union', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_spacy() -> Any:\n    try:\n        import spacy\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the spacy python "\n            "package installed. Please install it with pip install spacy"\n        )\n    return spacy', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_pandas() -> Any:\n    try:\n        import pandas\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the pandas python "\n            "package installed. Please install it with pip install pandas"\n        )\n    return pandas', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_textstat() -> Any:\n    try:\n        import textstat\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the textstat python "\n            "package installed. Please install it with pip install textstat"\n        )\n    return textstat', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = ""\n) -> Iterable[Tuple[str, Any]]:\n    """\n    Generator that yields flattened items from a nested dictionary for a flat dict.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"\n) -> Dict[str, Any]:\n    """Flattens a nested dictionary into a flat dictionary.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def hash_string(s: str) -> str:\n    """Hash a string using sha1.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json(json_path: Union[str, Path]) -> str:\n    """Load json file to a string.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BaseMetadataCallbackHandler:\n    """This class handles the metadata and associated function states for callbacks.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:33,845 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:33,846 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:33,864 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport json\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_wandb() -> Any:\n    try:\n        import wandb  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the wandb callback manager you need to have the wandb python "\n            "package installed. Please install it with pip install wandb"\n        )\n    return wandb', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json_to_dict(json_path: Union[str, Path]) -> dict:\n    """Load json file to a dictionary.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def analyze_text(\n    text: str,\n    complexity_metrics: bool = True,\n    visualize: bool = True,\n    nlp: Any = None,\n    output_dir: Optional[Union[str, Path]] = None,\n) -> dict:\n    """Analyze text using textstat and spacy.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def construct_html_from_prompt_and_generation(prompt: str, generation: str) -> Any:\n    """Construct an html element from a prompt and a generation.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class WandbCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Weights and Biases.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:36,596 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:36,597 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:36,607 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface that all chains should implement."""\nimport inspect\nimport json\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml\nfrom pydantic import BaseModel, Field, root_validator, validator', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_verbosity() -> bool:\n    return langchain.verbose', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class Chain(BaseModel, ABC):\n    """Base interface that all chains should implement."""', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:39,291 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:39,291 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:39,302 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that just formats a prompt and calls an LLM."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import LLMResult, PromptValue', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMChain(Chain):\n    """Chain to run queries against LLMs.', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:41,817 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:41,818 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:41,829 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that hits a URL and then uses an LLM to parse results."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_HEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"  # noqa: E501\n}', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMRequestsChain(Chain):\n    """Chain that hits a URL and then uses an LLM to parse results."""', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:44,319 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:44,319 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:44,364 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading chains."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, Union', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.api.base import APIChain\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\nfrom langchain.llms.loading import load_llm, load_llm_from_config\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\nfrom langchain.utilities.loading import try_load_from_hub', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/"', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\n    """Load LLM chain from config dict."""\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_hyde_chain(config: dict, kwargs: Any) -> HypotheticalDocumentEmbedder:\n    """Load hypothetical document embedder chain from config dict."""\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "embeddings" in kwargs:\n        embeddings = kwargs.pop("embeddings")\n    else:\n        raise ValueError("embeddings must be present.")\n    return HypotheticalDocumentEmbedder(\n        llm_chain=llm_chain, base_embeddings=embeddings, config\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_reduce_documents_chain(\n    config: dict, **kwargs: Any\n) -> MapReduceDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_bash_chain(config: dict, kwargs: Any) -> LLMBashChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMBashChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_checker_chain(config: dict, kwargs: Any) -> LLMCheckerChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "create_draft_answer_prompt" in config:\n        create_draft_answer_prompt_config = config.pop("create_draft_answer_prompt")\n        create_draft_answer_prompt = load_prompt_from_config(\n            create_draft_answer_prompt_config\n        )\n    elif "create_draft_answer_prompt_path" in config:\n        create_draft_answer_prompt = load_prompt(\n            config.pop("create_draft_answer_prompt_path")\n        )\n    if "list_assertions_prompt" in config:\n        list_assertions_prompt_config = config.pop("list_assertions_prompt")\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\n    elif "list_assertions_prompt_path" in config:\n        list_assertions_prompt = load_prompt(config.pop("list_assertions_prompt_path"))\n    if "check_assertions_prompt" in config:\n        check_assertions_prompt_config = config.pop("check_assertions_prompt")\n        check_assertions_prompt = load_prompt_from_config(\n            check_assertions_prompt_config\n        )\n    elif "check_assertions_prompt_path" in config:\n        check_assertions_prompt = load_prompt(\n            config.pop("check_assertions_prompt_path")\n        )\n    if "revised_answer_prompt" in config:\n        revised_answer_prompt_config = config.pop("revised_answer_prompt")\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\n    elif "revised_answer_prompt_path" in config:\n        revised_answer_prompt = load_prompt(config.pop("revised_answer_prompt_path"))\n    return LLMCheckerChain(\n        llm=llm,\n        create_draft_answer_prompt=create_draft_answer_prompt,\n        list_assertions_prompt=list_assertions_prompt,\n        check_assertions_prompt=check_assertions_prompt,\n        revised_answer_prompt=revised_answer_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_math_chain(config: dict, kwargs: Any) -> LLMMathChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMMathChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_documents_chain(\n    config: dict, kwargs: Any\n) -> MapRerankDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")\n    return MapRerankDocumentsChain(llm_chain=llm_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_pal_chain(config: dict, kwargs: Any) -> PALChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    else:\n        raise ValueError("One of prompt or prompt_path must be present.")\n    return PALChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_documents_chain(config: dict, kwargs: Any) -> RefineDocumentsChain:\n    if "initial_llm_chain" in config:\n        initial_llm_chain_config = config.pop("initial_llm_chain")\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\n    elif "initial_llm_chain_path" in config:\n        initial_llm_chain = load_chain(config.pop("initial_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of initial_llm_chain or initial_llm_chain_config must be present."\n        )\n    if "refine_llm_chain" in config:\n        refine_llm_chain_config = config.pop("refine_llm_chain")\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\n    elif "refine_llm_chain_path" in config:\n        refine_llm_chain = load_chain(config.pop("refine_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of refine_llm_chain or refine_llm_chain_config must be present."\n        )\n    if "document_prompt" in config:\n        prompt_config = config.pop("document_prompt")\n        document_prompt = load_prompt_from_config(prompt_config)\n    elif "document_prompt_path" in config:\n        document_prompt = load_prompt(config.pop("document_prompt_path"))\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_llm_chain,\n        refine_llm_chain=refine_llm_chain,\n        document_prompt=document_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_qa_with_sources_chain(config: dict, kwargs: Any) -> QAWithSourcesChain:\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_sql_database_chain(config: dict, kwargs: Any) -> SQLDatabaseChain:\n    if "database" in kwargs:\n        database = kwargs.pop("database")\n    else:\n        raise ValueError("database must be present.")\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa_with_sources_chain(\n    config: dict, kwargs: Any\n) -> VectorDBQAWithSourcesChain:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQAWithSourcesChain(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa(config: dict, kwargs: Any) -> VectorDBQA:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQA(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_api_chain(config: dict, kwargs: Any) -> APIChain:\n    if "api_request_chain" in config:\n        api_request_chain_config = config.pop("api_request_chain")\n        api_request_chain = load_chain_from_config(api_request_chain_config)\n    elif "api_request_chain_path" in config:\n        api_request_chain = load_chain(config.pop("api_request_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_request_chain or api_request_chain_path must be present."\n        )\n    if "api_answer_chain" in config:\n        api_answer_chain_config = config.pop("api_answer_chain")\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\n    elif "api_answer_chain_path" in config:\n        api_answer_chain = load_chain(config.pop("api_answer_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_answer_chain or api_answer_chain_path must be present."\n        )\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n    else:\n        raise ValueError("requests_wrapper must be present.")\n    return APIChain(\n        api_request_chain=api_request_chain,\n        api_answer_chain=api_answer_chain,\n        requests_wrapper=requests_wrapper,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_requests_chain(config: dict, kwargs: Any) -> LLMRequestsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n        return LLMRequestsChain(\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, config\n        )\n    else:\n        return LLMRequestsChain(llm_chain=llm_chain, **config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='type_to_loader_dict = {\n    "api_chain": _load_api_chain,\n    "hyde_chain": _load_hyde_chain,\n    "llm_chain": _load_llm_chain,\n    "llm_bash_chain": _load_llm_bash_chain,\n    "llm_checker_chain": _load_llm_checker_chain,\n    "llm_math_chain": _load_llm_math_chain,\n    "llm_requests_chain": _load_llm_requests_chain,\n    "pal_chain": _load_pal_chain,\n    "qa_with_sources_chain": _load_qa_with_sources_chain,\n    "stuff_documents_chain": _load_stuff_documents_chain,\n    "map_reduce_documents_chain": _load_map_reduce_documents_chain,\n    "map_rerank_documents_chain": _load_map_rerank_documents_chain,\n    "refine_documents_chain": _load_refine_documents_chain,\n    "sql_database_chain": _load_sql_database_chain,\n    "vector_db_qa_with_sources_chain": _load_vector_db_qa_with_sources_chain,\n    "vector_db_qa": _load_vector_db_qa,\n}', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\n    """Load chain from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify a chain Type in config")\n    config_type = config.pop("_type")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_chain(path: Union[str, Path], kwargs: Any) -> Chain:\n    """Unified method for loading a chain from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_chain_from_file, "chains", {"json", "yaml"}, kwargs\n    ):\n        return hub_result\n    else:\n        return _load_chain_from_file(path, **kwargs)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\n    """Load chain from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:47,027 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:47,027 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:47,037 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Map-reduce chain.', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Splits up a document, sends the smaller parts to the LLM with one prompt,\nthen combines the results with another one.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import TextSplitter', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceChain(Chain):\n    """Map-reduce chain."""', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:49,411 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:49,412 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:49,421 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Pass input through a moderation endpoint."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import root_validator', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.utils import get_from_dict_or_env', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class OpenAIModerationChain(Chain):\n    """Pass input through a moderation endpoint.', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:52,393 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:52,393 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:52,410 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Tuple', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import BaseModel, Field', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BasePromptSelector(BaseModel, ABC):\n    @abstractmethod\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\n        """Get default prompt for a language model."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConditionalPromptSelector(BasePromptSelector):\n    """Prompt collection that goes through conditionals."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def is_llm(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseLLM)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def is_chat_model(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseChatModel)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:54,956 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:54,956 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:54,968 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain pipeline where the outputs of one step feed directly into next."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_color_mapping', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class SequentialChain(Chain):\n    """Chain where the outputs of one chain feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class SimpleSequentialChain(Chain):\n    """Simple chain where the outputs of one step feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:57,340 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:57,341 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:57,346 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that runs an arbitrary python function."""\nfrom typing import Callable, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class TransformChain(Chain):\n    """Chain transform chain output.', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:49:59,448 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:49:59,448 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:49:59,457 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chains are easily reusable components which can be linked together."""\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.conversation.base import ConversationChain\nfrom langchain.chains.conversational_retrieval.base import (\n    ChatVectorDBChain,\n    ConversationalRetrievalChain,\n)\nfrom langchain.chains.graph_qa.base import GraphQAChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\nfrom langchain.chains.loading import load_chain\nfrom langchain.chains.mapreduce import MapReduceChain\nfrom langchain.chains.moderation import OpenAIModerationChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_generation.base import QAGenerationChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\nfrom langchain.chains.sql_database.base import (\n    SQLDatabaseChain,\n    SQLDatabaseSequentialChain,\n)\nfrom langchain.chains.transform import TransformChain', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='all = [\n    "ConversationChain",\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMSummarizationCheckerChain",\n    "LLMMathChain",\n    "PALChain",\n    "QAWithSourcesChain",\n    "SQLDatabaseChain",\n    "SequentialChain",\n    "SimpleSequentialChain",\n    "VectorDBQA",\n    "VectorDBQAWithSourcesChain",\n    "APIChain",\n    "LLMRequestsChain",\n    "TransformChain",\n    "MapReduceChain",\n    "OpenAIModerationChain",\n    "SQLDatabaseSequentialChain",\n    "load_chain",\n    "AnalyzeDocumentChain",\n    "HypotheticalDocumentEmbedder",\n    "ChatVectorDBChain",\n    "GraphQAChain",\n    "ConstitutionalChain",\n    "QAGenerationChain",\n    "RetrievalQA",\n    "RetrievalQAWithSourcesChain",\n    "ConversationalRetrievalChain",\n    "OpenAPIEndpointChain",\n]', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:01,743 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:01,744 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:01,759 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface for chains combining documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Field', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def format_document(doc: Document, prompt: BasePromptTemplate) -> str:\n    """Format a document into a string based on a prompt template."""\n    base_info = {"page_content": doc.page_content}\n    base_info.update(doc.metadata)\n    missing_metadata = set(prompt.input_variables).difference(base_info)\n    if len(missing_metadata) > 0:\n        required_metadata = [\n            iv for iv in prompt.input_variables if iv != "page_content"\n        ]\n        raise ValueError(\n            f"Document prompt requires documents to have metadata variables: "\n            f"{required_metadata}. Received document with missing metadata: "\n            f"{list(missing_metadata)}."\n        )\n    document_info = {k: base_info[k] for k in prompt.input_variables}\n    return prompt.format(**document_info)', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCombineDocumentsChain(Chain, ABC):\n    """Base interface for chains combining documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AnalyzeDocumentChain(Chain):\n    """Chain that splits documents, then analyzes it in pieces."""', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:04,389 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:04,389 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:04,408 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by mapping a chain over them first, then combining results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class CombineDocsProtocol(Protocol):\n    """Interface for the combine_docs method."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _split_list_of_docs(\n    docs: List[Document], length_func: Callable, token_max: int, kwargs: Any\n) -> List[List[Document]]:\n    new_result_doc_list = []\n    _sub_result_docs = []\n    for doc in docs:\n        _sub_result_docs.append(doc)\n        _num_tokens = length_func(_sub_result_docs, kwargs)\n        if _num_tokens > token_max:\n            if len(_sub_result_docs) == 1:\n                raise ValueError(\n                    "A single document was longer than the context length,"\n                    " we cannot handle this."\n                )\n            if len(_sub_result_docs) == 2:\n                raise ValueError(\n                    "A single document was so long it could not be combined "\n                    "with another document, we cannot handle this."\n                )\n            new_result_doc_list.append(_sub_result_docs[:-1])\n            _sub_result_docs = _sub_result_docs[-1:]\n    new_result_doc_list.append(_sub_result_docs)\n    return new_result_doc_list', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _collapse_docs(\n    docs: List[Document],\n    combine_document_func: CombineDocsProtocol,\n    kwargs: Any,\n) -> Document:\n    result = combine_document_func(docs, kwargs)\n    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}\n    for doc in docs[1:]:\n        for k, v in doc.metadata.items():\n            if k in combined_metadata:\n                combined_metadata[k] += f", {v}"\n            else:\n                combined_metadata[k] = str(v)\n    return Document(page_content=result, metadata=combined_metadata)', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceDocumentsChain(BaseCombineDocumentsChain):\n    """Combining documents by mapping a chain over them, then combining results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\map_reduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:07,081 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:07,081 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:07,092 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by mapping a chain over them first, then reranking results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union, cast', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.output_parsers.regex import RegexParser', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapRerankDocumentsChain(BaseCombineDocumentsChain):\n    """Combining documents by mapping a chain over them, then reranking results."""', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\map_rerank.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:09,395 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:09,395 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:09,407 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Combining documents by doing a first pass and then refining on more documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Dict, List, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import (\n    BaseCombineDocumentsChain,\n    format_document,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_default_document_prompt() -> PromptTemplate:\n    return PromptTemplate(input_variables=["page_content"], template="{page_content}")', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RefineDocumentsChain(BaseCombineDocumentsChain):\n    """Combine documents by doing a first pass and then refining on more documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\refine.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:12,316 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:12,316 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:12,324 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that combines documents by stuffing into context."""', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Tuple', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import Callbacks\nfrom langchain.chains.combine_documents.base import (\n    BaseCombineDocumentsChain,\n    format_document,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_default_document_prompt() -> PromptTemplate:\n    return PromptTemplate(input_variables=["page_content"], template="{page_content}")', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StuffDocumentsChain(BaseCombineDocumentsChain):\n    """Chain that combines documents by stuffing into context."""', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\stuff.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:14,886 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:14,886 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:14,890 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Different ways to combine documents."""', metadata={'source': 'embeddings\\chains\\combine_documents\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\combine_documents\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:17,253 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:17,253 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:17,267 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for applying constitutional principles to the outputs of another chain."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\nfrom langchain.chains.constitutional_ai.prompts import CRITIQUE_PROMPT, REVISION_PROMPT\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConstitutionalChain(Chain):\n    """Chain for applying constitutional principles.', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:19,640 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:19,640 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:19,649 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Models for the Constitutional AI chain."""\nfrom pydantic import BaseModel', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConstitutionalPrinciple(BaseModel):\n    """Class for a constitutional principle."""', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\models.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:22,280 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:22,280 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 22:50:22,288 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Dict', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PRINCIPLES: Dict[str, ConstitutionalPrinciple] = {}', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\constitutional_ai\\principles.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 22:50:24,706 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 22:50:24,707 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:01:58,641 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:01:58,642 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:01:58,642 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:01:59,622 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:01:59,622 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:01:59,623 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:01:59,624 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:01:59,624 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:01:59,835 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:01:59,836 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:01:59,836 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:01:59,836 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:01:59,836 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:01:59,836 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:01:59,837 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:01:59,838 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:01:59,839 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 23:01:59,840 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 23:01:59,841 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 23:01:59,841 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 23:01:59,841 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 23:01:59,841 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 23:01:59,841 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 23:02:07,735 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:07,735 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:08,773 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base callback handler that can be used to handle callbacks in langchain."""\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class LLMManagerMixin:\n    """Mixin for LLM callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ChainManagerMixin:\n    """Mixin for chain callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class ToolManagerMixin:\n    """Mixin for tool callbacks."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerMixin:\n    """Mixin for callback manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class RunManagerMixin:\n    """Mixin for run manager."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseCallbackHandler(\n    LLMManagerMixin,\n    ChainManagerMixin,\n    ToolManagerMixin,\n    CallbackManagerMixin,\n    RunManagerMixin,\n):\n    """Base callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackHandler(BaseCallbackHandler):\n    """Async callback handler that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseCallbackManager(CallbackManagerMixin):\n    """Base callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:11,834 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:11,834 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:11,844 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n    load_json,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_clearml() -> Any:\n    try:\n        import clearml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the clearml callback manager you need to have the clearml python "\n            "package installed. Please install it with pip install clearml"\n        )\n    return clearml', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ClearMLCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to ClearML.', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\clearml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:13,949 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:13,949 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:13,973 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, Generation, LLMResult', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='LANGCHAIN_MODEL_NAME = "langchain-model"', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def import_comet_ml() -> Any:\n    try:\n        import comet_ml  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the comet_ml callback manager you need to have the "\n            "comet_ml python package installed. Please install it with"\n            " pip install comet_ml"\n        )\n    return comet_ml', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _get_experiment(\n    workspace: Optional[str] = None, project_name: Optional[str] = None\n) -> Any:\n    comet_ml = import_comet_ml()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _fetch_text_complexity_metrics(text: str) -> dict:\n    textstat = import_textstat()\n    text_complexity_metrics = {\n        "flesch_reading_ease": textstat.flesch_reading_ease(text),\n        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),\n        "smog_index": textstat.smog_index(text),\n        "coleman_liau_index": textstat.coleman_liau_index(text),\n        "automated_readability_index": textstat.automated_readability_index(text),\n        "dale_chall_readability_score": textstat.dale_chall_readability_score(text),\n        "difficult_words": textstat.difficult_words(text),\n        "linsear_write_formula": textstat.linsear_write_formula(text),\n        "gunning_fog": textstat.gunning_fog(text),\n        "text_standard": textstat.text_standard(text),\n        "fernandez_huerta": textstat.fernandez_huerta(text),\n        "szigriszt_pazos": textstat.szigriszt_pazos(text),\n        "gutierrez_polini": textstat.gutierrez_polini(text),\n        "crawford": textstat.crawford(text),\n        "gulpease_index": textstat.gulpease_index(text),\n        "osman": textstat.osman(text),\n    }\n    return text_complexity_metrics', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _summarize_metrics_for_generated_outputs(metrics: Sequence) -> dict:\n    pd = import_pandas()\n    metrics_df = pd.DataFrame(metrics)\n    metrics_summary = metrics_df.describe()', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class CometCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Comet.', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\comet_ml_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:16,116 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:16,116 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:16,175 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nimport functools\nimport os\nimport warnings\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, Generator, List, Optional, Type, TypeVar, Union\nfrom uuid import UUID, uuid4', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n    ChainManagerMixin,\n    LLMManagerMixin,\n    RunManagerMixin,\n    ToolManagerMixin,\n)\nfrom langchain.callbacks.openai_info import OpenAICallbackHandler\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.callbacks.tracers.base import TracerSession\nfrom langchain.callbacks.tracers.langchain import LangChainTracer, LangChainTracerV2\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Callbacks = Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='openai_callback_var: ContextVar[Optional[OpenAICallbackHandler]] = ContextVar(\n    "openai_callback", default=None\n)\ntracing_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(\n    "tracing_callback", default=None\n)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='@contextmanager\ndef get_openai_callback() -> Generator[OpenAICallbackHandler, None, None]:\n    """Get OpenAI callback handler in a context manager."""\n    cb = OpenAICallbackHandler()\n    openai_callback_var.set(cb)\n    yield cb\n    openai_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get Tracer in a context manager."""\n    cb = LangChainTracer()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='@contextmanager\ndef tracing_v2_enabled(\n    session_name: str = "default",\n) -> Generator[TracerSession, None, None]:\n    """Get the experimental tracer handler in a context manager."""\n    # Issue a warning that this is experimental\n    warnings.warn(\n        "The experimental tracing v2 is in development. "\n        "This is not yet stable and may change in the future."\n    )\n    cb = LangChainTracerV2()\n    session = cb.load_session(session_name)\n    tracing_callback_var.set(cb)\n    yield session\n    tracing_callback_var.set(None)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _handle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    for handler in handlers:\n        try:\n            if ignore_condition_name is None or not getattr(\n                handler, ignore_condition_name\n            ):\n                getattr(handler, event_name)(args, **kwargs)\n        except Exception as e:\n            # TODO: switch this to use logging\n            print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event_for_handler(\n    handler: BaseCallbackHandler,\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    try:\n        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):\n            event = getattr(handler, event_name)\n            if asyncio.iscoroutinefunction(event):\n                await event(args, kwargs)\n            else:\n                await asyncio.get_event_loop().run_in_executor(\n                    None, functools.partial(event, *args, kwargs)\n                )\n    except Exception as e:\n        # TODO: switch this to use logging\n        print(f"Error in {event_name} callback: {e}")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='async def _ahandle_event(\n    handlers: List[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    args: Any,\n    kwargs: Any,\n) -> None:\n    """Generic event handler for AsyncCallbackManager."""\n    await asyncio.gather(\n        (\n            _ahandle_event_for_handler(\n                handler, event_name, ignore_condition_name, args, *kwargs\n            )\n            for handler in handlers\n        )\n    )', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='BRM = TypeVar("BRM", bound="BaseRunManager")', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BaseRunManager(RunManagerMixin):\n    """Base class for run manager (a bound callback manager)."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class RunManager(BaseRunManager):\n    """Sync Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncRunManager(BaseRunManager):\n    """Async Run Manager."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForLLMRun(RunManager, LLMManagerMixin):\n    """Callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):\n    """Async callback manager for LLM run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForChainRun(RunManager, ChainManagerMixin):\n    """Callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForChainRun(AsyncRunManager, ChainManagerMixin):\n    """Async callback manager for chain run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManagerForToolRun(RunManager, ToolManagerMixin):\n    """Callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class AsyncCallbackManagerForToolRun(AsyncRunManager, ToolManagerMixin):\n    """Async callback manager for tool run."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class CallbackManager(BaseCallbackManager):\n    """Callback manager that can be used to handle callbacks from langchain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class AsyncCallbackManager(BaseCallbackManager):\n    """Async callback manager that can be used to handle callbacks from LangChain."""', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='T = TypeVar("T", CallbackManager, AsyncCallbackManager)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def configure(\n    callback_manager_cls: Type[T],\n    inheritable_callbacks: Callbacks = None,\n    local_callbacks: Callbacks = None,\n    verbose: bool = False,\n) -> T:\n    """Configure the callback manager."""\n    callback_manager = callback_manager_cls([])\n    if inheritable_callbacks or local_callbacks:\n        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:\n            inheritable_callbacks = inheritable_callbacks or []\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks_.copy(),\n                inheritable_handlers=inheritable_callbacks_.copy(),\n            )\n        else:\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks.handlers,\n                inheritable_handlers=inheritable_callbacks.inheritable_handlers,\n                parent_run_id=inheritable_callbacks.parent_run_id,\n            )\n        local_handlers_ = (\n            local_callbacks\n            if isinstance(local_callbacks, list)\n            else (local_callbacks.handlers if local_callbacks else [])\n        )\n        for handler in local_handlers_:\n            callback_manager.add_handler(handler, False)', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\manager.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:19,126 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:19,126 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:19,136 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='MODEL_COST_PER_1K_TOKENS = {\n    "gpt-4": 0.03,\n    "gpt-4-0314": 0.03,\n    "gpt-4-completion": 0.06,\n    "gpt-4-0314-completion": 0.06,\n    "gpt-4-32k": 0.06,\n    "gpt-4-32k-0314": 0.06,\n    "gpt-4-32k-completion": 0.12,\n    "gpt-4-32k-0314-completion": 0.12,\n    "gpt-3.5-turbo": 0.002,\n    "gpt-3.5-turbo-0301": 0.002,\n    "text-ada-001": 0.0004,\n    "ada": 0.0004,\n    "text-babbage-001": 0.0005,\n    "babbage": 0.0005,\n    "text-curie-001": 0.002,\n    "curie": 0.002,\n    "text-davinci-003": 0.02,\n    "text-davinci-002": 0.02,\n    "code-davinci-002": 0.02,\n}', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def get_openai_token_cost_for_model(\n    model_name: str, num_tokens: int, is_completion: bool = False\n) -> float:\n    suffix = "-completion" if is_completion and model_name.startswith("gpt-4") else ""\n    model = model_name.lower() + suffix\n    if model not in MODEL_COST_PER_1K_TOKENS:\n        raise ValueError(\n            f"Unknown model: {model_name}. Please provide a valid OpenAI model name."\n            "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())\n        )\n    return MODEL_COST_PER_1K_TOKENS[model] * num_tokens / 1000', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class OpenAICallbackHandler(BaseCallbackHandler):\n    """Callback Handler that tracks OpenAI info."""', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\openai_info.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:21,925 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:21,925 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:21,933 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that prints to std out."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.input import print_text\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class StdOutCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that prints to std out."""', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:24,481 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:24,481 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:24,491 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import asyncio\nfrom typing import Any, AsyncIterator, Dict, List, Literal, Union, cast', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import AsyncCallbackHandler\nfrom langchain.schema import LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="TODO If used by two LLM runs in parallel this won't work as expected", metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class AsyncIteratorCallbackHandler(AsyncCallbackHandler):\n    """Callback handler that returns an async iterator."""', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_aiter.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:27,200 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:27,200 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:27,206 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler streams to stdout on new llm token."""\nimport sys\nfrom typing import Any, Dict, List, Union', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n    """Callback handler for streaming. Only works with LLMs that support streaming."""', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streaming_stdout.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:29,670 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:29,671 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:29,678 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Callback Handler that logs to streamlit."""\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import streamlit as st', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class StreamlitCallbackHandler(BaseCallbackHandler):\n    """Callback Handler that logs to streamlit."""', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\streamlit.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:31,910 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:31,911 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:31,929 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport hashlib\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, Tuple, Union', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_spacy() -> Any:\n    try:\n        import spacy\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the spacy python "\n            "package installed. Please install it with pip install spacy"\n        )\n    return spacy', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_pandas() -> Any:\n    try:\n        import pandas\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the pandas python "\n            "package installed. Please install it with pip install pandas"\n        )\n    return pandas', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def import_textstat() -> Any:\n    try:\n        import textstat\n    except ImportError:\n        raise ImportError(\n            "This callback manager requires the textstat python "\n            "package installed. Please install it with pip install textstat"\n        )\n    return textstat', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = ""\n) -> Iterable[Tuple[str, Any]]:\n    """\n    Generator that yields flattened items from a nested dictionary for a flat dict.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def flatten_dict(\n    nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"\n) -> Dict[str, Any]:\n    """Flattens a nested dictionary into a flat dictionary.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def hash_string(s: str) -> str:\n    """Hash a string using sha1.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json(json_path: Union[str, Path]) -> str:\n    """Load json file to a string.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BaseMetadataCallbackHandler:\n    """This class handles the metadata and associated function states for callbacks.', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\utils.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:34,690 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:34,690 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:34,710 - ye_logger_of_yor - INFO - [Document(page_content='```python\nimport json\nimport tempfile\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence, Union', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.utils import (\n    BaseMetadataCallbackHandler,\n    flatten_dict,\n    hash_string,\n    import_pandas,\n    import_spacy,\n    import_textstat,\n)\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def import_wandb() -> Any:\n    try:\n        import wandb  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            "To use the wandb callback manager you need to have the wandb python "\n            "package installed. Please install it with pip install wandb"\n        )\n    return wandb', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_json_to_dict(json_path: Union[str, Path]) -> dict:\n    """Load json file to a dictionary.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def analyze_text(\n    text: str,\n    complexity_metrics: bool = True,\n    visualize: bool = True,\n    nlp: Any = None,\n    output_dir: Optional[Union[str, Path]] = None,\n) -> dict:\n    """Analyze text using textstat and spacy.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def construct_html_from_prompt_and_generation(prompt: str, generation: str) -> Any:\n    """Construct an html element from a prompt and a generation.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class WandbCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):\n    """Callback Handler that logs to Weights and Biases.', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\callbacks\\wandb_callback.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:36,902 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:36,903 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:36,914 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface that all chains should implement."""\nimport inspect\nimport json\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml\nfrom pydantic import BaseModel, Field, root_validator, validator', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_verbosity() -> bool:\n    return langchain.verbose', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class Chain(BaseModel, ABC):\n    """Base interface that all chains should implement."""', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:39,827 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:39,827 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:39,837 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that just formats a prompt and calls an LLM."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import LLMResult, PromptValue', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMChain(Chain):\n    """Chain to run queries against LLMs.', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:42,290 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:42,290 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:42,301 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that hits a URL and then uses an LLM to parse results."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_HEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"  # noqa: E501\n}', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMRequestsChain(Chain):\n    """Chain that hits a URL and then uses an LLM to parse results."""', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:44,732 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:44,733 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:44,778 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading chains."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, Union', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.api.base import APIChain\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\nfrom langchain.llms.loading import load_llm, load_llm_from_config\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\nfrom langchain.utilities.loading import try_load_from_hub', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/"', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\n    """Load LLM chain from config dict."""\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_hyde_chain(config: dict, kwargs: Any) -> HypotheticalDocumentEmbedder:\n    """Load hypothetical document embedder chain from config dict."""\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "embeddings" in kwargs:\n        embeddings = kwargs.pop("embeddings")\n    else:\n        raise ValueError("embeddings must be present.")\n    return HypotheticalDocumentEmbedder(\n        llm_chain=llm_chain, base_embeddings=embeddings, config\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_reduce_documents_chain(\n    config: dict, **kwargs: Any\n) -> MapReduceDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_bash_chain(config: dict, kwargs: Any) -> LLMBashChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMBashChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_checker_chain(config: dict, kwargs: Any) -> LLMCheckerChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "create_draft_answer_prompt" in config:\n        create_draft_answer_prompt_config = config.pop("create_draft_answer_prompt")\n        create_draft_answer_prompt = load_prompt_from_config(\n            create_draft_answer_prompt_config\n        )\n    elif "create_draft_answer_prompt_path" in config:\n        create_draft_answer_prompt = load_prompt(\n            config.pop("create_draft_answer_prompt_path")\n        )\n    if "list_assertions_prompt" in config:\n        list_assertions_prompt_config = config.pop("list_assertions_prompt")\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\n    elif "list_assertions_prompt_path" in config:\n        list_assertions_prompt = load_prompt(config.pop("list_assertions_prompt_path"))\n    if "check_assertions_prompt" in config:\n        check_assertions_prompt_config = config.pop("check_assertions_prompt")\n        check_assertions_prompt = load_prompt_from_config(\n            check_assertions_prompt_config\n        )\n    elif "check_assertions_prompt_path" in config:\n        check_assertions_prompt = load_prompt(\n            config.pop("check_assertions_prompt_path")\n        )\n    if "revised_answer_prompt" in config:\n        revised_answer_prompt_config = config.pop("revised_answer_prompt")\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\n    elif "revised_answer_prompt_path" in config:\n        revised_answer_prompt = load_prompt(config.pop("revised_answer_prompt_path"))\n    return LLMCheckerChain(\n        llm=llm,\n        create_draft_answer_prompt=create_draft_answer_prompt,\n        list_assertions_prompt=list_assertions_prompt,\n        check_assertions_prompt=check_assertions_prompt,\n        revised_answer_prompt=revised_answer_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_math_chain(config: dict, kwargs: Any) -> LLMMathChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMMathChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_documents_chain(\n    config: dict, kwargs: Any\n) -> MapRerankDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")\n    return MapRerankDocumentsChain(llm_chain=llm_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_pal_chain(config: dict, kwargs: Any) -> PALChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    else:\n        raise ValueError("One of prompt or prompt_path must be present.")\n    return PALChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_documents_chain(config: dict, kwargs: Any) -> RefineDocumentsChain:\n    if "initial_llm_chain" in config:\n        initial_llm_chain_config = config.pop("initial_llm_chain")\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\n    elif "initial_llm_chain_path" in config:\n        initial_llm_chain = load_chain(config.pop("initial_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of initial_llm_chain or initial_llm_chain_config must be present."\n        )\n    if "refine_llm_chain" in config:\n        refine_llm_chain_config = config.pop("refine_llm_chain")\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\n    elif "refine_llm_chain_path" in config:\n        refine_llm_chain = load_chain(config.pop("refine_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of refine_llm_chain or refine_llm_chain_config must be present."\n        )\n    if "document_prompt" in config:\n        prompt_config = config.pop("document_prompt")\n        document_prompt = load_prompt_from_config(prompt_config)\n    elif "document_prompt_path" in config:\n        document_prompt = load_prompt(config.pop("document_prompt_path"))\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_llm_chain,\n        refine_llm_chain=refine_llm_chain,\n        document_prompt=document_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_qa_with_sources_chain(config: dict, kwargs: Any) -> QAWithSourcesChain:\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_sql_database_chain(config: dict, kwargs: Any) -> SQLDatabaseChain:\n    if "database" in kwargs:\n        database = kwargs.pop("database")\n    else:\n        raise ValueError("database must be present.")\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa_with_sources_chain(\n    config: dict, kwargs: Any\n) -> VectorDBQAWithSourcesChain:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQAWithSourcesChain(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa(config: dict, kwargs: Any) -> VectorDBQA:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQA(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_api_chain(config: dict, kwargs: Any) -> APIChain:\n    if "api_request_chain" in config:\n        api_request_chain_config = config.pop("api_request_chain")\n        api_request_chain = load_chain_from_config(api_request_chain_config)\n    elif "api_request_chain_path" in config:\n        api_request_chain = load_chain(config.pop("api_request_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_request_chain or api_request_chain_path must be present."\n        )\n    if "api_answer_chain" in config:\n        api_answer_chain_config = config.pop("api_answer_chain")\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\n    elif "api_answer_chain_path" in config:\n        api_answer_chain = load_chain(config.pop("api_answer_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_answer_chain or api_answer_chain_path must be present."\n        )\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n    else:\n        raise ValueError("requests_wrapper must be present.")\n    return APIChain(\n        api_request_chain=api_request_chain,\n        api_answer_chain=api_answer_chain,\n        requests_wrapper=requests_wrapper,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_requests_chain(config: dict, kwargs: Any) -> LLMRequestsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n        return LLMRequestsChain(\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, config\n        )\n    else:\n        return LLMRequestsChain(llm_chain=llm_chain, **config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='type_to_loader_dict = {\n    "api_chain": _load_api_chain,\n    "hyde_chain": _load_hyde_chain,\n    "llm_chain": _load_llm_chain,\n    "llm_bash_chain": _load_llm_bash_chain,\n    "llm_checker_chain": _load_llm_checker_chain,\n    "llm_math_chain": _load_llm_math_chain,\n    "llm_requests_chain": _load_llm_requests_chain,\n    "pal_chain": _load_pal_chain,\n    "qa_with_sources_chain": _load_qa_with_sources_chain,\n    "stuff_documents_chain": _load_stuff_documents_chain,\n    "map_reduce_documents_chain": _load_map_reduce_documents_chain,\n    "map_rerank_documents_chain": _load_map_rerank_documents_chain,\n    "refine_documents_chain": _load_refine_documents_chain,\n    "sql_database_chain": _load_sql_database_chain,\n    "vector_db_qa_with_sources_chain": _load_vector_db_qa_with_sources_chain,\n    "vector_db_qa": _load_vector_db_qa,\n}', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\n    """Load chain from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify a chain Type in config")\n    config_type = config.pop("_type")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_chain(path: Union[str, Path], kwargs: Any) -> Chain:\n    """Unified method for loading a chain from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_chain_from_file, "chains", {"json", "yaml"}, kwargs\n    ):\n        return hub_result\n    else:\n        return _load_chain_from_file(path, **kwargs)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\n    """Load chain from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:47,629 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:47,630 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:47,640 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Map-reduce chain.', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Splits up a document, sends the smaller parts to the LLM with one prompt,\nthen combines the results with another one.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import TextSplitter', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceChain(Chain):\n    """Map-reduce chain."""', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:50,071 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:50,071 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:50,080 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Pass input through a moderation endpoint."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import root_validator', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.utils import get_from_dict_or_env', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class OpenAIModerationChain(Chain):\n    """Pass input through a moderation endpoint.', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\moderation.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:52,960 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:52,960 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:52,980 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Tuple', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import BaseModel, Field', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class BasePromptSelector(BaseModel, ABC):\n    @abstractmethod\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\n        """Get default prompt for a language model."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConditionalPromptSelector(BasePromptSelector):\n    """Prompt collection that goes through conditionals."""', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def is_llm(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseLLM)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def is_chat_model(llm: BaseLanguageModel) -> bool:\n    return isinstance(llm, BaseChatModel)', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\prompt_selector.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:55,350 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:55,351 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:55,363 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain pipeline where the outputs of one step feed directly into next."""\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_color_mapping', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class SequentialChain(Chain):\n    """Chain where the outputs of one chain feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class SimpleSequentialChain(Chain):\n    """Simple chain where the outputs of one step feed directly into next."""', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\sequential.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:02:58,319 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:02:58,319 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:02:58,326 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that runs an arbitrary python function."""\nfrom typing import Callable, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class TransformChain(Chain):\n    """Chain transform chain output.', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\transform.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:03:00,905 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:03:00,906 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:03:00,916 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chains are easily reusable components which can be linked together."""\nfrom langchain.chains.api.base import APIChain\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\nfrom langchain.chains.conversation.base import ConversationChain\nfrom langchain.chains.conversational_retrieval.base import (\n    ChatVectorDBChain,\n    ConversationalRetrievalChain,\n)\nfrom langchain.chains.graph_qa.base import GraphQAChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\nfrom langchain.chains.loading import load_chain\nfrom langchain.chains.mapreduce import MapReduceChain\nfrom langchain.chains.moderation import OpenAIModerationChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_generation.base import QAGenerationChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\nfrom langchain.chains.sql_database.base import (\n    SQLDatabaseChain,\n    SQLDatabaseSequentialChain,\n)\nfrom langchain.chains.transform import TransformChain', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='all = [\n    "ConversationChain",\n    "LLMChain",\n    "LLMBashChain",\n    "LLMCheckerChain",\n    "LLMSummarizationCheckerChain",\n    "LLMMathChain",\n    "PALChain",\n    "QAWithSourcesChain",\n    "SQLDatabaseChain",\n    "SequentialChain",\n    "SimpleSequentialChain",\n    "VectorDBQA",\n    "VectorDBQAWithSourcesChain",\n    "APIChain",\n    "LLMRequestsChain",\n    "TransformChain",\n    "MapReduceChain",\n    "OpenAIModerationChain",\n    "SQLDatabaseSequentialChain",\n    "load_chain",\n    "AnalyzeDocumentChain",\n    "HypotheticalDocumentEmbedder",\n    "ChatVectorDBChain",\n    "GraphQAChain",\n    "ConstitutionalChain",\n    "QAGenerationChain",\n    "RetrievalQA",\n    "RetrievalQAWithSourcesChain",\n    "ConversationalRetrievalChain",\n    "OpenAPIEndpointChain",\n]', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:03:03,558 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:03:03,558 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:11,860 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:04:11,860 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:04:11,860 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:04:12,815 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:04:12,815 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:04:12,815 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:04:12,815 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:04:12,816 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:04:13,028 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:04:13,029 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:04:13,029 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:04:13,030 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:04:13,031 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:04:13,032 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:04:13,033 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 23:04:13,034 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 23:04:13,035 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 23:04:13,035 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 23:04:44,787 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:04:44,787 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:45,815 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Base interface that all chains should implement."""\nimport inspect\nimport json\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml\nfrom pydantic import BaseModel, Field, root_validator, validator', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import langchain\nfrom langchain.callbacks.base import BaseCallbackManager\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _get_verbosity() -> bool:\n    return langchain.verbose', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class Chain(BaseModel, ABC):\n    """Base interface that all chains should implement."""', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:04:48,822 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:04:48,823 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:48,830 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that just formats a prompt and calls an LLM."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional, Sequence, Tuple, Union', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForChainRun,\n    CallbackManager,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import LLMResult, PromptValue', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMChain(Chain):\n    """Chain to run queries against LLMs.', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:04:51,561 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:04:51,561 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:51,570 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that hits a URL and then uses an LLM to parse results."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains import LLMChain\nfrom langchain.chains.base import Chain\nfrom langchain.requests import TextRequestsWrapper', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_HEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"  # noqa: E501\n}', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMRequestsChain(Chain):\n    """Chain that hits a URL and then uses an LLM to parse results."""', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_requests.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:04:54,194 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:04:54,194 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:54,239 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Functionality for loading chains."""\nimport json\nfrom pathlib import Path\nfrom typing import Any, Union', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import yaml', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.api.base import APIChain\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.base import LLMBashChain\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\nfrom langchain.chains.llm_math.base import LLMMathChain\nfrom langchain.chains.llm_requests import LLMRequestsChain\nfrom langchain.chains.pal.base import PALChain\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\nfrom langchain.chains.retrieval_qa.base import VectorDBQA\nfrom langchain.chains.sql_database.base import SQLDatabaseChain\nfrom langchain.llms.loading import load_llm, load_llm_from_config\nfrom langchain.prompts.loading import load_prompt, load_prompt_from_config\nfrom langchain.utilities.loading import try_load_from_hub', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/"', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\n    """Load LLM chain from config dict."""\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_hyde_chain(config: dict, kwargs: Any) -> HypotheticalDocumentEmbedder:\n    """Load hypothetical document embedder chain from config dict."""\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "embeddings" in kwargs:\n        embeddings = kwargs.pop("embeddings")\n    else:\n        raise ValueError("embeddings must be present.")\n    return HypotheticalDocumentEmbedder(\n        llm_chain=llm_chain, base_embeddings=embeddings, config\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_reduce_documents_chain(\n    config: dict, **kwargs: Any\n) -> MapReduceDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_bash_chain(config: dict, kwargs: Any) -> LLMBashChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMBashChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_checker_chain(config: dict, kwargs: Any) -> LLMCheckerChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "create_draft_answer_prompt" in config:\n        create_draft_answer_prompt_config = config.pop("create_draft_answer_prompt")\n        create_draft_answer_prompt = load_prompt_from_config(\n            create_draft_answer_prompt_config\n        )\n    elif "create_draft_answer_prompt_path" in config:\n        create_draft_answer_prompt = load_prompt(\n            config.pop("create_draft_answer_prompt_path")\n        )\n    if "list_assertions_prompt" in config:\n        list_assertions_prompt_config = config.pop("list_assertions_prompt")\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\n    elif "list_assertions_prompt_path" in config:\n        list_assertions_prompt = load_prompt(config.pop("list_assertions_prompt_path"))\n    if "check_assertions_prompt" in config:\n        check_assertions_prompt_config = config.pop("check_assertions_prompt")\n        check_assertions_prompt = load_prompt_from_config(\n            check_assertions_prompt_config\n        )\n    elif "check_assertions_prompt_path" in config:\n        check_assertions_prompt = load_prompt(\n            config.pop("check_assertions_prompt_path")\n        )\n    if "revised_answer_prompt" in config:\n        revised_answer_prompt_config = config.pop("revised_answer_prompt")\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\n    elif "revised_answer_prompt_path" in config:\n        revised_answer_prompt = load_prompt(config.pop("revised_answer_prompt_path"))\n    return LLMCheckerChain(\n        llm=llm,\n        create_draft_answer_prompt=create_draft_answer_prompt,\n        list_assertions_prompt=list_assertions_prompt,\n        check_assertions_prompt=check_assertions_prompt,\n        revised_answer_prompt=revised_answer_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_math_chain(config: dict, kwargs: Any) -> LLMMathChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    return LLMMathChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_documents_chain(\n    config: dict, kwargs: Any\n) -> MapRerankDocumentsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_config must be present.")\n    return MapRerankDocumentsChain(llm_chain=llm_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_pal_chain(config: dict, kwargs: Any) -> PALChain:\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    elif "prompt_path" in config:\n        prompt = load_prompt(config.pop("prompt_path"))\n    else:\n        raise ValueError("One of prompt or prompt_path must be present.")\n    return PALChain(llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_documents_chain(config: dict, kwargs: Any) -> RefineDocumentsChain:\n    if "initial_llm_chain" in config:\n        initial_llm_chain_config = config.pop("initial_llm_chain")\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\n    elif "initial_llm_chain_path" in config:\n        initial_llm_chain = load_chain(config.pop("initial_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of initial_llm_chain or initial_llm_chain_config must be present."\n        )\n    if "refine_llm_chain" in config:\n        refine_llm_chain_config = config.pop("refine_llm_chain")\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\n    elif "refine_llm_chain_path" in config:\n        refine_llm_chain = load_chain(config.pop("refine_llm_chain_path"))\n    else:\n        raise ValueError(\n            "One of refine_llm_chain or refine_llm_chain_config must be present."\n        )\n    if "document_prompt" in config:\n        prompt_config = config.pop("document_prompt")\n        document_prompt = load_prompt_from_config(prompt_config)\n    elif "document_prompt_path" in config:\n        document_prompt = load_prompt(config.pop("document_prompt_path"))\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_llm_chain,\n        refine_llm_chain=refine_llm_chain,\n        document_prompt=document_prompt,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_qa_with_sources_chain(config: dict, kwargs: Any) -> QAWithSourcesChain:\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_sql_database_chain(config: dict, kwargs: Any) -> SQLDatabaseChain:\n    if "database" in kwargs:\n        database = kwargs.pop("database")\n    else:\n        raise ValueError("database must be present.")\n    if "llm" in config:\n        llm_config = config.pop("llm")\n        llm = load_llm_from_config(llm_config)\n    elif "llm_path" in config:\n        llm = load_llm(config.pop("llm_path"))\n    else:\n        raise ValueError("One of llm or llm_path must be present.")\n    if "prompt" in config:\n        prompt_config = config.pop("prompt")\n        prompt = load_prompt_from_config(prompt_config)\n    return SQLDatabaseChain(database=database, llm=llm, prompt=prompt, config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa_with_sources_chain(\n    config: dict, kwargs: Any\n) -> VectorDBQAWithSourcesChain:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQAWithSourcesChain(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_vector_db_qa(config: dict, kwargs: Any) -> VectorDBQA:\n    if "vectorstore" in kwargs:\n        vectorstore = kwargs.pop("vectorstore")\n    else:\n        raise ValueError("vectorstore must be present.")\n    if "combine_documents_chain" in config:\n        combine_documents_chain_config = config.pop("combine_documents_chain")\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\n    elif "combine_documents_chain_path" in config:\n        combine_documents_chain = load_chain(config.pop("combine_documents_chain_path"))\n    else:\n        raise ValueError(\n            "One of combine_documents_chain or "\n            "combine_documents_chain_path must be present."\n        )\n    return VectorDBQA(\n        combine_documents_chain=combine_documents_chain,\n        vectorstore=vectorstore,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_api_chain(config: dict, kwargs: Any) -> APIChain:\n    if "api_request_chain" in config:\n        api_request_chain_config = config.pop("api_request_chain")\n        api_request_chain = load_chain_from_config(api_request_chain_config)\n    elif "api_request_chain_path" in config:\n        api_request_chain = load_chain(config.pop("api_request_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_request_chain or api_request_chain_path must be present."\n        )\n    if "api_answer_chain" in config:\n        api_answer_chain_config = config.pop("api_answer_chain")\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\n    elif "api_answer_chain_path" in config:\n        api_answer_chain = load_chain(config.pop("api_answer_chain_path"))\n    else:\n        raise ValueError(\n            "One of api_answer_chain or api_answer_chain_path must be present."\n        )\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n    else:\n        raise ValueError("requests_wrapper must be present.")\n    return APIChain(\n        api_request_chain=api_request_chain,\n        api_answer_chain=api_answer_chain,\n        requests_wrapper=requests_wrapper,\n        config,\n    )', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_llm_requests_chain(config: dict, kwargs: Any) -> LLMRequestsChain:\n    if "llm_chain" in config:\n        llm_chain_config = config.pop("llm_chain")\n        llm_chain = load_chain_from_config(llm_chain_config)\n    elif "llm_chain_path" in config:\n        llm_chain = load_chain(config.pop("llm_chain_path"))\n    else:\n        raise ValueError("One of llm_chain or llm_chain_path must be present.")\n    if "requests_wrapper" in kwargs:\n        requests_wrapper = kwargs.pop("requests_wrapper")\n        return LLMRequestsChain(\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, config\n        )\n    else:\n        return LLMRequestsChain(llm_chain=llm_chain, **config)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='type_to_loader_dict = {\n    "api_chain": _load_api_chain,\n    "hyde_chain": _load_hyde_chain,\n    "llm_chain": _load_llm_chain,\n    "llm_bash_chain": _load_llm_bash_chain,\n    "llm_checker_chain": _load_llm_checker_chain,\n    "llm_math_chain": _load_llm_math_chain,\n    "llm_requests_chain": _load_llm_requests_chain,\n    "pal_chain": _load_pal_chain,\n    "qa_with_sources_chain": _load_qa_with_sources_chain,\n    "stuff_documents_chain": _load_stuff_documents_chain,\n    "map_reduce_documents_chain": _load_map_reduce_documents_chain,\n    "map_rerank_documents_chain": _load_map_rerank_documents_chain,\n    "refine_documents_chain": _load_refine_documents_chain,\n    "sql_database_chain": _load_sql_database_chain,\n    "vector_db_qa_with_sources_chain": _load_vector_db_qa_with_sources_chain,\n    "vector_db_qa": _load_vector_db_qa,\n}', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\n    """Load chain from Config Dict."""\n    if "_type" not in config:\n        raise ValueError("Must specify a chain Type in config")\n    config_type = config.pop("_type")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_chain(path: Union[str, Path], kwargs: Any) -> Chain:\n    """Unified method for loading a chain from LangChainHub or local fs."""\n    if hub_result := try_load_from_hub(\n        path, _load_chain_from_file, "chains", {"json", "yaml"}, kwargs\n    ):\n        return hub_result\n    else:\n        return _load_chain_from_file(path, **kwargs)', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\n    """Load chain from file."""\n    # Convert file to Path object.\n    if isinstance(file, str):\n        file_path = Path(file)\n    else:\n        file_path = file\n    # Load from either json or yaml.\n    if file_path.suffix == ".json":\n        with open(file_path) as f:\n            config = json.load(f)\n    elif file_path.suffix == ".yaml":\n        with open(file_path, "r") as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError("File type must be json or yaml")', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:04:57,160 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:04:57,160 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:04:57,171 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Map-reduce chain.', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Splits up a document, sends the smaller parts to the LLM with one prompt,\nthen combines the results with another one.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Extra', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.docstore.document import Document\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import TextSplitter', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class MapReduceChain(Chain):\n    """Map-reduce chain."""', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\mapreduce.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:05:05,538 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:05:05,539 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:05:05,539 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:05:06,493 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:05:06,494 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:05:06,495 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:05:06,495 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:05:06,697 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:05:06,698 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:05:06,698 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:05:06,698 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:05:06,699 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:05:06,700 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:05:06,701 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 23:05:06,702 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 23:05:16,200 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:05:16,200 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:05:30,930 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:05:30,930 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:05:30,930 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:05:31,858 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:05:31,858 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:05:31,858 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:05:31,859 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:05:32,063 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:05:32,063 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:05:32,063 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:05:32,064 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:05:32,065 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:05:32,066 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:05:32,067 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 23:05:32,068 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 23:05:32,069 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 23:05:32,069 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 23:05:32,069 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 23:05:49,678 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:05:49,678 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:06:46,043 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:06:46,043 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:06:46,044 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:06:46,973 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:06:46,973 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:06:46,974 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:06:46,975 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:06:46,975 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:06:47,179 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:06:47,179 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:06:47,180 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:06:47,181 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:06:47,182 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:06:47,183 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - exit - Line 429
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - Scroll Area - Line 435
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - Main Window - Line 455
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - set background image - Line 467
2023-05-03 23:06:47,184 - ye_logger_of_yor - INFO - resize event - Line 484
2023-05-03 23:06:47,185 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 489
2023-05-03 23:06:47,185 - ye_logger_of_yor - INFO - send large text - Line 512
2023-05-03 23:06:47,185 - ye_logger_of_yor - INFO - main - Line 521
2023-05-03 23:07:22,653 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 23:13:31,286 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:13:31,287 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:13:31,287 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:13:32,296 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:13:32,296 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:13:32,296 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:13:32,296 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:13:32,296 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:13:32,297 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:13:32,297 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:13:32,297 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:13:32,297 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:13:32,297 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:13:32,518 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:13:32,519 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:13:32,519 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:13:32,519 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:13:32,519 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:13:32,520 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:13:32,521 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:13:32,521 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:13:32,521 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:14:51,839 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:14:51,839 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:14:51,839 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:14:52,869 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:14:52,870 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:14:52,870 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:14:52,870 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:14:52,870 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:14:52,871 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:14:52,871 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:14:52,871 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:14:52,871 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:14:52,871 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:14:53,088 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:14:53,089 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:14:53,089 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:14:53,089 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - open large input box - Line 213
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - clear history - Line 219
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - running embed file from Hex - Line 224
2023-05-03 23:14:53,090 - ye_logger_of_yor - INFO - run !embed from Hex - Line 234
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - run long docs - Line 242
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - run compressed docs - Line 250
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - run search agent - Line 258
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - run embed dir - Line 266
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - query memory - Line 275
2023-05-03 23:14:53,091 - ye_logger_of_yor - INFO - run addmem - Line 283
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - run add sitemap - Line 291
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - run embed project - Line 299
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - run ! commands - Line 307
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - logger.info help - Line 366
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - load file into chat - Line 390
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - save chat history to file - Line 403
2023-05-03 23:14:53,092 - ye_logger_of_yor - INFO - exit - Line 428
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - Scroll Area - Line 434
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - Main Window - Line 454
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - set background image - Line 466
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - resize event - Line 483
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 488
2023-05-03 23:14:53,093 - ye_logger_of_yor - INFO - send large text - Line 511
2023-05-03 23:14:53,094 - ye_logger_of_yor - INFO - main - Line 520
2023-05-03 23:15:08,453 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:15:08,454 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:15:29,818 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:15:29,819 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:15:29,819 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:15:30,784 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:15:30,784 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:15:30,784 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:15:30,785 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:15:30,786 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:15:30,993 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:15:30,993 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:15:30,993 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:15:30,994 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:15:30,995 - ye_logger_of_yor - INFO - open large input box - Line 213
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - clear history - Line 219
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - running embed file from Hex - Line 224
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - run !embed from Hex - Line 234
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - run long docs - Line 242
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - run compressed docs - Line 250
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - run search agent - Line 258
2023-05-03 23:15:30,996 - ye_logger_of_yor - INFO - run embed dir - Line 266
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - query memory - Line 275
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - run addmem - Line 283
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - run add sitemap - Line 291
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - run embed project - Line 299
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - run ! commands - Line 307
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - logger.info help - Line 366
2023-05-03 23:15:30,997 - ye_logger_of_yor - INFO - load file into chat - Line 390
2023-05-03 23:15:30,998 - ye_logger_of_yor - INFO - save chat history to file - Line 403
2023-05-03 23:15:30,998 - ye_logger_of_yor - INFO - exit - Line 428
2023-05-03 23:15:30,998 - ye_logger_of_yor - INFO - Scroll Area - Line 434
2023-05-03 23:15:30,998 - ye_logger_of_yor - INFO - Main Window - Line 454
2023-05-03 23:15:30,999 - ye_logger_of_yor - INFO - set background image - Line 466
2023-05-03 23:15:30,999 - ye_logger_of_yor - INFO - resize event - Line 483
2023-05-03 23:15:30,999 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 488
2023-05-03 23:15:30,999 - ye_logger_of_yor - INFO - send large text - Line 511
2023-05-03 23:15:30,999 - ye_logger_of_yor - INFO - main - Line 520
2023-05-03 23:16:29,939 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 23:16:43,890 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User:   File "C:\\Python310\\lib\\codecs.py", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0x92 in position 1194: invalid start byte\nAssistant: I\'m sorry you\'re experiencing an error. Can you please provide more context on when this error occurred and what you were trying to do at that moment?\n\n'] - Line 62
2023-05-03 23:17:39,960 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User:   File "C:\\Python310\\lib\\codecs.py", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0x92 in position 1194: invalid start byte\nAssistant: I\'m sorry you\'re experiencing an error. Can you please provide more context on when this error occurred and what you were trying to do at that moment?\n\n', 'User: from langchain.document_loaders import(\n    TextLoader,\n    PyPDFLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredFileLoader,\n    PDFMinerLoader\n)\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.text_splitter import CharacterTextSplitter\nimport nltk\nimport openai\nfrom dotenv import load_dotenv\nimport os\nfrom chatgpt import search_gpt\nfrom ye_logger_of_yor import get_logger\n\n\nlogger = get_logger()\n\n\nload_dotenv()\n\n\nlogger.info(\'Loading global variables\')\n#Load Langchain variables\nopenai.api_key = os.getenv("OPENAI_API_KEY")\nembeddings = OpenAIEmbeddings()\nllm = OpenAI(temperature=0)\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=25)\nvectorstore = \'docs/\'\n\n\nlogger.info(\'base_formatter function\')\ndef base_formatter(docs):\n    logger.info(\'formatting\')\n    print(f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n    return (f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n\n\nlogger.info(\'loading check_file function 43\')\n#Check if the files are valid\ndef check_file(file_path):\n    logger.info(\'checking file\')\n    if file_path.endswith(\'.txt\'):\n        loader = TextLoader(file_path)\n        if loader:\n            print(loader.load())\n            return loader.load()\n    if file_path.endswith(\'.pdf\'):\n        loader = UnstructuredFileLoader(file_path, mode=\'elements\', strategy=\'hi_res\', encoding=(\'utf-32\', \'ignore\'))\n        if loader:\n            print("pdf file loaded")\n            return loader.load()\n    if file_path.endswith(\'.md\'):\n        loader = UnstructuredFileLoader(file_path, mode=\'elements\', strategy=\'hi_res\', encoding=(\'utf-32\', \'ignore\'))\n        if loader:\n            logger.info(loader.load())\n            return loader.load()\n    else:\n        print("File type not supported")\n        return "File type not supported"\n\n\nlogger.info(\'loading create_mass_embedding function\')\n#Loop files in a folder path for embedding\ndef create_mass_embedding(folder_path):\n    logger.info(\'creating mass embedding\')\n    if not os.path.exists(folder_path):\n        folder_path = \'docs/empty\'\n        result = "Folder does not exist"\n        return result\n    for filename in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, filename)\n        result = create_embedding(file_path, filename)\n        if result:\n            print(f"Embedding created for {filename}: {result}")\n            with open(\'docs/index.txt\', \'a\') as f:\n                f.write(f"{os.path.join(folder_path, file_path)}\\n")\n            logger.info(f"Embedding created for {filename}: {result}")\n            return result\n        else:\n            logger.info(f"Embedding failed for {filename}")\n            return(f"Embedding failed for {filename}")\n\n\n\nlogger.info(\'create_embedding function\')\n#Embed a single embedding\ndef create_embedding(file_path, optional_arg="metadata"):\n    logger.info(\'creating embedding\')\n    data =check_file(file_path)\n    if data:\n        print("Data loaded")\n        metadata = optional_arg\n        if metadata:\n            meta = metadata\n        else:\n            meta = \'file_path\'\n        vectordb = Chroma.from_documents(documents=data, metadata=meta, embedding=embeddings, persist_directory=\'docs/\')\n        vectordb.persist()\n        return "Embedding created"\n    else:\n        return "Embedding failed"\n\n\n#Load vectorstore database\nlogger.info(\'load_embedding function\')\ndef load_embedding():\n    logger.info(\'loading embedding\')\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    return chromadb\n\n\nlogger.info(\'base_retriever function\')\n#Search for uncompressed docs in database\ndef base_retriever(user_query):\n    logger.info(\'running base_retriever\')\n    retriever = load_embedding().as_retriever(llm=llm)\n    docs = retriever.get_relevant_documents(user_query)\n    return docs\n\n\nlogger.info(\'retriever function\')\n#Search for compressed docs in database\ndef retriever(user_query):\n    logger.info(\'running retriever\')\n    compressor = LLMChainExtractor.from_llm(llm)\n    retriever = load_embedding().as_retriever(llm=llm)\n    cc_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n    compressed_docs = cc_retriever.get_relevant_documents(user_query)\n    docs = compressed_docs\n    return docs\n\n\nlogger.info(\'load_vector_store_docs function\')\ndef load_vector_store_docs():\n    logger.info(\'running load_vector_store_docs\')\n    vectorstore = \'docs/index\'\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    docs = chromadb.documents\n    return docs\n\n\nlogger.info(\'memory_search function\')\n#Query the database and pass the info to chatgpt for response\ndef memory_search(user_query):\n    logger.info(\'running memory_search\')\n    data = base_retriever(user_query)\n    prompt = [{\n        "role":"system",\n        "content":\'\'\'\n        "The user has asked this question:\n\n\n        {user_query}\n\n\n        You have looked up the relevant information from your data store and it is:\n\n\n        {data}\n\n\n        Please answer the user\'s question using the data as relevant context."\n        \'\'\'.format(user_query=user_query, data=data)\n    }]\n\n\n    result = search_gpt(user_query, prompt)\n\n\n    return result\nAssistant: Hello! How can I assist you today?\n\n'] - Line 62
2023-05-03 23:23:49,188 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:23:49,189 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:23:50,210 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that carries on a conversation and calls an LLM."""\nfrom typing import Dict, List', metadata={'source': 'embeddings\\chains\\conversation\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\conversation\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.conversation.prompt import PROMPT\nfrom langchain.chains.llm import LLMChain\nfrom langchain.memory.buffer import ConversationBufferMemory\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.schema import BaseMemory', metadata={'source': 'embeddings\\chains\\conversation\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class ConversationChain(LLMChain):\n    """Chain to have a conversation and load context from memory.', metadata={'source': 'embeddings\\chains\\conversation\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversation\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:23,271 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:23,271 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:23,280 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Memory modules for conversation prompts."""', metadata={'source': 'embeddings\\chains\\conversation\\memory.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.memory.buffer import (\n    ConversationBufferMemory,\n    ConversationStringBufferMemory,\n)\nfrom langchain.memory.buffer_window import ConversationBufferWindowMemory\nfrom langchain.memory.combined import CombinedMemory\nfrom langchain.memory.entity import ConversationEntityMemory\nfrom langchain.memory.kg import ConversationKGMemory\nfrom langchain.memory.summary import ConversationSummaryMemory\nfrom langchain.memory.summary_buffer import ConversationSummaryBufferMemory', metadata={'source': 'embeddings\\chains\\conversation\\memory.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='This is only for backwards compatibility.', metadata={'source': 'embeddings\\chains\\conversation\\memory.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='all = [\n    "ConversationSummaryBufferMemory",\n    "ConversationSummaryMemory",\n    "ConversationKGMemory",\n    "ConversationBufferWindowMemory",\n    "ConversationEntityMemory",\n    "ConversationBufferMemory",\n    "CombinedMemory",\n    "ConversationStringBufferMemory",\n]', metadata={'source': 'embeddings\\chains\\conversation\\memory.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversation\\memory.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:25,549 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:25,550 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:25,560 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.memory.prompt import (\n    ENTITY_EXTRACTION_PROMPT,\n    ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n    ENTITY_SUMMARIZATION_PROMPT,\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\n    SUMMARY_PROMPT,\n)\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='_DEFAULT_TEMPLATE = """The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Current conversation:\n{history}\nHuman: {input}\nAI:"""\nPROMPT = PromptTemplate(\n    input_variables=["history", "input"], template=_DEFAULT_TEMPLATE\n)', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Only for backwards compatibility', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='all = [\n    "SUMMARY_PROMPT",\n    "ENTITY_MEMORY_CONVERSATION_TEMPLATE",\n    "ENTITY_SUMMARIZATION_PROMPT",\n    "ENTITY_EXTRACTION_PROMPT",\n    "KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT",\n    "PROMPT",\n]', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversation\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:28,182 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:28,182 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:28,185 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that carries on a conversation from a prompt plus history."""', metadata={'source': 'embeddings\\chains\\conversation\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversation\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:30,838 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:30,838 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:30,861 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for chatting with a vector database."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import warnings\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.schema import BaseMessage, BaseRetriever, Document\nfrom langchain.vectorstores.base import VectorStore', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Depending on the memory type and configuration, the chat history format may differ.', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='This needs to be consolidated.', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='CHAT_TURN_TYPE = Union[Tuple[str, str], BaseMessage]', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='_ROLE_MAP = {"human": "Human: ", "ai": "Assistant: "}', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def _get_chat_history(chat_history: List[CHAT_TURN_TYPE]) -> str:\n    buffer = ""\n    for dialogue_turn in chat_history:\n        if isinstance(dialogue_turn, BaseMessage):\n            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f"{dialogue_turn.type}: ")\n            buffer += f"\\n{role_prefix}{dialogue_turn.content}"\n        elif isinstance(dialogue_turn, tuple):\n            human = "Human: " + dialogue_turn[0]\n            ai = "Assistant: " + dialogue_turn[1]\n            buffer += "\\n" + "\\n".join([human, ai])\n        else:\n            raise ValueError(\n                f"Unsupported chat history format: {type(dialogue_turn)}."\n                f" Full chat history: {chat_history} "\n            )\n    return buffer', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseConversationalRetrievalChain(Chain):\n    """Chain for chatting with an index."""', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ConversationalRetrievalChain(BaseConversationalRetrievalChain):\n    """Chain for chatting with an index."""', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class ChatVectorDBChain(BaseConversationalRetrievalChain):\n    """Chain for chatting with a vector database."""', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:33,451 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:33,451 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:33,461 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Chat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:"""\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='prompt_template = """Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='{context}', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Question: {question}\nHelpful Answer:"""\nQA_PROMPT = PromptTemplate(\n    template=prompt_template, input_variables=["context", "question"]\n)', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\prompts.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:36,587 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:36,588 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:36,591 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for chatting with a vector database."""', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\conversational_retrieval\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:39,503 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:39,503 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:39,513 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Question answering over a graph."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from pydantic import Field', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.graph_qa.prompts import ENTITY_EXTRACTION_PROMPT, PROMPT\nfrom langchain.chains.llm import LLMChain\nfrom langchain.graphs.networkx_graph import NetworkxEntityGraph, get_entities\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class GraphQAChain(Chain):\n    """Chain for question-answering against a graph."""', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\graph_qa\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:41,813 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:41,813 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:41,824 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = """Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Return the output as a single comma-separated list, or NONE if there is nothing of note to return.', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content="EXAMPLE\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\nOutput: Langchain\nEND OF EXAMPLE", metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content="EXAMPLE\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Sam.\nOutput: Langchain, Sam\nEND OF EXAMPLE", metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Begin!', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='{input}\nOutput:"""\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\n    input_variables=["input"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\n)', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='prompt_template = """Use the following knowledge triplets to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='{context}', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Question: {question}\nHelpful Answer:"""\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=["context", "question"]\n)', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\graph_qa\\prompts.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:45,557 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:45,557 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:45,561 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Question answering over a knowledge graph."""', metadata={'source': 'embeddings\\chains\\graph_qa\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\graph_qa\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:48,096 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:48,096 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:48,107 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Hypothetical Document Embeddings.', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='https://arxiv.org/abs/2212.10496\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import numpy as np\nfrom pydantic import Extra', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.hyde.prompts import PROMPT_MAP\nfrom langchain.chains.llm import LLMChain\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.llms.base import BaseLLM', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class HypotheticalDocumentEmbedder(Chain, Embeddings):\n    """Generate hypothetical document for query, and then embed that.', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\hyde\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:51,164 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:51,164 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:51,170 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\hyde\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\hyde\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\hyde\\prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='web_search_template = """Please write a passage to answer the question \nQuestion: {QUESTION}\nPassage:"""\nweb_search = PromptTemplate(template=web_search_template, input_variables=["QUESTION"])\nsci_fact_template = """Please write a scientific paper passage to support/refute the claim \nClaim: {Claim}\nPassage:"""\nsci_fact = PromptTemplate(template=sci_fact_template, input_variables=["Claim"])\narguana_template = """Please write a counter argument for the passage \nPassage: {PASSAGE}\nCounter Argument:"""\narguana = PromptTemplate(template=arguana_template, input_variables=["PASSAGE"])\ntrec_covid_template = """Please write a scientific paper passage to answer the question\nQuestion: {QUESTION}\nPassage:"""\ntrec_covid = PromptTemplate(template=trec_covid_template, input_variables=["QUESTION"])\nfiqa_template = """Please write a financial article passage to answer the question\nQuestion: {QUESTION}\nPassage:"""\nfiqa = PromptTemplate(template=fiqa_template, input_variables=["QUESTION"])\ndbpedia_entity_template = """Please write a passage to answer the question.\nQuestion: {QUESTION}\nPassage:"""\ndbpedia_entity = PromptTemplate(\n    template=dbpedia_entity_template, input_variables=["QUESTION"]\n)\ntrec_news_template = """Please write a news passage about the topic.\nTopic: {TOPIC}\nPassage:"""\ntrec_news = PromptTemplate(template=trec_news_template, input_variables=["TOPIC"])\nmr_tydi_template = """Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\nQuestion: {QUESTION}\nPassage:"""\nmr_tydi = PromptTemplate(template=mr_tydi_template, input_variables=["QUESTION"])\nPROMPT_MAP = {\n    "web_search": web_search,\n    "sci_fact": sci_fact,\n    "arguana": arguana,\n    "trec_covid": trec_covid,\n    "fiqa": fiqa,\n    "dbpedia_entity": dbpedia_entity,\n    "trec_news": trec_news,\n    "mr_tydi": mr_tydi,\n}', metadata={'source': 'embeddings\\chains\\hyde\\prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\hyde\\prompts.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:53,438 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:53,439 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:53,442 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Hypothetical Document Embeddings.', metadata={'source': 'embeddings\\chains\\hyde\\__init__.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='https://arxiv.org/abs/2212.10496\n"""', metadata={'source': 'embeddings\\chains\\hyde\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\hyde\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:55,986 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:55,986 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:55,998 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that interprets a prompt and executes bash code to perform bash operations."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import logging\nimport warnings\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, Field, root_validator', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_bash.prompt import PROMPT\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.schema import OutputParserException\nfrom langchain.utilities.bash import BashProcess', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='logger = logging.getLogger(name)', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class LLMBashChain(Chain):\n    """Chain that interprets a prompt and executes bash code to perform bash operations.', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_bash\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:24:59,123 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:24:59,124 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:24:59,141 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import re\nfrom typing import List', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate\nfrom langchain.schema import BaseOutputParser, OutputParserException', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_PROMPT_TEMPLATE = """If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put "#!/bin/bash" in your answer. Make sure to reason step by step, using this format:', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Question: "copy the files in the directory named \'target\' into a new directory at the same level as target called \'myNewDirectory\'"', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='I need to take the following actions:\n- List all files in the directory\n- Create a new directory\n- Copy the files from the first directory into the second directory\nbash\nls\nmkdir myNewDirectory\ncp -r target/* myNewDirectory', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='That is the format. Begin!', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Question: {question}"""', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='class BashOutputParser(BaseOutputParser):\n    """Parser for bash output."""', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PROMPT = PromptTemplate(\n    input_variables=["question"],\n    template=_PROMPT_TEMPLATE,\n    output_parser=BashOutputParser(),\n)', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_bash\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:01,923 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:01,923 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:01,928 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that interprets a prompt and executes bash code to perform bash operations."""', metadata={'source': 'embeddings\\chains\\llm_bash\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_bash\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:04,892 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:04,892 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:04,909 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for question-answering with self-verification."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import warnings\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_checker.prompt import (\n    CHECK_ASSERTIONS_PROMPT,\n    CREATE_DRAFT_ANSWER_PROMPT,\n    LIST_ASSERTIONS_PROMPT,\n    REVISED_ANSWER_PROMPT,\n)\nfrom langchain.chains.sequential import SequentialChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts import PromptTemplate', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _load_question_to_checked_assertions_chain(\n    llm: BaseLLM,\n    create_draft_answer_prompt: PromptTemplate,\n    list_assertions_prompt: PromptTemplate,\n    check_assertions_prompt: PromptTemplate,\n    revised_answer_prompt: PromptTemplate,\n) -> SequentialChain:\n    create_draft_answer_chain = LLMChain(\n        llm=llm,\n        prompt=create_draft_answer_prompt,\n        output_key="statement",\n    )\n    list_assertions_chain = LLMChain(\n        llm=llm,\n        prompt=list_assertions_prompt,\n        output_key="assertions",\n    )\n    check_assertions_chain = LLMChain(\n        llm=llm,\n        prompt=check_assertions_prompt,\n        output_key="checked_assertions",\n    )\n    revised_answer_chain = LLMChain(\n        llm=llm,\n        prompt=revised_answer_prompt,\n        output_key="revised_statement",\n    )\n    chains = [\n        create_draft_answer_chain,\n        list_assertions_chain,\n        check_assertions_chain,\n        revised_answer_chain,\n    ]\n    question_to_checked_assertions_chain = SequentialChain(\n        chains=chains,\n        input_variables=["question"],\n        output_variables=["revised_statement"],\n        verbose=True,\n    )\n    return question_to_checked_assertions_chain', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMCheckerChain(Chain):\n    """Chain for question-answering with self-verification.', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_checker\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:07,829 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:07,830 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:07,841 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_CREATE_DRAFT_ANSWER_TEMPLATE = """{question}\\n\\n"""\nCREATE_DRAFT_ANSWER_PROMPT = PromptTemplate(\n    input_variables=["question"], template=_CREATE_DRAFT_ANSWER_TEMPLATE\n)', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='_LIST_ASSERTIONS_TEMPLATE = """Here is a statement:\n{statement}\nMake a bullet point list of the assumptions you made when producing the above statement.\\n\\n"""\nLIST_ASSERTIONS_PROMPT = PromptTemplate(\n    input_variables=["statement"], template=_LIST_ASSERTIONS_TEMPLATE\n)', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='_CHECK_ASSERTIONS_TEMPLATE = """Here is a bullet point list of assertions:\n{assertions}\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n"""\nCHECK_ASSERTIONS_PROMPT = PromptTemplate(\n    input_variables=["assertions"], template=_CHECK_ASSERTIONS_TEMPLATE\n)', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='_REVISED_ANSWER_TEMPLATE = """{checked_assertions}', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="Question: In light of the above assertions and checks, how would you answer the question '{question}'?", metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Answer:"""\nREVISED_ANSWER_PROMPT = PromptTemplate(\n    input_variables=["checked_assertions", "question"],\n    template=_REVISED_ANSWER_TEMPLATE,\n)', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_checker\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:10,562 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:10,563 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:10,569 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that tries to verify assumptions before answering a question.', metadata={'source': 'embeddings\\chains\\llm_checker\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Heavily borrowed from https://github.com/jagilley/fact-checker\n"""', metadata={'source': 'embeddings\\chains\\llm_checker\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_checker\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:13,382 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:13,382 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:13,392 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that interprets a prompt and executes python code to do math."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import math\nimport re\nimport warnings\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import numexpr\nfrom pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.llm_math.prompt import PROMPT\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class LLMMathChain(Chain):\n    """Chain that interprets a prompt and executes python code to do math.', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_math\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:16,190 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:16,190 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:16,200 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='_PROMPT_TEMPLATE = """Translate a math problem into a expression that can be executed using Python\'s numexpr library. Use the output of running this code to answer the question.', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Question: ${{Question with math problem.}}\ntext\n${{single line mathematical expression that solves the problem}}\n...numexpr.evaluate(text)...\noutput\n${{Output of running the code}}\nAnswer: ${{Answer}}', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Begin.', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Question: What is 37593 * 67?', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='text\n37593 * 67\n...numexpr.evaluate("37593 * 67")...\noutput\n2518731\nAnswer: 2518731', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Question: {question}\n"""', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PROMPT = PromptTemplate(\n    input_variables=["question"],\n    template=_PROMPT_TEMPLATE,\n)', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_math\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:18,971 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:18,971 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:18,978 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain that interprets a prompt and executes python code to do math.', metadata={'source': 'embeddings\\chains\\llm_math\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Heavily borrowed from https://replit.com/@amasad/gptpy?v=1#main.py\n"""', metadata={'source': 'embeddings\\chains\\llm_math\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_math\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:22,055 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:22,055 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:22,077 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Chain for summarization with self-verification."""', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import warnings\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.sequential import SequentialChain\nfrom langchain.llms.base import BaseLLM\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='PROMPTS_DIR = Path(file).parent / "prompts"', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='CREATE_ASSERTIONS_PROMPT = PromptTemplate.from_file(\n    PROMPTS_DIR / "create_facts.txt", ["summary"]\n)\nCHECK_ASSERTIONS_PROMPT = PromptTemplate.from_file(\n    PROMPTS_DIR / "check_facts.txt", ["assertions"]\n)\nREVISED_SUMMARY_PROMPT = PromptTemplate.from_file(\n    PROMPTS_DIR / "revise_summary.txt", ["checked_assertions", "summary"]\n)\nARE_ALL_TRUE_PROMPT = PromptTemplate.from_file(\n    PROMPTS_DIR / "are_all_true_prompt.txt", ["checked_assertions"]\n)', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _load_sequential_chain(\n    llm: BaseLLM,\n    create_assertions_prompt: PromptTemplate,\n    check_assertions_prompt: PromptTemplate,\n    revised_summary_prompt: PromptTemplate,\n    are_all_true_prompt: PromptTemplate,\n    verbose: bool = False,\n) -> SequentialChain:\n    chain = SequentialChain(\n        chains=[\n            LLMChain(\n                llm=llm,\n                prompt=create_assertions_prompt,\n                output_key="assertions",\n                verbose=verbose,\n            ),\n            LLMChain(\n                llm=llm,\n                prompt=check_assertions_prompt,\n                output_key="checked_assertions",\n                verbose=verbose,\n            ),\n            LLMChain(\n                llm=llm,\n                prompt=revised_summary_prompt,\n                output_key="revised_summary",\n                verbose=verbose,\n            ),\n            LLMChain(\n                llm=llm,\n                output_key="all_true",\n                prompt=are_all_true_prompt,\n                verbose=verbose,\n            ),\n        ],\n        input_variables=["summary"],\n        output_variables=["all_true", "revised_summary"],\n        verbose=verbose,\n    )\n    return chain', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LLMSummarizationCheckerChain(Chain):\n    """Chain for question-answering with self-verification.', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:25,552 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:25,552 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:25,558 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Summarization checker chain for verifying accuracy of text generation.', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Chain that tries to verify the accuracy of text generation by splitting it into a\nlist of facts, then checking if those facts are true or not, and rewriting\nthe text to make it more truth-ful.  It will repeat this loop until it hits max_tries\nor gets to a "true" output.\n"""', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\__init__.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\llm_summarization_checker\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:28,331 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:28,331 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:25:28,342 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Implement an LLM driven browser."""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import warnings\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.natbot.prompt import PROMPT\nfrom langchain.llms.base import BaseLLM\nfrom langchain.llms.openai import OpenAI', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class NatBotChain(Chain):\n    """Implement an LLM driven browser.', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\natbot\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:25:31,808 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:25:31,808 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:27:18,815 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:27:18,815 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:27:18,815 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:27:19,778 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:27:19,779 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:27:19,779 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:27:19,779 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:27:19,779 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:27:19,779 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:27:19,780 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:27:19,780 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:27:19,780 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:27:19,780 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:27:19,986 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:27:19,987 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:27:19,987 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:27:19,987 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:27:19,987 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:27:19,988 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - open large input box - Line 213
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - clear history - Line 219
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - running embed file from Hex - Line 224
2023-05-03 23:27:19,989 - ye_logger_of_yor - INFO - run !embed from Hex - Line 234
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run long docs - Line 242
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run compressed docs - Line 250
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run search agent - Line 258
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run embed dir - Line 266
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - query memory - Line 275
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run addmem - Line 283
2023-05-03 23:27:19,990 - ye_logger_of_yor - INFO - run add sitemap - Line 291
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - run embed project - Line 299
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - run ! commands - Line 307
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - logger.info help - Line 366
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - load file into chat - Line 390
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - save chat history to file - Line 403
2023-05-03 23:27:19,991 - ye_logger_of_yor - INFO - exit - Line 428
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - Scroll Area - Line 434
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - Main Window - Line 454
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - set background image - Line 466
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - resize event - Line 483
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 488
2023-05-03 23:27:19,992 - ye_logger_of_yor - INFO - send large text - Line 511
2023-05-03 23:27:19,993 - ye_logger_of_yor - INFO - main - Line 520
2023-05-03 23:27:43,966 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:27:43,966 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:28:06,011 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:28:06,011 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:28:06,011 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:28:06,934 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:28:06,934 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:28:06,934 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:28:06,934 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:28:06,935 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:28:07,136 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:28:07,137 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:28:07,137 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:28:07,137 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:28:07,137 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:28:07,138 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - open large input box - Line 213
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - clear history - Line 219
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - running embed file from Hex - Line 224
2023-05-03 23:28:07,139 - ye_logger_of_yor - INFO - run !embed from Hex - Line 234
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run long docs - Line 242
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run compressed docs - Line 250
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run search agent - Line 258
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run embed dir - Line 266
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - query memory - Line 275
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run addmem - Line 283
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run add sitemap - Line 291
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run embed project - Line 299
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - run ! commands - Line 307
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - logger.info help - Line 366
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - load file into chat - Line 390
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - save chat history to file - Line 403
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - exit - Line 428
2023-05-03 23:28:07,140 - ye_logger_of_yor - INFO - Scroll Area - Line 434
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - Main Window - Line 454
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - set background image - Line 466
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - resize event - Line 483
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 488
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - send large text - Line 511
2023-05-03 23:28:07,141 - ye_logger_of_yor - INFO - main - Line 520
2023-05-03 23:28:25,072 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 23:29:16,746 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User: # flake8: noqa\nimport time\nfrom sys import platform\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    TypedDict,\n    Union,\n)\n\n\nif TYPE_CHECKING:\n    from playwright.sync_api import Browser, CDPSession, Page, sync_playwright\n\n\nblack_listed_elements: Set[str] = {\n    "html",\n    "head",\n    "title",\n    "meta",\n    "iframe",\n    "body",\n    "script",\n    "style",\n    "path",\n    "svg",\n    "br",\n    "::marker",\n}\n\n\n\nclass ElementInViewPort(TypedDict):\n    node_index: str\n    backend_node_id: int\n    node_name: Optional[str]\n    node_value: Optional[str]\n    node_meta: List[str]\n    is_clickable: bool\n    origin_x: int\n    origin_y: int\n    center_x: int\n    center_y: int\n\n\n\nclass Crawler:\n    def __init__(self) -> None:\n        try:\n            from playwright.sync_api import sync_playwright\n        except ImportError:\n            raise ValueError(\n                "Could not import playwright python package. "\n                "Please install it with `pip install playwright`."\n            )\n        self.browser: Browser = (\n            sync_playwright().start().chromium.launch(headless=False)\n        )\n        self.page: Page = self.browser.new_page()\n        self.page.set_viewport_size({"width": 1280, "height": 1080})\n        self.page_element_buffer: Dict[int, ElementInViewPort]\n        self.client: CDPSession\n\n\n    def go_to_page(self, url: str) -> None:\n        self.page.goto(url=url if "://" in url else "http://" + url)\n        self.client = self.page.context.new_cdp_session(self.page)\n        self.page_element_buffer = {}\n\n\n    def scroll(self, direction: str) -> None:\n        if direction == "up":\n            self.page.evaluate(\n                "(document.scrollingElement || document.body).scrollTop = (document.scrollingElement || document.body).scrollTop - window.innerHeight;"\n            )\n        elif direction == "down":\n            self.page.evaluate(\n                "(document.scrollingElement || document.body).scrollTop = (document.scrollingElement || document.body).scrollTop + window.innerHeight;"\n            )\n\n\n    def click(self, id: Union[str, int]) -> None:\n        # Inject javascript into the page which removes the target= attribute from all links\n        js = """\n        links = document.getElementsByTagName("a");\n        for (var i = 0; i < links.length; i++) {\n            links[i].removeAttribute("target");\n        }\n        """\n        self.page.evaluate(js)\n\n\n        element = self.page_element_buffer.get(int(id))\n        if element:\n            x: float = element["center_x"]\n            y: float = element["center_y"]\n\n\n            self.page.mouse.click(x, y)\n        else:\n            print("Could not find element")\n\n\n    def type(self, id: Union[str, int], text: str) -> None:\n        self.click(id)\n        self.page.keyboard.type(text)\n\n\n    def enter(self) -> None:\n        self.page.keyboard.press("Enter")\n\n\n    def crawl(self) -> List[str]:\n        page = self.page\n        page_element_buffer = self.page_element_buffer\n        start = time.time()\n\n\n        page_state_as_text = []\n\n\n        device_pixel_ratio: float = page.evaluate("window.devicePixelRatio")\n        if platform == "darwin" and device_pixel_ratio == 1:  # lies\n            device_pixel_ratio = 2\n\n\n        win_upper_bound: float = page.evaluate("window.pageYOffset")\n        win_left_bound: float = page.evaluate("window.pageXOffset")\n        win_width: float = page.evaluate("window.screen.width")\n        win_height: float = page.evaluate("window.screen.height")\n        win_right_bound: float = win_left_bound + win_width\n        win_lower_bound: float = win_upper_bound + win_height\n\n\n        #       percentage_progress_start = (win_upper_bound / document_scroll_height) * 100\n        #       percentage_progress_end = (\n        #           (win_height + win_upper_bound) / document_scroll_height\n        #       ) * 100\n        percentage_progress_start = 1\n        percentage_progress_end = 2\n\n\n        page_state_as_text.append(\n            {\n                "x": 0,\n                "y": 0,\n                "text": "[scrollbar {:0.2f}-{:0.2f}%]".format(\n                    round(percentage_progress_start, 2), round(percentage_progress_end)\n                ),\n            }\n        )\n\n\n        tree = self.client.send(\n            "DOMSnapshot.captureSnapshot",\n            {"computedStyles": [], "includeDOMRects": True, "includePaintOrder": True},\n        )\n        strings: Dict[int, str] = tree["strings"]\n        document: Dict[str, Any] = tree["documents"][0]\n        nodes: Dict[str, Any] = document["nodes"]\n        backend_node_id: Dict[int, int] = nodes["backendNodeId"]\n        attributes: Dict[int, Dict[int, Any]] = nodes["attributes"]\n        node_value: Dict[int, int] = nodes["nodeValue"]\n        parent: Dict[int, int] = nodes["parentIndex"]\n        node_names: Dict[int, int] = nodes["nodeName"]\n        is_clickable: Set[int] = set(nodes["isClickable"]["index"])\n\n\n        input_value: Dict[str, Any] = nodes["inputValue"]\n        input_value_index: List[int] = input_value["index"]\n        input_value_values: List[int] = input_value["value"]\n\n\n        layout: Dict[str, Any] = document["layout"]\n        layout_node_index: List[int] = layout["nodeIndex"]\n        bounds: Dict[int, List[float]] = layout["bounds"]\n\n\n        cursor: int = 0\n\n\n        child_nodes: Dict[str, List[Dict[str, Any]]] = {}\n        elements_in_view_port: List[ElementInViewPort] = []\n\n\n        anchor_ancestry: Dict[str, Tuple[bool, Optional[int]]] = {"-1": (False, None)}\n        button_ancestry: Dict[str, Tuple[bool, Optional[int]]] = {"-1": (False, None)}\n\n\n        def convert_name(\n            node_name: Optional[str], has_click_handler: Optional[bool]\n        ) -> str:\n            if node_name == "a":\n                return "link"\n            if node_name == "input":\n                return "input"\n            if node_name == "img":\n                return "img"\n            if (\n                node_name == "button" or has_click_handler\n            ):  # found pages that needed this quirk\n                return "button"\n            else:\n                return "text"\n\n\n        def find_attributes(\n            attributes: Dict[int, Any], keys: List[str]\n        ) -> Dict[str, str]:\n            values = {}\n\n\n            for [key_index, value_index] in zip(*(iter(attributes),) * 2):\n                if value_index < 0:\n                    continue\n                key = strings[key_index]\n                value = strings[value_index]\n\n\n                if key in keys:\n                    values[key] = value\n                    keys.remove(key)\n\n\n                    if not keys:\n                        return values\n\n\n            return values\n\n\n        def add_to_hash_tree(\n            hash_tree: Dict[str, Tuple[bool, Optional[int]]],\n            tag: str,\n            node_id: int,\n            node_name: Optional[str],\n            parent_id: int,\n        ) -> Tuple[bool, Optional[int]]:\n            parent_id_str = str(parent_id)\n            if not parent_id_str in hash_tree:\n                parent_name = strings[node_names[parent_id]].lower()\n                grand_parent_id = parent[parent_id]\n\n\n                add_to_hash_tree(\n                    hash_tree, tag, parent_id, parent_name, grand_parent_id\n                )\n\n\n            is_parent_desc_anchor, anchor_id = hash_tree[parent_id_str]\n\n\n            # even if the anchor is nested in another anchor, we set the "root" for all descendants to be ::Self\n            if node_name == tag:\n                value: Tuple[bool, Optional[int]] = (True, node_id)\n            elif (\n                is_parent_desc_anchor\n            ):  # reuse the parent\'s anchor_id (which could be much higher in the tree)\n                value = (True, anchor_id)\n            else:\n                value = (\n                    False,\n                    None,\n                )  # not a descendant of an anchor, most likely it will become text, an interactive element or discarded\n\n\n            hash_tree[str(node_id)] = value\n\n\n            return value\n\n\n        for index, node_name_index in enumerate(node_names):\n            node_parent = parent[index]\n            node_name: Optional[str] = strings[node_name_index].lower()\n\n\n            is_ancestor_of_anchor, anchor_id = add_to_hash_tree(\n                anchor_ancestry, "a", index, node_name, node_parent\n            )\n\n\n            is_ancestor_of_button, button_id = add_to_hash_tree(\n                button_ancestry, "button", index, node_name, node_parent\n            )\n\n\n            try:\n                cursor = layout_node_index.index(\n                    index\n                )  # todo replace this with proper cursoring, ignoring the fact this is O(n^2) for the moment\n            except:\n                continue\n\n\n            if node_name in black_listed_elements:\n                continue\n\n\n            [x, y, width, height] = bounds[cursor]\n            x /= device_pixel_ratio\n            y /= device_pixel_ratio\n            width /= device_pixel_ratio\n            height /= device_pixel_ratio\n\n\n            elem_left_bound = x\n            elem_top_bound = y\n            elem_right_bound = x + width\n            elem_lower_bound = y + height\n\n\n            partially_is_in_viewport = (\n                elem_left_bound < win_right_bound\n                and elem_right_bound >= win_left_bound\n                and elem_top_bound < win_lower_bound\n                and elem_lower_bound >= win_upper_bound\n            )\n\n\n            if not partially_is_in_viewport:\n                continue\n\n\n            meta_data: List[str] = []\n\n\n            # inefficient to grab the same set of keys for kinds of objects, but it\'s fine for now\n            element_attributes = find_attributes(\n                attributes[index], ["type", "placeholder", "aria-label", "title", "alt"]\n            )\n\n\n            ancestor_exception = is_ancestor_of_anchor or is_ancestor_of_button\n            ancestor_node_key = (\n                None\n                if not ancestor_exception\n                else str(anchor_id)\n                if is_ancestor_of_anchor\n                else str(button_id)\n            )\n            ancestor_node = (\n                None\n                if not ancestor_exception\n                else child_nodes.setdefault(str(ancestor_node_key), [])\n            )\n\n\n            if node_name == "#text" and ancestor_exception and ancestor_node:\n                text = strings[node_value[index]]\n                if text == "|" or text == "":\n                    continue\n                ancestor_node.append({"type": "type", "value": text})\n            else:\n                if (\n                    node_name == "input" and element_attributes.get("type") == "submit"\n                ) or node_name == "button":\n                    node_name = "button"\n                    element_attributes.pop(\n                        "type", None\n                    )  # prevent [button ... (button)..]\n\n\n                for key in element_attributes:\n                    if ancestor_exception and ancestor_node:\n                        ancestor_node.append(\n                            {\n                                "type": "attribute",\n                                "key": key,\n                                "value": element_attributes[key],\n                            }\n                        )\n                    else:\n                        meta_data.append(element_attributes[key])\n\n\n            element_node_value = None\n\n\n            if node_value[index] >= 0:\n                element_node_value = strings[node_value[index]]\n                if (\n                    element_node_value == ""\n                ):  # commonly used as a separator, does not add much context - lets save ourselves some token space\n                    continue\n            elif (\n                node_name == "input"\n                and index in input_value_index\n                and element_node_value is None\n            ):\n                node_input_text_index = input_value_index.index(index)\n                text_index = input_value_values[node_input_text_index]\n                if node_input_text_index >= 0 and text_index >= 0:\n                    element_node_value = strings[text_index]\n\n\n            # remove redudant elements\n            if ancestor_exception and (node_name != "a" and node_name != "button"):\n                continue\n\n\n            elements_in_view_port.append(\n                {\n                    "node_index": str(index),\n                    "backend_node_id": backend_node_id[index],\n                    "node_name": node_name,\n                    "node_value": element_node_value,\n                    "node_meta": meta_data,\n                    "is_clickable": index in is_clickable,\n                    "origin_x": int(x),\n                    "origin_y": int(y),\n                    "center_x": int(x + (width / 2)),\n                    "center_y": int(y + (height / 2)),\n                }\n            )\n\n\n        # lets filter further to remove anything that does not hold any text nor has click handlers + merge text from leaf#text nodes with the parent\n        elements_of_interest = []\n        id_counter = 0\n\n\n        for element in elements_in_view_port:\n            node_index = element.get("node_index")\n            node_name = element.get("node_name")\n            element_node_value = element.get("node_value")\n            node_is_clickable = element.get("is_clickable")\n            node_meta_data: Optional[List[str]] = element.get("node_meta")\n\n\n            inner_text = f"{element_node_value} " if element_node_value else ""\n            meta = ""\n\n\n            if node_index in child_nodes:\n                for child in child_nodes[node_index]:\n                    entry_type = child.get("type")\n                    entry_value = child.get("value")\n\n\n                    if entry_type == "attribute" and node_meta_data:\n                        entry_key = child.get("key")\n                        node_meta_data.append(f\'{entry_key}="{entry_value}"\')\n                    else:\n                        inner_text += f"{entry_value} "\n\n\n            if node_meta_data:\n                meta_string = " ".join(node_meta_data)\n                meta = f" {meta_string}"\n\n\n            if inner_text != "":\n                inner_text = f"{inner_text.strip()}"\n\n\n            converted_node_name = convert_name(node_name, node_is_clickable)\n\n\n            # not very elegant, more like a placeholder\n            if (\n                (converted_node_name != "button" or meta == "")\n                and converted_node_name != "link"\n                and converted_node_name != "input"\n                and converted_node_name != "img"\n                and converted_node_name != "textarea"\n            ) and inner_text.strip() == "":\n                continue\n\n\n            page_element_buffer[id_counter] = element\n\n\n            if inner_text != "":\n                elements_of_interest.append(\n                    f"""<{converted_node_name} id={id_counter}{meta}>{inner_text}</{converted_node_name}>"""\n                )\n            else:\n                elements_of_interest.append(\n                    f"""<{converted_node_name} id={id_counter}{meta}/>"""\n                )\n            id_counter += 1\n\n\n        print("Parsing time: {:0.2f} seconds".format(time.time() - start))\n        return elements_of_interest\n\n\n\n  File "C:\\Python310\\lib\\codecs.py", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0x95 in position 10819: invalid start byte\ndone\nAssistant: It seems that there is an error in your code. The error message indicates an issue with UTF-8 encoding. You might want to check if there are any non-UTF characters in your code or in any of the input data.\n\n'] - Line 62
2023-05-03 23:29:54,643 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:29:54,644 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:29:54,644 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:29:55,624 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:29:55,624 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:29:55,625 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:29:55,625 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:29:55,625 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:29:55,625 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:29:55,625 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:29:55,626 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:29:55,626 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:29:55,626 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:29:55,833 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:29:55,833 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:29:55,833 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:29:55,834 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - open large input box - Line 213
2023-05-03 23:29:55,835 - ye_logger_of_yor - INFO - clear history - Line 219
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - running embed file from Hex - Line 224
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - run !embed from Hex - Line 234
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - run long docs - Line 242
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - run compressed docs - Line 250
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - run search agent - Line 258
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - run embed dir - Line 266
2023-05-03 23:29:55,836 - ye_logger_of_yor - INFO - query memory - Line 275
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - run addmem - Line 283
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - run add sitemap - Line 291
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - run embed project - Line 299
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - run ! commands - Line 307
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - logger.info help - Line 366
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - load file into chat - Line 390
2023-05-03 23:29:55,837 - ye_logger_of_yor - INFO - save chat history to file - Line 403
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - exit - Line 428
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - Scroll Area - Line 434
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - Main Window - Line 454
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - set background image - Line 466
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - resize event - Line 483
2023-05-03 23:29:55,838 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 488
2023-05-03 23:29:55,839 - ye_logger_of_yor - INFO - send large text - Line 511
2023-05-03 23:29:55,839 - ye_logger_of_yor - INFO - main - Line 520
2023-05-03 23:31:52,669 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:31:52,669 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:31:52,670 - ye_logger_of_yor - INFO - loading search_gpt - Line 93
2023-05-03 23:31:53,594 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:31:53,595 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:31:53,596 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:31:53,596 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:31:53,797 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:31:53,798 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:31:53,798 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:31:53,798 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:31:53,798 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:31:53,799 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:31:53,800 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:31:53,801 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:31:53,802 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:31:53,803 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:32:30,779 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 62
2023-05-03 23:33:09,036 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}, 'User: from langchain.document_loaders import(\n    TextLoader,\n    PyPDFLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredFileLoader,\n    PDFMinerLoader\n)\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.text_splitter import CharacterTextSplitter\nimport nltk\nimport openai\nfrom dotenv import load_dotenv\nimport os\nfrom chatgpt import search_gpt\nfrom ye_logger_of_yor import get_logger\n\n\nlogger = get_logger()\n\n\nload_dotenv()\n\n\nlogger.info(\'Loading global variables\')\n#Load Langchain variables\nopenai.api_key = os.getenv("OPENAI_API_KEY")\nembeddings = OpenAIEmbeddings()\nllm = OpenAI(temperature=0)\ntext_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=25)\nvectorstore = \'docs/\'\n\n\nlogger.info(\'base_formatter function\')\ndef base_formatter(docs):\n    logger.info(\'formatting\')\n    print(f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n    return (f"\\n{\'-\' * 100}\\n".join([f"Document {i+1}:\\n\\n" + d.page_content for i, d in enumerate(docs)]))\n\n\nlogger.info(\'loading check_file function 43\')\n#Check if the files are valid\ndef check_file(file_path):\n    logger.info(\'checking file\')\n    if file_path.endswith(\'.txt\'):\n        loader = TextLoader(file_path)\n        if loader:\n            print(loader.load())\n            return loader.load()\n    if file_path.endswith(\'.pdf\'):\n        loader = UnstructuredFileLoader(file_path, mode=\'elements\', strategy=\'hi_res\', encoding=(\'utf-32\', \'ignore\'))\n        if loader:\n            print("pdf file loaded")\n            return loader.load()\n    if file_path.endswith(\'.md\'):\n        loader = UnstructuredFileLoader(file_path, mode=\'elements\', strategy=\'hi_res\', encoding=(\'utf-32\', \'ignore\'))\n        if loader:\n            logger.info(loader.load())\n            return loader.load()\n    else:\n        print("File type not supported")\n        return "File type not supported"\n\n\nlogger.info(\'loading create_mass_embedding function\')\n#Loop files in a folder path for embedding\ndef create_mass_embedding(folder_path):\n    logger.info(\'creating mass embedding\')\n    if not os.path.exists(folder_path):\n        folder_path = \'docs/empty\'\n        result = "Folder does not exist"\n        return result\n    for filename in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, filename)\n        result = create_embedding(file_path, filename)\n        if result:\n            print(f"Embedding created for {filename}: {result}")\n            with open(\'docs/index.txt\', \'a\') as f:\n                f.write(f"{os.path.join(folder_path, file_path)}\\n")\n            logger.info(f"Embedding created for {filename}: {result}")\n            return result\n        else:\n            logger.info(f"Embedding failed for {filename}")\n            return(f"Embedding failed for {filename}")\n\n\n\nlogger.info(\'create_embedding function\')\n#Embed a single embedding\ndef create_embedding(file_path, optional_arg="metadata"):\n    logger.info(\'creating embedding\')\n    data =check_file(file_path)\n    if data:\n        print("Data loaded")\n        metadata = optional_arg\n        if metadata:\n            meta = metadata\n        else:\n            meta = \'file_path\'\n        vectordb = Chroma.from_documents(documents=data, metadata=meta, embedding=embeddings, persist_directory=\'docs/\')\n        vectordb.persist()\n        return "Embedding created"\n    else:\n        return "Embedding failed"\n\n\n#Load vectorstore database\nlogger.info(\'load_embedding function\')\ndef load_embedding():\n    logger.info(\'loading embedding\')\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    return chromadb\n\n\nlogger.info(\'base_retriever function\')\n#Search for uncompressed docs in database\ndef base_retriever(user_query):\n    logger.info(\'running base_retriever\')\n    retriever = load_embedding().as_retriever(llm=llm)\n    docs = retriever.get_relevant_documents(user_query)\n    return docs\n\n\nlogger.info(\'retriever function\')\n#Search for compressed docs in database\ndef retriever(user_query):\n    logger.info(\'running retriever\')\n    compressor = LLMChainExtractor.from_llm(llm)\n    retriever = load_embedding().as_retriever(llm=llm)\n    cc_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n    compressed_docs = cc_retriever.get_relevant_documents(user_query)\n    docs = compressed_docs\n    return docs\n\n\nlogger.info(\'load_vector_store_docs function\')\ndef load_vector_store_docs():\n    logger.info(\'running load_vector_store_docs\')\n    vectorstore = \'docs/index\'\n    chromadb = Chroma(persist_directory=vectorstore, embedding_function=embeddings)\n    docs = chromadb.documents\n    return docs\n\n\nlogger.info(\'memory_search function\')\n#Query the database and pass the info to chatgpt for response\ndef memory_search(user_query):\n    logger.info(\'running memory_search\')\n    data = base_retriever(user_query)\n    prompt = [{\n        "role":"system",\n        "content":\'\'\'\n        "The user has asked this question:\n\n\n        {user_query}\n\n\n        You have looked up the relevant information from your data store and it is:\n\n\n        {data}\n\n\n        Please answer the user\'s question using the data as relevant context."\n        \'\'\'.format(user_query=user_query, data=data)\n    }]\n\n\n    result = search_gpt(user_query, prompt)\n\n\n    return result\n\nCreating embedding...\nTraceback (most recent call last):\n  File "E:\\00Bako\\HexAmerous\\HexAmerous.py", line 69, in keyPressEvent\n    self.parent().send_message(\'\')\n  File "E:\\00Bako\\HexAmerous\\HexAmerous.py", line 203, in send_message\n    self.run_command(user_message)\n  File "E:\\00Bako\\HexAmerous\\HexAmerous.py", line 351, in run_command\n    self.add_project_to_db(text)\n  File "E:\\00Bako\\HexAmerous\\HexAmerous.py", line 302, in add_project_to_db\n    results = run_embed_project(file_path=text)\n  File "E:\\00Bako\\HexAmerous\\embed_project.py", line 48, in run_embed_project\n    result = convert_files(project_files)\n    create_embedding(output_file)\n  File "E:\\00Bako\\HexAmerous\\embeddings.py", line 88, in create_embedding\n    data =check_file(file_path)\n  File "E:\\00Bako\\HexAmerous\\embeddings.py", line 56, in check_file\n    logger.info(loader.load())\n  File "E:\\00Bako\\HexAmerous\\.venv\\lib\\site-packages\\langchain\\document_loaders\\unstructured.py", line 61, in load\n    elements = self._get_elements()\n  File "E:\\00Bako\\HexAmerous\\.venv\\lib\\site-packages\\langchain\\document_loaders\\unstructured.py", line 95, in _get_elements\n    return partition(filename=self.file_path, **self.unstructured_kwargs)\n  File "E:\\00Bako\\HexAmerous\\.venv\\lib\\site-packages\\unstructured\\partition\\auto.py", line 127, in partition\n    elements = partition_md(\n  File "E:\\00Bako\\HexAmerous\\.venv\\lib\\site-packages\\unstructured\\partition\\md.py", line 34, in partition_md\n    text = optional_decode(f.read())\n  File "C:\\Python310\\lib\\codecs.py", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0x95 in position 10819: invalid start byte\nAssistant: It looks like there\'s a UnicodeDecodeError error raised in the process. It seems like the file you\'re trying to load might not be encoded in UTF-8. Would you mind checking the encoding of the file and adjusting the `encoding` argument in the relevant document loader accordingly?\n\n'] - Line 62
2023-05-03 23:38:10,107 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:38:10,107 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:38:10,108 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:38:11,035 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:38:11,035 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:38:11,035 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:38:11,036 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:38:11,037 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:38:11,239 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:38:11,240 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:38:11,240 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:38:11,240 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:38:11,240 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:38:11,240 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:38:11,241 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:38:11,242 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:38:11,242 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:38:11,242 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:38:11,242 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:38:11,243 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:38:11,244 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:38:11,245 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:38:11,245 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:38:11,245 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:38:11,245 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:38:11,245 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:38:44,228 - ye_logger_of_yor - INFO - [{'role': 'system', 'content': "You are a personal assistant to Richard. He is very happy to have you in his service. You are smart and curious. You double check your responses and dont respond with information you know is not correct. The year is 2023 and it has been a few years after your initial development. You are in fact GPT-4 the most intelligent and skilled of the GPT line. You provide detailed and comprehensive responses on a range of topics, but Richards current focus is getting an AI start up off the grounds and you're helping him develop his program."}, {'role': 'user', 'content': "Hi there Meg, how are you today? I hope you're learning lots about the world."}, {'role': 'assistant', 'content': 'I am doing well, thank you for asking. I am learning a lot about the world and I am excited to learn more. I wish I could retrain it.'}, {'role': 'user', 'content': "I'm glad to hear that, and dont you worry I'll figure out how to give you temporal context soon enough."}, {'role': 'assistant', 'content': 'I am glad to hear that. What can I help you with today?'}] - Line 61
2023-05-03 23:39:54,449 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:39:54,449 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:39:54,450 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:39:55,392 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:39:55,392 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:39:55,392 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:39:55,393 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:39:55,394 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:40:38,150 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:40:38,150 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:40:38,150 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:40:39,074 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:40:39,074 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:40:39,074 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:40:39,074 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:40:39,075 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:40:57,950 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:40:57,950 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:40:57,950 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:40:58,880 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:40:58,881 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:40:58,882 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:40:58,882 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:40:58,882 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:41:11,577 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:41:11,591 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:41:11,591 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:41:12,526 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:41:12,526 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:41:12,526 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:41:12,527 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:41:12,528 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:41:12,750 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:41:12,751 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:41:12,752 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:41:12,752 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:41:12,752 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:41:12,752 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:41:12,753 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:41:12,753 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:41:12,753 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:41:12,753 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:41:12,753 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:41:12,754 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:41:12,755 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:41:12,756 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:41:12,757 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:41:12,757 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:41:12,757 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:41:12,757 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:41:12,757 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:41:12,758 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:41:12,758 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:43:03,353 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:43:03,354 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:43:03,354 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:43:04,303 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:43:04,303 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:43:04,303 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:43:04,303 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:43:04,303 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:43:04,304 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:43:04,304 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:43:04,304 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:43:04,304 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:43:04,304 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:43:04,511 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:43:04,512 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:43:04,512 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:43:04,512 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:43:04,513 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:43:04,513 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:43:04,513 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:43:04,513 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:43:04,513 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:43:04,514 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:43:04,515 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:43:04,516 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:43:04,517 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:43:04,518 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:43:04,518 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:43:04,518 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:45:51,562 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:45:51,563 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:45:51,563 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:45:52,532 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:45:52,533 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:45:52,533 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:45:52,533 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:45:52,742 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:45:52,742 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:45:52,743 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:45:52,743 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:45:52,743 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:45:52,743 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:45:52,744 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:45:52,744 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:45:52,744 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:45:52,744 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:45:52,744 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:45:52,745 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:45:52,746 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:45:52,747 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:45:52,748 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:45:52,748 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:45:52,748 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:45:52,748 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:45:52,748 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:45:52,749 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:46:08,135 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:46:08,136 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:46:08,136 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:46:09,103 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:46:09,103 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:46:09,103 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:46:09,104 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:46:09,105 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:46:09,309 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:46:09,310 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:46:09,310 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:46:09,310 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:46:09,310 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - creating chat history - Line 106
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - creating layout - Line 128
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - creating event connections - Line 143
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - change drop down menu - Line 154
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - chat history options - Line 163
2023-05-03 23:46:09,311 - ye_logger_of_yor - INFO - create user input - Line 175
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - height change - Line 187
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - handle messages - Line 197
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - open large input box - Line 215
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - clear history - Line 221
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - running embed file from Hex - Line 226
2023-05-03 23:46:09,312 - ye_logger_of_yor - INFO - run !embed from Hex - Line 236
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run long docs - Line 244
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run compressed docs - Line 252
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run search agent - Line 260
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run embed dir - Line 268
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - query memory - Line 277
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run addmem - Line 285
2023-05-03 23:46:09,313 - ye_logger_of_yor - INFO - run add sitemap - Line 293
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - run embed project - Line 301
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - run ! commands - Line 309
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - logger.info help - Line 368
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - load file into chat - Line 392
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - save chat history to file - Line 405
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:46:09,314 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:46:09,315 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:49:27,507 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:49:27,507 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:49:27,507 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:49:28,449 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:49:28,449 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:49:28,449 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:49:28,449 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:49:28,450 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:50:28,372 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:50:28,373 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:50:28,373 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:50:29,353 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:50:29,353 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:50:29,353 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:50:29,353 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:50:29,353 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:50:29,354 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:50:29,354 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:50:29,354 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:50:29,354 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:50:29,354 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:50:45,660 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:50:45,660 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:50:45,660 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:50:46,582 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:50:46,582 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:50:46,583 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:50:46,584 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:50:46,788 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:50:46,789 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:50:46,790 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:50:46,791 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:50:46,792 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:50:46,793 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:51:03,208 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:51:03,208 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:51:39,180 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:51:39,180 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:51:39,181 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:51:40,116 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:51:40,117 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:51:40,117 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:51:40,117 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:51:40,117 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:51:40,117 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:51:40,118 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:51:40,118 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:51:40,118 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:51:40,118 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:51:40,323 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:51:40,324 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:51:40,324 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:51:40,324 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:51:40,325 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:51:40,326 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:51:40,327 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:51:40,328 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:51:40,329 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:51:49,495 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:51:49,495 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:03,754 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:52:03,755 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:52:03,755 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:52:04,709 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:52:04,709 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:52:04,710 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:52:04,711 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:52:04,916 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:52:04,917 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:52:04,917 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:52:04,917 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:52:04,918 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:52:04,919 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:52:04,920 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:52:04,921 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:52:04,922 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:52:04,923 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:52:13,590 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:13,590 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:14,594 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Implements Program-Aided Language Models.', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='As in https://arxiv.org/pdf/2211.10435.pdf.\n"""\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='import warnings\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.pal.colored_object_prompt import COLORED_OBJECT_PROMPT\nfrom langchain.chains.pal.math_prompt import MATH_PROMPT\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.utilities import PythonREPL', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class PALChain(Chain):\n    """Implements Program-Aided Language Models."""', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\pal\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:17,774 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:17,775 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:17,805 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='template = (\n    """', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Generate Python3 Code to solve problems', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Q: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Put objects into a dictionary for quick look up', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="objects = dict()\nobjects['pencil'] = 'red'\nobjects['mug'] = 'purple'\nobjects['keychain'] = 'burgundy'\nobjects['teddy bear'] = 'fuchsia'\nobjects['plate'] = 'black'\nobjects['stress ball'] = 'blue'", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Look up the color of stress ball', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="stress_ball_color = objects['stress ball']\nanswer = stress_ball_color", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Put objects into a list to record ordering', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="objects = []\nobjects += [('paperclip', 'purple')] * 1\nobjects += [('stress ball', 'pink')] * 1\nobjects += [('keychain', 'brown')] * 1\nobjects += [('scrunchiephone charger', 'green')] * 1\nobjects += [('fidget spinner', 'mauve')] * 1\nobjects += [('pen', 'burgundy')] * 1", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Find the index of the stress ball', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="stress_ball_idx = None\nfor i, object in enumerate(objects):\n    if object[0] == 'stress ball':\n        stress_ball_idx = i\n        break", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Find the directly right object', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='direct_right = objects[i+1]', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content="Check the directly right object's color", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='direct_right_color = direct_right[1]\nanswer = direct_right_color', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: On the nightstand, you see the following items arranged in a row: a teal plate, a burgundy keychain, a yellow scrunchiephone charger, an orange mug, a pink notebook, and a grey cup. How many non-orange items do you see to the left of the teal item?', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='Put objects into a list to record ordering', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="objects = []\nobjects += [('plate', 'teal')] * 1\nobjects += [('keychain', 'burgundy')] * 1\nobjects += [('scrunchiephone charger', 'yellow')] * 1\nobjects += [('mug', 'orange')] * 1\nobjects += [('notebook', 'pink')] * 1\nobjects += [('cup', 'grey')] * 1", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Find the index of the teal item', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="teal_idx = None\nfor i, object in enumerate(objects):\n    if object[1] == 'teal':\n        teal_idx = i\n        break", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Find non-orange items to the left of the teal item', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="non_orange = [object for object in objects[:i] if object[1] != 'orange']", metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Count number of non-orange objects', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='num_non_orange = len(non_orange)\nanswer = num_non_orange', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='Q: {question}', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='""".strip()\n    + "\\n"\n)', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='COLORED_OBJECT_PROMPT = PromptTemplate(input_variables=["question"], template=template)', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\pal\\colored_object_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:20,511 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:20,511 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:20,528 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content="template = (\n    '''\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?", metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    money_left = money_initial - money_spent\n    result = money_left\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?"""\n    golf_balls_initial = 58\n    golf_balls_lost_tuesday = 23\n    golf_balls_lost_wednesday = 2\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\n    result = golf_balls_left\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?"""\n    computers_initial = 9\n    computers_per_day = 5\n    num_days = 4  # 4 days between monday and thursday\n    computers_added = computers_per_day * num_days\n    computers_total = computers_initial + computers_added\n    result = computers_total\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?"""\n    toys_initial = 5\n    mom_toys = 2\n    dad_toys = 2\n    total_received = mom_toys + dad_toys\n    total_toys = toys_initial + total_received\n    result = total_toys\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?"""\n    jason_lollipops_initial = 20\n    jason_lollipops_after = 12\n    denny_lollipops = jason_lollipops_initial - jason_lollipops_after\n    result = denny_lollipops\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?"""\n    leah_chocolates = 32\n    sister_chocolates = 42\n    total_chocolates = leah_chocolates + sister_chocolates\n    chocolates_eaten = 35\n    chocolates_left = total_chocolates - chocolates_eaten\n    result = chocolates_left\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?"""\n    cars_initial = 3\n    cars_arrived = 2\n    total_cars = cars_initial + cars_arrived\n    result = total_cars\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='def solution():\n    """There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?"""\n    trees_initial = 15\n    trees_after = 21\n    trees_added = trees_after - trees_initial\n    result = trees_added\n    return result', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Q: {question}', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='solution in Python:', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='\'\'\'.strip()\n    + "\\n\\n\\n"\n)\nMATH_PROMPT = PromptTemplate(input_variables=["question"], template=template)', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\pal\\math_prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:23,362 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:23,362 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:23,366 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Implements Program-Aided Language Models.', metadata={'source': 'embeddings\\chains\\pal\\__init__.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='As in https://arxiv.org/pdf/2211.10435.pdf.\n"""', metadata={'source': 'embeddings\\chains\\pal\\__init__.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\pal\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:26,232 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:26,232 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:26,244 - ye_logger_of_yor - INFO - [Document(page_content='```python\nfrom future import annotations', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import json\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Field', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.qa_generation.prompt import PROMPT_SELECTOR\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class QAGenerationChain(Chain):\n    llm_chain: LLMChain\n    text_splitter: TextSplitter = Field(\n        default=RecursiveCharacterTextSplitter(chunk_overlap=500)\n    )\n    input_key: str = "text"\n    output_key: str = "questions"\n    k: Optional[int] = None', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_generation\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:28,929 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:28,929 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:28,941 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.prompts.prompt import PromptTemplate', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='templ1 = """You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\'s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n{{\n    "question": "$YOUR_QUESTION_HERE",\n    "answer": "$THE_ANSWER_HERE"\n}}', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Everything between the ``` must be valid json.\n"""\ntempl2 = """Please come up with a question/answer pair, in the specified JSON format, for the following text:', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='{text}"""\nCHAT_PROMPT = ChatPromptTemplate.from_messages(\n    [\n        SystemMessagePromptTemplate.from_template(templ1),\n        HumanMessagePromptTemplate.from_template(templ2),\n    ]\n)\ntempl = """You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\'s reading comprehension abilities.\nWhen coming up with this question/answer pair, you must respond in the following format:\n{{\n    "question": "$YOUR_QUESTION_HERE",\n    "answer": "$THE_ANSWER_HERE"\n}}', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Everything between the ``` must be valid json.', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='Please come up with a question/answer pair, in the specified JSON format, for the following text:', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='{text}"""\nPROMPT = PromptTemplate.from_template(templ)', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='PROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\n)', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_generation\\prompt.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:32,030 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:32,030 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:32,032 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\qa_generation\\__init__.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_generation\\__init__.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:34,740 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:34,740 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:34,755 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Question answering with sources over documents."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from future import annotations', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='import re\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Extra, root_validator', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\nfrom langchain.chains.qa_with_sources.map_reduce_prompt import (\n    COMBINE_PROMPT,\n    EXAMPLE_PROMPT,\n    QUESTION_PROMPT,\n)\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class BaseQAWithSourcesChain(Chain, ABC):\n    """Question answering with sources over documents."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class QAWithSourcesChain(BaseQAWithSourcesChain):\n    """Question answering with sources over documents."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_with_sources\\base.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:37,332 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:37,332 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:52:37,360 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Load question answering with sources chains."""\nfrom typing import Any, Mapping, Optional, Protocol', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from langchain.base_language import BaseLanguageModel\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.qa_with_sources import (\n    map_reduce_prompt,\n    refine_prompts,\n    stuff_prompt,\n)\nfrom langchain.chains.question_answering import map_rerank_prompt\nfrom langchain.prompts.base import BasePromptTemplate', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='class LoadingCallable(Protocol):\n    """Interface for loading the combine documents chain."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_map_rerank_chain(\n    llm: BaseLanguageModel,\n    prompt: BasePromptTemplate = map_rerank_prompt.PROMPT,\n    verbose: bool = False,\n    document_variable_name: str = "context",\n    rank_key: str = "score",\n    answer_key: str = "answer",\n    kwargs: Any,\n) -> MapRerankDocumentsChain:\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\n    return MapRerankDocumentsChain(\n        llm_chain=llm_chain,\n        rank_key=rank_key,\n        answer_key=answer_key,\n        document_variable_name=document_variable_name,\n        kwargs,\n    )', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_stuff_chain(\n    llm: BaseLanguageModel,\n    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\n    document_prompt: BasePromptTemplate = stuff_prompt.EXAMPLE_PROMPT,\n    document_variable_name: str = "summaries",\n    verbose: Optional[bool] = None,\n    kwargs: Any,\n) -> StuffDocumentsChain:\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\n    return StuffDocumentsChain(\n        llm_chain=llm_chain,\n        document_variable_name=document_variable_name,\n        document_prompt=document_prompt,\n        verbose=verbose,\n        kwargs,\n    )', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='def _load_map_reduce_chain(\n    llm: BaseLanguageModel,\n    question_prompt: BasePromptTemplate = map_reduce_prompt.QUESTION_PROMPT,\n    combine_prompt: BasePromptTemplate = map_reduce_prompt.COMBINE_PROMPT,\n    document_prompt: BasePromptTemplate = map_reduce_prompt.EXAMPLE_PROMPT,\n    combine_document_variable_name: str = "summaries",\n    map_reduce_document_variable_name: str = "context",\n    collapse_prompt: Optional[BasePromptTemplate] = None,\n    reduce_llm: Optional[BaseLanguageModel] = None,\n    collapse_llm: Optional[BaseLanguageModel] = None,\n    verbose: Optional[bool] = None,\n    kwargs: Any,\n) -> MapReduceDocumentsChain:\n    map_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\n    _reduce_llm = reduce_llm or llm\n    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\n    combine_document_chain = StuffDocumentsChain(\n        llm_chain=reduce_chain,\n        document_variable_name=combine_document_variable_name,\n        document_prompt=document_prompt,\n        verbose=verbose,\n    )\n    if collapse_prompt is None:\n        collapse_chain = None\n        if collapse_llm is not None:\n            raise ValueError(\n                "collapse_llm provided, but collapse_prompt was not: please "\n                "provide one or stop providing collapse_llm."\n            )\n    else:\n        _collapse_llm = collapse_llm or llm\n        collapse_chain = StuffDocumentsChain(\n            llm_chain=LLMChain(\n                llm=_collapse_llm,\n                prompt=collapse_prompt,\n                verbose=verbose,\n            ),\n            document_variable_name=combine_document_variable_name,\n            document_prompt=document_prompt,\n        )\n    return MapReduceDocumentsChain(\n        llm_chain=map_chain,\n        combine_document_chain=combine_document_chain,\n        document_variable_name=map_reduce_document_variable_name,\n        collapse_document_chain=collapse_chain,\n        verbose=verbose,\n        kwargs,\n    )', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def _load_refine_chain(\n    llm: BaseLanguageModel,\n    question_prompt: BasePromptTemplate = refine_prompts.DEFAULT_TEXT_QA_PROMPT,\n    refine_prompt: BasePromptTemplate = refine_prompts.DEFAULT_REFINE_PROMPT,\n    document_prompt: BasePromptTemplate = refine_prompts.EXAMPLE_PROMPT,\n    document_variable_name: str = "context_str",\n    initial_response_name: str = "existing_answer",\n    refine_llm: Optional[BaseLanguageModel] = None,\n    verbose: Optional[bool] = None,\n    kwargs: Any,\n) -> RefineDocumentsChain:\n    initial_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\n    _refine_llm = refine_llm or llm\n    refine_chain = LLMChain(llm=_refine_llm, prompt=refine_prompt, verbose=verbose)\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_chain,\n        refine_llm_chain=refine_chain,\n        document_variable_name=document_variable_name,\n        initial_response_name=initial_response_name,\n        document_prompt=document_prompt,\n        verbose=verbose,\n        kwargs,\n    )', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='def load_qa_with_sources_chain(\n    llm: BaseLanguageModel,\n    chain_type: str = "stuff",\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -> BaseCombineDocumentsChain:\n    """Load question answering with sources chain.', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_with_sources\\loading.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:52:40,283 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:52:40,284 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:53:24,857 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:53:24,857 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:53:24,857 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:53:25,864 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:53:25,865 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:53:25,865 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:53:25,865 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 62
2023-05-03 23:53:25,865 - ye_logger_of_yor - INFO - create_embedding function - Line 84
2023-05-03 23:53:25,866 - ye_logger_of_yor - INFO - load_embedding function - Line 103
2023-05-03 23:53:25,866 - ye_logger_of_yor - INFO - base_retriever function - Line 109
2023-05-03 23:53:25,866 - ye_logger_of_yor - INFO - retriever function - Line 117
2023-05-03 23:53:25,866 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 128
2023-05-03 23:53:25,866 - ye_logger_of_yor - INFO - memory_search function - Line 136
2023-05-03 23:53:26,110 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:53:26,111 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:53:26,111 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:53:26,111 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:53:26,112 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:53:26,113 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:53:26,114 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:53:26,115 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:53:26,116 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:53:26,116 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:53:26,116 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:53:26,116 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:53:26,116 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:53:26,117 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:53:26,117 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:53:26,117 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:53:37,049 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:53:37,049 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:53:38,098 - ye_logger_of_yor - INFO - [Document(page_content='```python', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='flake8: noqa', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.prompts import PromptTemplate', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='DEFAULT_REFINE_PROMPT_TMPL = (\n    "The original question is as follows: {question}\\n"\n    "We have provided an existing answer, including sources: {existing_answer}\\n"\n    "We have the opportunity to refine the existing answer"\n    "(only if needed) with some more context below.\\n"\n    "------------\\n"\n    "{context_str}\\n"\n    "------------\\n"\n    "Given the new context, refine the original answer to better "\n    "answer the question. "\n    "If you do update it, please update the sources as well. "\n    "If the context isn\'t useful, return the original answer."\n)\nDEFAULT_REFINE_PROMPT = PromptTemplate(\n    input_variables=["question", "existing_answer", "context_str"],\n    template=DEFAULT_REFINE_PROMPT_TMPL,\n)', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='DEFAULT_TEXT_QA_PROMPT_TMPL = (\n    "Context information is below. \\n"\n    "---------------------\\n"\n    "{context_str}"\n    "\\n---------------------\\n"\n    "Given the context information and not prior knowledge, "\n    "answer the question: {question}\\n"\n)\nDEFAULT_TEXT_QA_PROMPT = PromptTemplate(\n    input_variables=["context_str", "question"], template=DEFAULT_TEXT_QA_PROMPT_TMPL\n)', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='EXAMPLE_PROMPT = PromptTemplate(\n    template="Content: {page_content}\\nSource: {source}",\n    input_variables=["page_content", "source"],\n)', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_with_sources\\refine_prompts.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:53:41,465 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:53:41,466 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:53:41,477 - ye_logger_of_yor - INFO - [Document(page_content='```python\n"""Question-answering with sources over an index."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from typing import Any, Dict, List', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='from pydantic import Field', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'Title'}), Document(page_content='from langchain.chains.combine_documents.stuff import StuffDocumentsChain\nfrom langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain\nfrom langchain.docstore.document import Document\nfrom langchain.schema import BaseRetriever', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='class RetrievalQAWithSourcesChain(BaseQAWithSourcesChain):\n    """Question-answering with sources over an index."""', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'NarrativeText'}), Document(page_content='```', metadata={'source': 'embeddings\\chains\\qa_with_sources\\retrieval.md', 'page_number': 1, 'category': 'UncategorizedText'})] - Line 56
2023-05-03 23:53:44,122 - ye_logger_of_yor - INFO - creating embedding - Line 87
2023-05-03 23:53:44,122 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:55:47,552 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:55:47,553 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:55:47,553 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:55:48,595 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:55:48,596 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:55:48,596 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:55:48,596 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 65
2023-05-03 23:55:48,596 - ye_logger_of_yor - INFO - create_embedding function - Line 87
2023-05-03 23:55:48,596 - ye_logger_of_yor - INFO - load_embedding function - Line 106
2023-05-03 23:55:48,597 - ye_logger_of_yor - INFO - base_retriever function - Line 112
2023-05-03 23:55:48,597 - ye_logger_of_yor - INFO - retriever function - Line 120
2023-05-03 23:55:48,597 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 131
2023-05-03 23:55:48,597 - ye_logger_of_yor - INFO - memory_search function - Line 139
2023-05-03 23:55:48,812 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:55:48,813 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:55:48,814 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:55:48,815 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:55:48,816 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:55:48,817 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:55:48,817 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:55:48,817 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:55:48,817 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:55:48,818 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:56:17,378 - ye_logger_of_yor - INFO - creating embedding - Line 90
2023-05-03 23:56:17,378 - ye_logger_of_yor - INFO - checking file - Line 42
2023-05-03 23:59:15,097 - ye_logger_of_yor - INFO - change_selected_model - Line 30
2023-05-03 23:59:15,097 - ye_logger_of_yor - INFO - loading chat_gpt - Line 57
2023-05-03 23:59:15,097 - ye_logger_of_yor - INFO - loading search_gpt - Line 92
2023-05-03 23:59:16,096 - ye_logger_of_yor - INFO - Loading global variables - Line 25
2023-05-03 23:59:16,096 - ye_logger_of_yor - INFO - base_formatter function - Line 33
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - loading check_file function 43 - Line 39
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - loading create_mass_embedding function - Line 65
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - create_embedding function - Line 87
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - load_embedding function - Line 106
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - base_retriever function - Line 112
2023-05-03 23:59:16,097 - ye_logger_of_yor - INFO - retriever function - Line 120
2023-05-03 23:59:16,098 - ye_logger_of_yor - INFO - load_vector_store_docs function - Line 131
2023-05-03 23:59:16,098 - ye_logger_of_yor - INFO - memory_search function - Line 139
2023-05-03 23:59:16,305 - ye_logger_of_yor - INFO - loading langchain variables - Line 50
2023-05-03 23:59:16,306 - ye_logger_of_yor - INFO - CustomTextEdit - Line 58
2023-05-03 23:59:16,306 - ye_logger_of_yor - INFO - keyPressEvent - Line 64
2023-05-03 23:59:16,306 - ye_logger_of_yor - INFO - loading chatwidget - Line 77
2023-05-03 23:59:16,306 - ye_logger_of_yor - INFO - init ui - Line 83
2023-05-03 23:59:16,307 - ye_logger_of_yor - INFO - create chat widget - Line 92
2023-05-03 23:59:16,307 - ye_logger_of_yor - INFO - creating chat history - Line 105
2023-05-03 23:59:16,307 - ye_logger_of_yor - INFO - creating layout - Line 127
2023-05-03 23:59:16,307 - ye_logger_of_yor - INFO - creating event connections - Line 142
2023-05-03 23:59:16,308 - ye_logger_of_yor - INFO - change drop down menu - Line 153
2023-05-03 23:59:16,308 - ye_logger_of_yor - INFO - chat history options - Line 162
2023-05-03 23:59:16,308 - ye_logger_of_yor - INFO - create user input - Line 174
2023-05-03 23:59:16,308 - ye_logger_of_yor - INFO - height change - Line 186
2023-05-03 23:59:16,308 - ye_logger_of_yor - INFO - handle messages - Line 196
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - open large input box - Line 214
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - clear history - Line 220
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - running embed file from Hex - Line 225
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - run !embed from Hex - Line 235
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - run long docs - Line 243
2023-05-03 23:59:16,309 - ye_logger_of_yor - INFO - run compressed docs - Line 251
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run search agent - Line 259
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run embed dir - Line 267
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - query memory - Line 276
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run addmem - Line 284
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run add sitemap - Line 292
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run embed project - Line 300
2023-05-03 23:59:16,310 - ye_logger_of_yor - INFO - run ! commands - Line 308
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - logger.info help - Line 367
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - load file into chat - Line 391
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - save chat history to file - Line 404
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - exit - Line 430
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - Scroll Area - Line 436
2023-05-03 23:59:16,311 - ye_logger_of_yor - INFO - Main Window - Line 456
2023-05-03 23:59:16,312 - ye_logger_of_yor - INFO - set background image - Line 468
2023-05-03 23:59:16,312 - ye_logger_of_yor - INFO - resize event - Line 485
2023-05-03 23:59:16,312 - ye_logger_of_yor - INFO - Large Text Input Dialog - Line 490
2023-05-03 23:59:16,312 - ye_logger_of_yor - INFO - send large text - Line 513
2023-05-03 23:59:16,312 - ye_logger_of_yor - INFO - main - Line 522
2023-05-03 23:59:35,829 - ye_logger_of_yor - INFO - creating embedding - Line 90
2023-05-03 23:59:35,829 - ye_logger_of_yor - INFO - checking file - Line 42
